{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this experiment, using Split Conformal Prediction, we want to analyze the following:\n",
        "\n",
        "1.   How different can be $\\mathcal{D}_{\\text{train}}$ and $\\mathcal{D}_{\\text{calib}}$?\n",
        "2.   How much variation can $\\mathcal{D}_{\\text{calib}}$ take? Add Gaussian noise to assess it.\n",
        "\n",
        "1.   What partitions of a finite set works?\n",
        "2.   Evaluate Coverage and Prediction Set Size.\n",
        "\n",
        "1.   APS vs SCP is implemented as recalibrating/or not in the last loop.\n"
      ],
      "metadata": {
        "id": "O23ciYoWhFWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm torchvision matplotlib -q"
      ],
      "metadata": {
        "id": "yVCN7F_07lkI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de63eea1-53f6-4bd4-c4a0-9260145db013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXH0U8nMo_-2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import timm\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#from tensorflow.keras.datasets import mnist\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.nn.functional import softmax\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from captum.attr import LRP\n",
        "from captum.attr import visualization as viz\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Alternative as MNIST original link keeps failing...\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "X = mnist.data.numpy().reshape(-1, 784) / 255.0\n",
        "y = mnist.targets.numpy()\n",
        "\n",
        "y = y.astype(int)\n",
        "\n",
        "len(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6SFpfFM5Mid",
        "outputId": "9085d0f1-4fa3-4bfd-f109-872dec7d85ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 48.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 3.33MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.71MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display of the first 10 images\n",
        "fig, axes = plt.subplots(1, 10, figsize=(15, 3))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(X[i].reshape(28, 28), cmap='gray')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Label: {y[i]}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "0giCEwzIX6KH",
        "outputId": "c9e6a8bc-5216-4a1e-b154-d731a8e39a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP"
      ],
      "metadata": {
        "id": "DiIqknu7ZziI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting dataset randomly into Training (D_Train) and Calibration (D_Calib)\n",
        "# 70% for training, 30% for calibration\n",
        "X_train, X_calib, y_train, y_calib = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "daBV3B015ici"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model is a simple MLP classifier (Multilayer Perceptron)\n",
        "clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=20, random_state=42)\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "55xL9uio5ley",
        "outputId": "f8f655d2-6075-4c69-e2a3-1ddc7a1961db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(max_iter=20, random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=20, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(max_iter=20, random_state=42)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate accuracy\n",
        "def compute_accuracy(X, y, model):\n",
        "    y_pred = model.predict(X)\n",
        "    return accuracy_score(y, y_pred)\n",
        "\n",
        "original_accuracy = compute_accuracy(X_calib, y_calib, clf)\n",
        "print(f\"Original accuracy on calibration set: {original_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT1SsG1rKh_4",
        "outputId": "6778d28a-4cdd-45d1-94b7-7c0f4a7d76ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original accuracy on calibration set: 0.9727222222222223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install innvestigate scikit-learn -q\n",
        "!pip install captum -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r45AeuWYAwtB",
        "outputId": "c4f06d53-c5cf-40b2-b631-f29489fa47e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/840.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/840.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m696.3/840.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m827.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.14.1 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.14.1 which is incompatible.\n",
            "tensorstore 0.1.72 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NN - LRP"
      ],
      "metadata": {
        "id": "CkNijyseft9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)   # Sin stride (mantiene 28x28)\n",
        "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1, stride=2)  # Stride solo aquí (reduce a 14x14)\n",
        "        self.fc1 = nn.Linear(16 * 14 * 14, 60)  # Ajustar tamaño de entrada\n",
        "        self.fc2 = nn.Linear(60, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))  # 28x28x16\n",
        "        x = self.relu(self.conv2(x))  # 14x14x16 (stride=2)\n",
        "        x = x.view(x.size(0), -1)  # Aplanar a (batch, 16*14*14)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Filter dataset for only digits 1 and 9\n",
        "def filter_digits(dataset, digits=[1, 9]):\n",
        "    filtered_indices = [i for i, label in enumerate(dataset.targets) if label in digits]\n",
        "    filtered_dataset = torch.utils.data.Subset(dataset, filtered_indices)\n",
        "    return filtered_dataset\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Filter for only 1 and 9\n",
        "train_dataset = filter_digits(train_dataset, digits=[1, 9])\n",
        "test_dataset = filter_digits(test_dataset, digits=[1, 9])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train model (for demonstration, we train for 1 epoch)\n",
        "def train_model(model, train_loader, optimizer, criterion, epochs=1):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "UZfepzwQ7V6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the following line to train the model\n",
        "train_model(model, DataLoader(train_dataset, batch_size=64, shuffle=True), optimizer, criterion, epochs=1)"
      ],
      "metadata": {
        "id": "lub1rPLj7V3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e151d1cf-35d7-4782-a20d-5e78177e4efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 0.0008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(model, dataloader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "accuracy = compute_accuracy(model, test_loader)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "mrzpMg2SUcEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834729b5-1643-4902-ca3a-04858001ac01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 99.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained model (if available)\n",
        "try:\n",
        "    model.load_state_dict(torch.load(\"mnist_cnn.pth\"))\n",
        "    print(\"Loaded pretrained model.\")\n",
        "except:\n",
        "    print(\"Training model...\")\n",
        "    train_model(model, DataLoader(train_dataset, batch_size=64, shuffle=True), optimizer, criterion, epochs=1)\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pth\")\n",
        "\n",
        "# Function to get LRP attributions\n",
        "def get_lrp_attributions(image, model):\n",
        "    image = image.unsqueeze(0)  # Ensure the image has a batch dimension\n",
        "    output = model(image)\n",
        "    predicted_label = torch.argmax(output, dim=1).item()\n",
        "\n",
        "    # Apply LRP\n",
        "    lrp = LRP(model)\n",
        "    attributions = lrp.attribute(image, target=predicted_label)\n",
        "    return attributions, predicted_label"
      ],
      "metadata": {
        "id": "DkqG84cT7Vz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a857822e-30f3-408c-b288-7443bfeb73bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-5df669f29cb6>:3: FutureWarning:\n",
            "\n",
            "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 0.0544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select and visualize 3 random images and their corresponding LRP heatmaps\n",
        "for i, (image, label) in enumerate(test_loader):\n",
        "    if i == 4:  # Stop after 3 images\n",
        "        break\n",
        "\n",
        "    # Ensure the image is in the correct shape [batch_size, channels, height, width]\n",
        "    image = image.squeeze(0)  # Remove the extra batch dimension if it's a single image\n",
        "\n",
        "    # Get LRP attributions\n",
        "    attributions, predicted_label = get_lrp_attributions(image, model)\n",
        "\n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    ax[0].imshow(image.squeeze().cpu().numpy(), cmap=\"gray\")\n",
        "    ax[0].set_title(f\"True Label: {label.item()}, Predicted: {predicted_label}\")\n",
        "\n",
        "    # Detach and convert the attributions to numpy for visualization\n",
        "    ax[1].imshow(attributions.squeeze().detach().cpu().numpy(), cmap=\"jet\")\n",
        "    ax[1].set_title(f\"LRP Heatmap\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HF9fUhmc7Vwv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5d3b94b-aa23-4dfc-836e-d62f91537376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/captum/_utils/gradient.py:57: UserWarning:\n",
            "\n",
            "Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_model_layers(model):\n",
        "    \"\"\"\n",
        "    Provide a summary of layers and their neuron/parameter counts\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Model Layer Summary ---\")\n",
        "\n",
        "    # Convolutional Layers\n",
        "    print(f\"Conv1 Layer:\")\n",
        "    print(f\"  Input Channels: {model.conv1.in_channels}\")\n",
        "    print(f\"  Output Channels (Neurons): {model.conv1.out_channels}\")\n",
        "    print(f\"  Kernel Size: {model.conv1.kernel_size}\")\n",
        "\n",
        "    print(f\"\\nConv2 Layer:\")\n",
        "    print(f\"  Input Channels: {model.conv2.in_channels}\")\n",
        "    print(f\"  Output Channels (Neurons): {model.conv2.out_channels}\")\n",
        "    print(f\"  Kernel Size: {model.conv2.kernel_size}\")\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    print(f\"\\nFC1 Layer:\")\n",
        "    print(f\"  Input Features: {model.fc1.in_features}\")\n",
        "    print(f\"  Output Neurons: {model.fc1.out_features}\")\n",
        "\n",
        "    print(f\"\\nFC2 Layer (Output Layer):\")\n",
        "    print(f\"  Input Features: {model.fc2.in_features}\")\n",
        "    print(f\"  Output Neurons (Classes): {model.fc2.out_features}\")\n",
        "\n",
        "    # Total parameter count\n",
        "    def count_parameters(model):\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"\\nTotal Trainable Parameters: {count_parameters(model):,}\")\n",
        "\n",
        "#summarize_model_layers(model)"
      ],
      "metadata": {
        "id": "494g4aEmly_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_per_image_neuron_dataframes(model, test_loader, num_images=10):\n",
        "    \"\"\"\n",
        "    Create comprehensive DataFrames of neuron activations and connections for each image\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Neural network model\n",
        "        test_loader (DataLoader): Test data loader\n",
        "        num_images (int): Number of images to process\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries containing DataFrames for each image\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_images_data = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_idx, (image, label) in enumerate(test_loader):\n",
        "            if img_idx >= num_images:\n",
        "                break\n",
        "\n",
        "            # Prepare image\n",
        "            image = image.to(device)\n",
        "\n",
        "            # Activation and connection storage\n",
        "            neuron_data = {\n",
        "                'conv1': {'activations': [], 'connections': []},\n",
        "                'conv2': {'activations': [], 'connections': []},\n",
        "                'fc1':   {'activations': [], 'connections': []},\n",
        "                'fc2':   {'activations': [], 'connections': []}\n",
        "            }\n",
        "\n",
        "            # Hooks to capture layer details\n",
        "            def create_hook(layer_name):\n",
        "                def hook(module, input, output):\n",
        "                    # Capture activations\n",
        "                    if len(output.shape) == 4:  # Convolutional layer\n",
        "                        # Flatten channel activations\n",
        "                        activations = output.permute(1, 0, 2, 3).reshape(output.shape[1], -1).mean(dim=1)\n",
        "                        neuron_data[layer_name]['activations'] = activations.cpu().numpy()\n",
        "\n",
        "                        # Capture weights as connections\n",
        "                        weights = module.weight.data.cpu().numpy()\n",
        "                        neuron_data[layer_name]['connections'] = weights.reshape(weights.shape[0], -1)\n",
        "\n",
        "                    elif len(output.shape) == 2:  # Fully connected layer\n",
        "                        neuron_data[layer_name]['activations'] = output.cpu().numpy()[0]\n",
        "\n",
        "                        # Capture weights as connections\n",
        "                        weights = module.weight.data.cpu().numpy()\n",
        "                        neuron_data[layer_name]['connections'] = weights\n",
        "                return hook\n",
        "\n",
        "            # Register hooks\n",
        "            hooks = [\n",
        "                model.conv1.register_forward_hook(create_hook('conv1')),\n",
        "                model.conv2.register_forward_hook(create_hook('conv2')),\n",
        "                model.fc1.register_forward_hook(create_hook('fc1')),\n",
        "                model.fc2.register_forward_hook(create_hook('fc2'))\n",
        "            ]\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(image)\n",
        "\n",
        "            # Remove hooks\n",
        "            for hook in hooks:\n",
        "                hook.remove()\n",
        "\n",
        "            # Prepare image-specific DataFrames\n",
        "            image_dataframes = {\n",
        "                'image_index': img_idx,\n",
        "                'true_label': label.item(),\n",
        "                'predicted_label': torch.argmax(output, dim=1).item(),\n",
        "                'dataframes': {}\n",
        "            }\n",
        "\n",
        "            # Create DataFrame for each layer's activations and connections\n",
        "            for layer, data in neuron_data.items():\n",
        "                # Activation DataFrame\n",
        "                act_df = pd.DataFrame({\n",
        "                    'layer': layer,\n",
        "                    'neuron_index': range(len(data['activations'])),\n",
        "                    'activation_value': data['activations']\n",
        "                })\n",
        "\n",
        "                # Connection DataFrame\n",
        "                if len(data['connections'].shape) == 2:\n",
        "                    conn_df = pd.DataFrame(data['connections'])\n",
        "                    conn_df['layer'] = layer\n",
        "                    conn_df['neuron_index'] = range(conn_df.shape[0])\n",
        "\n",
        "                # Store DataFrames\n",
        "                image_dataframes['dataframes'][f'{layer}_activations'] = act_df\n",
        "                image_dataframes['dataframes'][f'{layer}_connections'] = conn_df\n",
        "\n",
        "            # Add to list of image data\n",
        "            all_images_data.append(image_dataframes)\n",
        "\n",
        "    return all_images_data\n",
        "\n",
        "# Run the analysis\n",
        "per_image_neuron_data = create_per_image_neuron_dataframes(model, test_loader)\n",
        "\n",
        "per_image_neuron_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1oSOEkTmb_j",
        "outputId": "2dfa9810-68b6-4c14-e825-ed8376126e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image_index': 0,\n",
              " 'true_label': 1,\n",
              " 'predicted_label': 1,\n",
              " 'dataframes': {'conv1_activations':     layer  neuron_index  activation_value\n",
              "  0   conv1             0          0.362543\n",
              "  1   conv1             1          0.347136\n",
              "  2   conv1             2          0.154901\n",
              "  3   conv1             3          0.090302\n",
              "  4   conv1             4          0.160147\n",
              "  5   conv1             5          0.258511\n",
              "  6   conv1             6          0.025814\n",
              "  7   conv1             7          0.376921\n",
              "  8   conv1             8          0.182516\n",
              "  9   conv1             9          0.182259\n",
              "  10  conv1            10          0.160326\n",
              "  11  conv1            11          0.252715\n",
              "  12  conv1            12          0.014187\n",
              "  13  conv1            13          0.150744\n",
              "  14  conv1            14         -0.014882\n",
              "  15  conv1            15          0.036231,\n",
              "  'conv1_connections':            0         1         2         3         4         5         6  \\\n",
              "  0   0.400412  0.399371  0.071231 -0.067315  0.237489  0.328836 -0.150906   \n",
              "  1   0.062670  0.420839  0.423627 -0.087857  0.270959 -0.074010  0.234360   \n",
              "  2   0.077322  0.239514  0.362599  0.313162 -0.159267 -0.146957 -0.197186   \n",
              "  3  -0.198520  0.416571  0.189112 -0.133381  0.282581  0.187163 -0.016161   \n",
              "  4  -0.100635 -0.007484  0.208964  0.262549  0.069296  0.175801  0.311726   \n",
              "  5  -0.273976 -0.003054  0.224042  0.069278  0.076746  0.157825  0.075656   \n",
              "  6  -0.428757 -0.251183 -0.254338  0.081380  0.144253  0.244236  0.337603   \n",
              "  7   0.378913  0.399986  0.071193  0.172418 -0.026089  0.130204 -0.220100   \n",
              "  8   0.225943  0.192161  0.387705  0.135396 -0.048392 -0.038235 -0.115391   \n",
              "  9   0.307195 -0.180329 -0.171382  0.319853  0.081811  0.181218  0.174932   \n",
              "  10  0.130797  0.370466  0.380331  0.015307 -0.078076  0.016426 -0.083861   \n",
              "  11  0.392175  0.247163  0.281231  0.259225 -0.270856 -0.165327  0.253949   \n",
              "  12 -0.210740 -0.048877 -0.131766  0.335698  0.006075 -0.116493 -0.017947   \n",
              "  13  0.119667  0.026561  0.093274  0.343750  0.204018 -0.236069  0.372014   \n",
              "  14  0.198226 -0.245182  0.121569 -0.049056 -0.133100 -0.077453  0.442164   \n",
              "  15  0.209107  0.173020 -0.301732  0.125146 -0.021763 -0.264287  0.322271   \n",
              "  \n",
              "             7         8  layer  neuron_index  \n",
              "  0   0.392861 -0.045586  conv1             0  \n",
              "  1  -0.093755  0.006661  conv1             1  \n",
              "  2   0.068652  0.012842  conv1             2  \n",
              "  3  -0.195803  0.062113  conv1             3  \n",
              "  4  -0.130634  0.326892  conv1             4  \n",
              "  5   0.073296 -0.115009  conv1             5  \n",
              "  6  -0.101311  0.121107  conv1             6  \n",
              "  7   0.377975 -0.272754  conv1             7  \n",
              "  8   0.228945  0.188063  conv1             8  \n",
              "  9   0.345546  0.172957  conv1             9  \n",
              "  10  0.056517  0.259929  conv1            10  \n",
              "  11 -0.198929  0.022492  conv1            11  \n",
              "  12  0.285077  0.004282  conv1            12  \n",
              "  13  0.204587 -0.130169  conv1            13  \n",
              "  14 -0.012433 -0.308704  conv1            14  \n",
              "  15 -0.069897  0.135285  conv1            15  ,\n",
              "  'conv2_activations':     layer  neuron_index  activation_value\n",
              "  0   conv2             0          0.247010\n",
              "  1   conv2             1          0.861713\n",
              "  2   conv2             2          0.410245\n",
              "  3   conv2             3          1.014889\n",
              "  4   conv2             4         -0.287871\n",
              "  5   conv2             5         -0.302608\n",
              "  6   conv2             6          0.596905\n",
              "  7   conv2             7          0.430440\n",
              "  8   conv2             8          0.257072\n",
              "  9   conv2             9          0.079023\n",
              "  10  conv2            10          0.932231\n",
              "  11  conv2            11          0.395869\n",
              "  12  conv2            12          0.505099\n",
              "  13  conv2            13         -0.011198\n",
              "  14  conv2            14          0.597844\n",
              "  15  conv2            15          0.355485,\n",
              "  'conv2_connections':            0         1         2         3         4         5         6  \\\n",
              "  0   0.053389  0.023715 -0.082655 -0.048249 -0.076035 -0.150770  0.021789   \n",
              "  1   0.047589 -0.093007  0.005160  0.069936  0.136776  0.046387  0.010860   \n",
              "  2   0.058461  0.100847 -0.038368  0.022629  0.103998 -0.064163 -0.030390   \n",
              "  3  -0.035365  0.023469  0.107092  0.101842  0.143467  0.217515 -0.012316   \n",
              "  4   0.062249 -0.131436 -0.046978  0.021597 -0.080085 -0.078712 -0.103583   \n",
              "  5  -0.019309 -0.080785  0.035847  0.020273  0.009665  0.035786 -0.006190   \n",
              "  6   0.049547  0.038741 -0.036449 -0.002058  0.041931  0.053819 -0.005733   \n",
              "  7  -0.152914 -0.041600  0.115807 -0.041148 -0.039541  0.071635 -0.011336   \n",
              "  8   0.125563  0.033356 -0.048423  0.110720 -0.018201 -0.147717 -0.042072   \n",
              "  9  -0.068213 -0.115001 -0.004743 -0.028494 -0.027653  0.079632  0.049930   \n",
              "  10 -0.032338 -0.013617  0.151200 -0.075678  0.075957  0.207855  0.038870   \n",
              "  11  0.051104  0.012186  0.086417  0.028352  0.156211 -0.002386  0.086014   \n",
              "  12  0.082138  0.005539 -0.044524  0.132945  0.106715  0.022446  0.116672   \n",
              "  13 -0.043048 -0.140184  0.012286 -0.060239 -0.119047  0.023106 -0.004832   \n",
              "  14 -0.021613  0.105474  0.025686  0.030302  0.136904 -0.020385 -0.046524   \n",
              "  15  0.117869  0.126556  0.051337  0.002796 -0.029835 -0.061113  0.004300   \n",
              "  \n",
              "             7         8         9  ...       136       137       138       139  \\\n",
              "  0   0.087136  0.136641  0.125749  ...  0.105212  0.008460  0.086248 -0.005925   \n",
              "  1   0.068729 -0.004232 -0.004889  ...  0.075859  0.051621  0.013238  0.045813   \n",
              "  2  -0.050265  0.004671  0.000319  ...  0.094657  0.007707  0.022765  0.004527   \n",
              "  3   0.084479  0.127317 -0.104037  ...  0.000802  0.089539 -0.032844  0.050453   \n",
              "  4   0.048199  0.034387 -0.074277  ...  0.045809 -0.082850  0.027167 -0.046521   \n",
              "  5  -0.040837 -0.020941  0.074329  ... -0.027615 -0.036336 -0.055786  0.008235   \n",
              "  6  -0.068649 -0.066126 -0.024237  ...  0.095926  0.090875 -0.025141  0.018905   \n",
              "  7   0.034141  0.078109 -0.036853  ...  0.086062 -0.007312  0.005437  0.073091   \n",
              "  8   0.026514 -0.127686 -0.001961  ...  0.017386 -0.016188  0.018640  0.010524   \n",
              "  9  -0.042845  0.092674  0.078472  ... -0.005325  0.106815  0.014020 -0.074019   \n",
              "  10 -0.053325  0.046839  0.006819  ...  0.041650  0.170038 -0.044293  0.004112   \n",
              "  11  0.042763 -0.056389 -0.043120  ...  0.042914 -0.032490  0.009025  0.004950   \n",
              "  12  0.132660 -0.116526 -0.137380  ... -0.008512 -0.055690  0.034679  0.085180   \n",
              "  13 -0.028790  0.080668 -0.064381  ... -0.020423  0.075944 -0.070604 -0.053503   \n",
              "  14  0.006731 -0.044295  0.048296  ... -0.026215  0.104971  0.072772  0.041096   \n",
              "  15  0.013864 -0.005772  0.182691  ... -0.027957 -0.073938 -0.020258 -0.038572   \n",
              "  \n",
              "           140       141       142       143  layer  neuron_index  \n",
              "  0  -0.033853  0.113257 -0.040305  0.088088  conv2             0  \n",
              "  1   0.091032  0.005281 -0.026384  0.078543  conv2             1  \n",
              "  2   0.002996  0.085109  0.008654  0.067731  conv2             2  \n",
              "  3  -0.007134  0.030011 -0.008236 -0.030027  conv2             3  \n",
              "  4   0.067527 -0.041539  0.057837  0.032542  conv2             4  \n",
              "  5  -0.075360 -0.063744  0.026530  0.010399  conv2             5  \n",
              "  6   0.031970 -0.055053  0.091992 -0.061894  conv2             6  \n",
              "  7   0.030808 -0.011768  0.093748  0.089395  conv2             7  \n",
              "  8  -0.089284  0.094356  0.033373  0.012280  conv2             8  \n",
              "  9   0.011533 -0.078723  0.016492 -0.013510  conv2             9  \n",
              "  10  0.017894 -0.050105  0.031401  0.045740  conv2            10  \n",
              "  11  0.000354  0.074826  0.052800  0.083592  conv2            11  \n",
              "  12  0.014633  0.005773  0.066127 -0.082251  conv2            12  \n",
              "  13  0.020353 -0.001231  0.079536  0.032849  conv2            13  \n",
              "  14  0.005841  0.030748 -0.013547 -0.054549  conv2            14  \n",
              "  15 -0.075074  0.053019 -0.002690 -0.082208  conv2            15  \n",
              "  \n",
              "  [16 rows x 146 columns],\n",
              "  'fc1_activations':    layer  neuron_index  activation_value\n",
              "  0    fc1             0         -4.456444\n",
              "  1    fc1             1         -4.267042\n",
              "  2    fc1             2        -11.516699\n",
              "  3    fc1             3         -4.154439\n",
              "  4    fc1             4         -5.867033\n",
              "  5    fc1             5          6.849453\n",
              "  6    fc1             6         -5.162141\n",
              "  7    fc1             7          4.395796\n",
              "  8    fc1             8         -0.609752\n",
              "  9    fc1             9         12.551845\n",
              "  10   fc1            10         -4.676147\n",
              "  11   fc1            11         -6.131161\n",
              "  12   fc1            12         17.422287\n",
              "  13   fc1            13         31.487080\n",
              "  14   fc1            14         -5.659576\n",
              "  15   fc1            15        -13.189085\n",
              "  16   fc1            16         -6.006960\n",
              "  17   fc1            17         -4.983891\n",
              "  18   fc1            18          3.676079\n",
              "  19   fc1            19          9.194911\n",
              "  20   fc1            20         26.456285\n",
              "  21   fc1            21         -9.694846\n",
              "  22   fc1            22        -12.955977\n",
              "  23   fc1            23         11.109831\n",
              "  24   fc1            24         -6.353340\n",
              "  25   fc1            25         -4.115302\n",
              "  26   fc1            26         -5.134408\n",
              "  27   fc1            27         -3.119771\n",
              "  28   fc1            28         -5.805737\n",
              "  29   fc1            29         11.130728\n",
              "  30   fc1            30         -5.984334\n",
              "  31   fc1            31        -13.703977\n",
              "  32   fc1            32         -7.644935\n",
              "  33   fc1            33         12.571311\n",
              "  34   fc1            34         -5.916789\n",
              "  35   fc1            35         19.704357\n",
              "  36   fc1            36         38.756954\n",
              "  37   fc1            37         17.854876\n",
              "  38   fc1            38          3.539701\n",
              "  39   fc1            39        -15.611642\n",
              "  40   fc1            40          6.480339\n",
              "  41   fc1            41         -4.732583\n",
              "  42   fc1            42         -4.810484\n",
              "  43   fc1            43         -4.283399\n",
              "  44   fc1            44         -5.013291\n",
              "  45   fc1            45         -5.690193\n",
              "  46   fc1            46         -4.625062\n",
              "  47   fc1            47         -3.662933\n",
              "  48   fc1            48         -3.836876\n",
              "  49   fc1            49         39.360073\n",
              "  50   fc1            50          9.538559\n",
              "  51   fc1            51         -3.134629\n",
              "  52   fc1            52         22.300701\n",
              "  53   fc1            53         -4.558513\n",
              "  54   fc1            54         -3.165549\n",
              "  55   fc1            55         -6.995828\n",
              "  56   fc1            56         -3.448275\n",
              "  57   fc1            57          4.120663\n",
              "  58   fc1            58         -3.984741\n",
              "  59   fc1            59         -6.052259,\n",
              "  'fc1_connections':            0         1         2         3         4         5         6  \\\n",
              "  0   0.012868  0.012967 -0.004651  0.004214  0.004450 -0.004286 -0.006892   \n",
              "  1  -0.006668  0.001739  0.008233 -0.015734 -0.018150  0.006245 -0.009190   \n",
              "  2  -0.004715 -0.008510 -0.027038 -0.012099 -0.017072 -0.001276 -0.004790   \n",
              "  3  -0.022022  0.006158 -0.000070  0.005327 -0.010999 -0.013339 -0.000802   \n",
              "  4  -0.021747  0.005062 -0.007141 -0.002509  0.002286 -0.004356 -0.013751   \n",
              "  5   0.000488  0.012703  0.014533  0.016341 -0.004459  0.019696  0.002125   \n",
              "  6   0.009300 -0.022666 -0.019488  0.000888 -0.000055 -0.011650 -0.015693   \n",
              "  7   0.031482  0.014408  0.003062  0.002343  0.027174  0.003222  0.007536   \n",
              "  8  -0.021455 -0.018184  0.010642 -0.009969 -0.002964 -0.022868  0.002633   \n",
              "  9   0.008664  0.005595  0.018707  0.036951  0.012074  0.024569  0.035212   \n",
              "  10 -0.015157 -0.015687 -0.020529  0.011342  0.001704 -0.014001 -0.023534   \n",
              "  11  0.033904  0.009318  0.023171  0.003949  0.003715  0.000735  0.023219   \n",
              "  12  0.024346  0.032305  0.026967  0.025876  0.010165  0.022697  0.011940   \n",
              "  13  0.008851  0.010857  0.002167  0.024771  0.005659  0.021011  0.000578   \n",
              "  14 -0.009209 -0.003457 -0.008076 -0.003801  0.010076 -0.013077 -0.018169   \n",
              "  15 -0.022892 -0.007363 -0.001998 -0.025108 -0.005332 -0.008478 -0.006340   \n",
              "  16 -0.011208 -0.008056 -0.020657 -0.002330  0.002901 -0.009348 -0.019556   \n",
              "  17 -0.004303 -0.011543 -0.022747 -0.002148 -0.000974  0.003379  0.001518   \n",
              "  18  0.014590  0.013593  0.026992  0.014942  0.002147  0.024546  0.023784   \n",
              "  19  0.013653  0.015041  0.026944  0.017540  0.012362  0.005006  0.020771   \n",
              "  20  0.000926 -0.002540  0.000976 -0.007426  0.008134  0.009121  0.006932   \n",
              "  21 -0.008481 -0.009680 -0.015762 -0.008213  0.005569 -0.011419 -0.018695   \n",
              "  22  0.002687 -0.004754  0.025822  0.022829  0.026687  0.024950  0.015143   \n",
              "  23  0.012675 -0.008788  0.020707 -0.002437 -0.000562 -0.001121  0.009253   \n",
              "  24 -0.021873 -0.006459  0.000207 -0.022165  0.008027 -0.006639  0.011209   \n",
              "  25 -0.009614 -0.022043 -0.007360  0.004037 -0.017339 -0.001730 -0.005209   \n",
              "  26 -0.003378 -0.006826 -0.019353 -0.008899  0.007771  0.007703 -0.008105   \n",
              "  27 -0.009939 -0.008868  0.000785  0.008521 -0.017249  0.009543 -0.006277   \n",
              "  28 -0.012323  0.002773 -0.002832  0.008380 -0.001049 -0.023035 -0.010383   \n",
              "  29  0.014561  0.016938  0.017683 -0.001824  0.010139  0.014729  0.012067   \n",
              "  30 -0.019001 -0.009498 -0.020953 -0.004602  0.011816  0.005326 -0.018519   \n",
              "  31  0.000243  0.000025  0.021961 -0.006239  0.018972 -0.004205  0.002377   \n",
              "  32 -0.003725 -0.003355 -0.020392  0.005503 -0.013929 -0.013021  0.001984   \n",
              "  33  0.016569  0.002955  0.006740  0.025918  0.010518  0.006316  0.009447   \n",
              "  34 -0.021455 -0.010343  0.011155  0.004942 -0.004358 -0.012782 -0.010427   \n",
              "  35 -0.004436 -0.022906 -0.004501  0.005599  0.007464 -0.000397 -0.000361   \n",
              "  36  0.028495  0.008049  0.033686  0.027099  0.001744  0.033556  0.005032   \n",
              "  37  0.017607  0.014472  0.003794  0.007778  0.026253  0.026272  0.000011   \n",
              "  38 -0.002645  0.017745  0.021209 -0.002706  0.004354  0.021500  0.015463   \n",
              "  39  0.013660  0.008610  0.018474  0.003934  0.033713  0.001638  0.016512   \n",
              "  40  0.030918  0.000622  0.009956  0.019433  0.000461 -0.000907  0.013701   \n",
              "  41 -0.014202 -0.010007 -0.013808 -0.019121  0.001727 -0.010824  0.006965   \n",
              "  42 -0.018572 -0.000763  0.011354 -0.020072 -0.000430  0.000737 -0.016534   \n",
              "  43 -0.016530 -0.011671 -0.013214 -0.002635  0.008195 -0.009454 -0.014988   \n",
              "  44 -0.004162  0.004954 -0.016971 -0.016766 -0.005005 -0.002056 -0.009664   \n",
              "  45 -0.006126  0.000294 -0.010289 -0.013374 -0.021398 -0.016979 -0.005077   \n",
              "  46 -0.003019  0.003975 -0.013475  0.004994 -0.012704 -0.002466 -0.018315   \n",
              "  47  0.002745  0.005883 -0.017230  0.000825 -0.021160 -0.008062 -0.022425   \n",
              "  48 -0.000349 -0.019200  0.004244  0.012784  0.002108 -0.011295 -0.014164   \n",
              "  49  0.019498  0.027400  0.033523  0.031643  0.003375  0.024539  0.018732   \n",
              "  50  0.002034  0.025831  0.021195  0.015002  0.011998  0.006490  0.015592   \n",
              "  51  0.002681 -0.004163 -0.002470  0.002025  0.000847 -0.004836 -0.020047   \n",
              "  52  0.031499  0.006817  0.030368  0.016483  0.009225  0.011744  0.023579   \n",
              "  53 -0.002231 -0.016763 -0.012501  0.002132 -0.016919  0.000812  0.009893   \n",
              "  54 -0.016136  0.002641  0.006465  0.005333  0.000611 -0.021958  0.005562   \n",
              "  55  0.006558  0.001421 -0.006817 -0.021912 -0.021949 -0.012012  0.008985   \n",
              "  56 -0.013326  0.005539 -0.001117  0.010829  0.002428 -0.019694 -0.017891   \n",
              "  57  0.000068  0.003597  0.006211  0.004677 -0.000560  0.023743  0.012332   \n",
              "  58  0.028027  0.022394  0.015621  0.022584  0.027930  0.013303  0.023036   \n",
              "  59 -0.016706  0.001549  0.002360 -0.005833 -0.013946 -0.005872 -0.015406   \n",
              "  \n",
              "             7         8         9  ...      3128      3129      3130      3131  \\\n",
              "  0   0.007324 -0.009616 -0.005067  ... -0.006856  0.010652  0.010737 -0.005627   \n",
              "  1  -0.018485 -0.011243 -0.014907  ...  0.003302 -0.023178 -0.002841 -0.002705   \n",
              "  2   0.003348 -0.021924  0.002252  ... -0.004555 -0.022496 -0.027291  0.000060   \n",
              "  3  -0.006648 -0.017124  0.011142  ... -0.008438  0.007751 -0.003895 -0.014379   \n",
              "  4   0.009772 -0.011616 -0.003061  ... -0.013893 -0.023509  0.007052  0.009660   \n",
              "  5   0.008894  0.007604  0.022646  ...  0.023266  0.041666  0.029488  0.033174   \n",
              "  6  -0.003789  0.009797 -0.006847  ...  0.009045 -0.017039 -0.017816  0.009705   \n",
              "  7   0.004008  0.015966  0.020602  ...  0.009192  0.034439  0.014094  0.028504   \n",
              "  8  -0.003198 -0.022776  0.003764  ... -0.001452 -0.006950 -0.022771  0.003933   \n",
              "  9   0.030930  0.004664  0.003770  ...  0.004832 -0.009257 -0.008188  0.001435   \n",
              "  10 -0.013386 -0.013077 -0.018894  ... -0.001648  0.000762 -0.017888 -0.016899   \n",
              "  11  0.006491  0.032536  0.000740  ...  0.003196  0.004032  0.010177  0.012247   \n",
              "  12  0.025155  0.015860  0.000769  ...  0.006513  0.015600  0.012484  0.016776   \n",
              "  13  0.029623  0.026839 -0.001954  ...  0.036832  0.027887  0.015517  0.010580   \n",
              "  14 -0.016337  0.004727  0.010331  ... -0.012719 -0.016295  0.002682 -0.014291   \n",
              "  15 -0.017671 -0.001590 -0.007524  ... -0.027042 -0.015744 -0.009734 -0.021105   \n",
              "  16 -0.015554  0.001033  0.005761  ...  0.009729 -0.022332 -0.021350 -0.016759   \n",
              "  17  0.010020 -0.019688 -0.004400  ...  0.007721 -0.004402 -0.016819 -0.015915   \n",
              "  18  0.002784 -0.006211  0.013223  ...  0.024819  0.008977  0.035165  0.001899   \n",
              "  19  0.014684  0.027412  0.018771  ...  0.027893  0.023726  0.017920  0.016699   \n",
              "  20 -0.019416 -0.022032 -0.013255  ...  0.047160  0.036355  0.025181  0.022490   \n",
              "  21  0.001027 -0.005196 -0.004424  ... -0.011853 -0.003937  0.003567  0.000304   \n",
              "  22  0.004678  0.017792 -0.000148  ...  0.014265  0.008728 -0.017063  0.005836   \n",
              "  23  0.005949 -0.008079  0.000984  ... -0.013547 -0.005859  0.002466  0.003944   \n",
              "  24 -0.012757 -0.023823 -0.018663  ...  0.011678 -0.019435  0.003086 -0.019769   \n",
              "  25 -0.010399  0.010380 -0.019753  ...  0.000844  0.010552 -0.019871 -0.019154   \n",
              "  26  0.010319 -0.010890  0.007982  ...  0.000887  0.000956  0.001107 -0.014635   \n",
              "  27 -0.004731 -0.009723 -0.003905  ...  0.005012 -0.016505 -0.011187 -0.001365   \n",
              "  28 -0.015085 -0.001597 -0.002212  ... -0.009379  0.010932  0.000835  0.011187   \n",
              "  29  0.009418  0.025151  0.020776  ... -0.004506 -0.015641 -0.013120 -0.010891   \n",
              "  30 -0.016916  0.009760 -0.014054  ...  0.007682 -0.003761 -0.015353 -0.000961   \n",
              "  31  0.012016 -0.000764 -0.007299  ... -0.012403 -0.009862 -0.011390 -0.003945   \n",
              "  32 -0.024166  0.000700  0.009655  ... -0.019903 -0.006628 -0.020596  0.001379   \n",
              "  33  0.010933  0.003280  0.013948  ... -0.003015  0.020623 -0.000906  0.015280   \n",
              "  34 -0.017208  0.006274  0.003570  ... -0.013828 -0.008093 -0.019012 -0.001721   \n",
              "  35  0.005101 -0.010963 -0.009127  ... -0.002773  0.011193  0.019547  0.002818   \n",
              "  36  0.012425  0.009884  0.031366  ...  0.042560  0.039935  0.023204  0.029524   \n",
              "  37  0.023419  0.006894  0.026621  ...  0.004115  0.000018  0.020470 -0.002871   \n",
              "  38  0.018896  0.024307 -0.000390  ... -0.004216 -0.012399 -0.001571 -0.018753   \n",
              "  39  0.010963  0.030405  0.032920  ... -0.013158  0.001995 -0.017575  0.000686   \n",
              "  40  0.026295  0.013387 -0.001529  ...  0.043029  0.038009  0.020855  0.017207   \n",
              "  41 -0.004680 -0.013764 -0.007863  ...  0.008035  0.010515 -0.006477  0.011474   \n",
              "  42 -0.000678  0.009945  0.003108  ...  0.008813  0.006511 -0.001635 -0.017173   \n",
              "  43 -0.011972 -0.016392  0.001932  ... -0.010337 -0.012674 -0.018367 -0.022919   \n",
              "  44  0.003259 -0.000585  0.001807  ...  0.008413 -0.014572 -0.008983  0.005895   \n",
              "  45  0.006896 -0.008086 -0.009275  ... -0.005778  0.007000  0.002406 -0.008275   \n",
              "  46  0.010720 -0.002166 -0.014401  ... -0.013205 -0.022235 -0.002017 -0.000171   \n",
              "  47 -0.005961 -0.003954  0.010139  ...  0.011701  0.001282 -0.018512 -0.003118   \n",
              "  48 -0.013545  0.006565 -0.005001  ... -0.010218  0.010282 -0.019655  0.010408   \n",
              "  49  0.018647  0.026510  0.016471  ...  0.039909  0.049060  0.028912  0.045582   \n",
              "  50  0.000300  0.010269  0.008490  ...  0.023184 -0.001352  0.019045  0.003499   \n",
              "  51  0.004892  0.007555  0.001219  ... -0.016053  0.001421 -0.013407 -0.006220   \n",
              "  52  0.030600  0.026217  0.023260  ...  0.017529  0.006934  0.017406  0.020949   \n",
              "  53 -0.010024 -0.009774 -0.018796  ... -0.015735 -0.020443  0.008829 -0.016565   \n",
              "  54  0.011039 -0.000971 -0.004749  ... -0.011213  0.000786 -0.019802 -0.001971   \n",
              "  55 -0.017711 -0.011881  0.008669  ...  0.003574  0.005007 -0.005511 -0.009074   \n",
              "  56 -0.018805 -0.010618  0.002681  ... -0.014544 -0.013959  0.007968 -0.004889   \n",
              "  57  0.013836  0.000618  0.024202  ...  0.020163  0.034249  0.023702  0.032346   \n",
              "  58  0.020271 -0.000923  0.004961  ...  0.030553  0.023524  0.025896  0.012225   \n",
              "  59  0.006506 -0.000810  0.000023  ... -0.021418 -0.022299 -0.008617 -0.017246   \n",
              "  \n",
              "          3132      3133      3134      3135  layer  neuron_index  \n",
              "  0  -0.006300  0.000761 -0.010077 -0.010894    fc1             0  \n",
              "  1   0.007173  0.003588 -0.009852 -0.016465    fc1             1  \n",
              "  2   0.005837 -0.015450 -0.013901  0.002791    fc1             2  \n",
              "  3  -0.012918  0.007607 -0.013114 -0.016117    fc1             3  \n",
              "  4   0.003922 -0.010699  0.009847 -0.021692    fc1             4  \n",
              "  5   0.028678  0.014802  0.011530  0.006059    fc1             5  \n",
              "  6  -0.014932  0.004915  0.002246 -0.007906    fc1             6  \n",
              "  7   0.013501  0.022247  0.021532 -0.003630    fc1             7  \n",
              "  8  -0.006297 -0.015515 -0.013881  0.004368    fc1             8  \n",
              "  9  -0.001923  0.008458  0.021580  0.029945    fc1             9  \n",
              "  10 -0.000172 -0.004697 -0.007486  0.003054    fc1            10  \n",
              "  11 -0.007124 -0.002595 -0.013320 -0.004145    fc1            11  \n",
              "  12 -0.001142  0.025493  0.015860  0.012417    fc1            12  \n",
              "  13  0.008905  0.014851  0.010457  0.007798    fc1            13  \n",
              "  14 -0.010744 -0.008016 -0.019082 -0.007001    fc1            14  \n",
              "  15 -0.008899 -0.000829 -0.016586 -0.028002    fc1            15  \n",
              "  16  0.011897 -0.018047 -0.002446 -0.017222    fc1            16  \n",
              "  17 -0.020536  0.006682 -0.007786 -0.019112    fc1            17  \n",
              "  18  0.014848  0.023633  0.009814 -0.009907    fc1            18  \n",
              "  19  0.032436  0.014681  0.007802  0.015888    fc1            19  \n",
              "  20  0.010047  0.026064  0.016482  0.030144    fc1            20  \n",
              "  21  0.008994  0.010006 -0.018727 -0.003075    fc1            21  \n",
              "  22 -0.000216 -0.005094  0.018505 -0.008893    fc1            22  \n",
              "  23  0.027010  0.004446 -0.004685  0.011521    fc1            23  \n",
              "  24 -0.006039  0.005686 -0.007776  0.007923    fc1            24  \n",
              "  25 -0.013028 -0.005355 -0.013795  0.000189    fc1            25  \n",
              "  26  0.001260  0.002817 -0.000567 -0.018858    fc1            26  \n",
              "  27 -0.012066  0.005803  0.008898 -0.020256    fc1            27  \n",
              "  28 -0.021011 -0.016277 -0.021642 -0.017041    fc1            28  \n",
              "  29  0.015582 -0.001358  0.009005  0.020085    fc1            29  \n",
              "  30  0.009142 -0.011048 -0.019065 -0.022964    fc1            30  \n",
              "  31 -0.017578  0.018184 -0.014494  0.006414    fc1            31  \n",
              "  32 -0.007214 -0.020134 -0.002980 -0.009061    fc1            32  \n",
              "  33 -0.002171 -0.004688  0.024596  0.012860    fc1            33  \n",
              "  34  0.006539  0.009605 -0.015453 -0.016520    fc1            34  \n",
              "  35 -0.018720 -0.022744 -0.017856  0.009409    fc1            35  \n",
              "  36  0.040159  0.015496  0.046070  0.038217    fc1            36  \n",
              "  37  0.016869  0.005396  0.012609  0.013487    fc1            37  \n",
              "  38 -0.000505  0.023441  0.003526  0.005332    fc1            38  \n",
              "  39 -0.019023 -0.006869 -0.015280  0.005979    fc1            39  \n",
              "  40  0.027607  0.004566  0.021181  0.012148    fc1            40  \n",
              "  41  0.004879 -0.012685 -0.009628 -0.022963    fc1            41  \n",
              "  42  0.007932 -0.012811 -0.015902 -0.008890    fc1            42  \n",
              "  43 -0.016014  0.007799  0.005429 -0.008904    fc1            43  \n",
              "  44  0.004329  0.007101 -0.003699 -0.012319    fc1            44  \n",
              "  45  0.005780  0.009909 -0.004622 -0.021927    fc1            45  \n",
              "  46 -0.013934 -0.011455 -0.007828 -0.010529    fc1            46  \n",
              "  47 -0.016764 -0.023845 -0.003573  0.011422    fc1            47  \n",
              "  48 -0.015247 -0.011081 -0.016139 -0.007430    fc1            48  \n",
              "  49  0.037967  0.039943  0.047666  0.032830    fc1            49  \n",
              "  50  0.002823 -0.002692 -0.008353  0.002865    fc1            50  \n",
              "  51 -0.014761  0.009049 -0.008702 -0.023180    fc1            51  \n",
              "  52  0.023632  0.022728  0.018886  0.025511    fc1            52  \n",
              "  53 -0.018722  0.011494  0.004309 -0.023839    fc1            53  \n",
              "  54  0.000339  0.006231  0.007703 -0.013026    fc1            54  \n",
              "  55 -0.014757 -0.008799 -0.020483 -0.005706    fc1            55  \n",
              "  56 -0.014795  0.005220 -0.021989 -0.015774    fc1            56  \n",
              "  57 -0.000363  0.006102 -0.002093 -0.001637    fc1            57  \n",
              "  58  0.026156  0.011964  0.002414  0.006485    fc1            58  \n",
              "  59 -0.003636  0.008666 -0.018831 -0.015422    fc1            59  \n",
              "  \n",
              "  [60 rows x 3138 columns],\n",
              "  'fc2_activations':   layer  neuron_index  activation_value\n",
              "  0   fc2             0        -13.368244\n",
              "  1   fc2             1         16.461422\n",
              "  2   fc2             2         -3.920114\n",
              "  3   fc2             3         -9.976067\n",
              "  4   fc2             4        -12.454081\n",
              "  5   fc2             5        -13.399258\n",
              "  6   fc2             6        -13.740579\n",
              "  7   fc2             7         -5.318144\n",
              "  8   fc2             8        -14.564055\n",
              "  9   fc2             9         15.788969,\n",
              "  'fc2_connections':           0         1         2         3         4         5         6  \\\n",
              "  0 -0.107950 -0.100333 -0.125853  0.091329 -0.090712 -0.024337 -0.035301   \n",
              "  1  0.131501 -0.110936 -0.085158  0.066024 -0.037845 -0.071343 -0.052721   \n",
              "  2  0.050709 -0.033907 -0.077746 -0.074659 -0.076966 -0.074498 -0.088701   \n",
              "  3  0.049182 -0.027318 -0.033456 -0.035291 -0.043763 -0.003068  0.013614   \n",
              "  4  0.042645  0.088435  0.038246  0.004014 -0.031166 -0.035241 -0.033445   \n",
              "  5  0.086447 -0.126483 -0.040570  0.021679  0.066402 -0.114365  0.069705   \n",
              "  6 -0.080449  0.085518 -0.090617 -0.053715 -0.023148 -0.005910  0.053310   \n",
              "  7  0.092976 -0.108292 -0.003311 -0.020869  0.013422 -0.048318  0.010532   \n",
              "  8 -0.123973 -0.038942  0.098014 -0.079174  0.093177  0.066027  0.115322   \n",
              "  9  0.017517 -0.083782  0.037875 -0.062554 -0.064439  0.184223  0.006078   \n",
              "  \n",
              "            7         8         9  ...        52        53        54        55  \\\n",
              "  0  0.090453 -0.134711  0.053470  ... -0.070573  0.083195  0.095116  0.083355   \n",
              "  1 -0.112283  0.045542  0.064597  ...  0.047196  0.127483  0.055454  0.038554   \n",
              "  2  0.057485 -0.012508 -0.048672  ...  0.098187  0.072752 -0.079741 -0.097680   \n",
              "  3 -0.134513 -0.108175  0.041074  ... -0.005931 -0.089947 -0.010503  0.077362   \n",
              "  4 -0.040576  0.073950 -0.058569  ... -0.079893 -0.038126 -0.095315  0.112316   \n",
              "  5 -0.126576 -0.152112 -0.069325  ... -0.028109  0.103588 -0.003243  0.043243   \n",
              "  6  0.102617  0.096054  0.003210  ... -0.036933  0.010865  0.066728  0.112904   \n",
              "  7  0.093271  0.076127 -0.004166  ... -0.105497  0.043385  0.102613  0.121523   \n",
              "  8  0.016770 -0.150593  0.057095  ... -0.014134 -0.041150  0.043610 -0.033362   \n",
              "  9  0.090238 -0.031772 -0.016623  ... -0.092037  0.067543 -0.015439  0.107121   \n",
              "  \n",
              "           56        57        58        59  layer  neuron_index  \n",
              "  0 -0.068317  0.057229 -0.010008 -0.046776    fc2             0  \n",
              "  1  0.154287 -0.029886 -0.088726  0.017459    fc2             1  \n",
              "  2 -0.002260 -0.142917  0.077585  0.014963    fc2             2  \n",
              "  3 -0.088103 -0.110321  0.010353  0.046034    fc2             3  \n",
              "  4  0.099081  0.096792  0.056523 -0.063165    fc2             4  \n",
              "  5 -0.133819 -0.110977 -0.123005 -0.015103    fc2             5  \n",
              "  6  0.097748 -0.037395  0.050451 -0.001918    fc2             6  \n",
              "  7 -0.045865 -0.148744  0.003459 -0.012868    fc2             7  \n",
              "  8 -0.111366 -0.025910  0.000013  0.098205    fc2             8  \n",
              "  9 -0.079098  0.062496 -0.018937  0.063364    fc2             9  \n",
              "  \n",
              "  [10 rows x 62 columns]}}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "per_image_neuron_data[3]['dataframes']['conv2_activations']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "Qeqv0_gdie69",
        "outputId": "e82eca52-9828-4adf-b69e-badab6d30a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    layer  neuron_index  activation_value\n",
              "0   conv2             0          0.070487\n",
              "1   conv2             1          0.445887\n",
              "2   conv2             2          0.160948\n",
              "3   conv2             3          0.557783\n",
              "4   conv2             4         -0.166093\n",
              "5   conv2             5         -0.234721\n",
              "6   conv2             6          0.333487\n",
              "7   conv2             7          0.272461\n",
              "8   conv2             8          0.156724\n",
              "9   conv2             9          0.061776\n",
              "10  conv2            10          0.475569\n",
              "11  conv2            11          0.157494\n",
              "12  conv2            12          0.291480\n",
              "13  conv2            13          0.029561\n",
              "14  conv2            14          0.280187\n",
              "15  conv2            15          0.192662"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f21d5cbb-491a-49a6-910b-381fe5d1e256\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>layer</th>\n",
              "      <th>neuron_index</th>\n",
              "      <th>activation_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>conv2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.070487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>conv2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.445887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>conv2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.160948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>conv2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.557783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>conv2</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.166093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>conv2</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.234721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>conv2</td>\n",
              "      <td>6</td>\n",
              "      <td>0.333487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>conv2</td>\n",
              "      <td>7</td>\n",
              "      <td>0.272461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>conv2</td>\n",
              "      <td>8</td>\n",
              "      <td>0.156724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>conv2</td>\n",
              "      <td>9</td>\n",
              "      <td>0.061776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>conv2</td>\n",
              "      <td>10</td>\n",
              "      <td>0.475569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>conv2</td>\n",
              "      <td>11</td>\n",
              "      <td>0.157494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>conv2</td>\n",
              "      <td>12</td>\n",
              "      <td>0.291480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>conv2</td>\n",
              "      <td>13</td>\n",
              "      <td>0.029561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>conv2</td>\n",
              "      <td>14</td>\n",
              "      <td>0.280187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>conv2</td>\n",
              "      <td>15</td>\n",
              "      <td>0.192662</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f21d5cbb-491a-49a6-910b-381fe5d1e256')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f21d5cbb-491a-49a6-910b-381fe5d1e256 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f21d5cbb-491a-49a6-910b-381fe5d1e256');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-aa1afe71-2b86-4ac3-98da-32f5ea524092\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aa1afe71-2b86-4ac3-98da-32f5ea524092')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-aa1afe71-2b86-4ac3-98da-32f5ea524092 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"per_image_neuron_data[3]['dataframes']['conv2_activations']\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"layer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"conv2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neuron_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 15,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"activation_value\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.07048669457435608\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "per_image_neuron_data[3]['dataframes']['conv2_connections']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "nK8Bp98KjCv0",
        "outputId": "c7eb5f7f-5d35-4ad9-de25-bd0efcea38be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6  \\\n",
              "0   0.053389  0.023715 -0.082655 -0.048249 -0.076035 -0.150770  0.021789   \n",
              "1   0.047589 -0.093007  0.005160  0.069936  0.136776  0.046387  0.010860   \n",
              "2   0.058461  0.100847 -0.038368  0.022629  0.103998 -0.064163 -0.030390   \n",
              "3  -0.035365  0.023469  0.107092  0.101842  0.143467  0.217515 -0.012316   \n",
              "4   0.062249 -0.131436 -0.046978  0.021597 -0.080085 -0.078712 -0.103583   \n",
              "5  -0.019309 -0.080785  0.035847  0.020273  0.009665  0.035786 -0.006190   \n",
              "6   0.049547  0.038741 -0.036449 -0.002058  0.041931  0.053819 -0.005733   \n",
              "7  -0.152914 -0.041600  0.115807 -0.041148 -0.039541  0.071635 -0.011336   \n",
              "8   0.125563  0.033356 -0.048423  0.110720 -0.018201 -0.147717 -0.042072   \n",
              "9  -0.068213 -0.115001 -0.004743 -0.028494 -0.027653  0.079632  0.049930   \n",
              "10 -0.032338 -0.013617  0.151200 -0.075678  0.075957  0.207855  0.038870   \n",
              "11  0.051104  0.012186  0.086417  0.028352  0.156211 -0.002386  0.086014   \n",
              "12  0.082138  0.005539 -0.044524  0.132945  0.106715  0.022446  0.116672   \n",
              "13 -0.043048 -0.140184  0.012286 -0.060239 -0.119047  0.023106 -0.004832   \n",
              "14 -0.021613  0.105474  0.025686  0.030302  0.136904 -0.020385 -0.046524   \n",
              "15  0.117869  0.126556  0.051337  0.002796 -0.029835 -0.061113  0.004300   \n",
              "\n",
              "           7         8         9  ...       136       137       138       139  \\\n",
              "0   0.087136  0.136641  0.125749  ...  0.105212  0.008460  0.086248 -0.005925   \n",
              "1   0.068729 -0.004232 -0.004889  ...  0.075859  0.051621  0.013238  0.045813   \n",
              "2  -0.050265  0.004671  0.000319  ...  0.094657  0.007707  0.022765  0.004527   \n",
              "3   0.084479  0.127317 -0.104037  ...  0.000802  0.089539 -0.032844  0.050453   \n",
              "4   0.048199  0.034387 -0.074277  ...  0.045809 -0.082850  0.027167 -0.046521   \n",
              "5  -0.040837 -0.020941  0.074329  ... -0.027615 -0.036336 -0.055786  0.008235   \n",
              "6  -0.068649 -0.066126 -0.024237  ...  0.095926  0.090875 -0.025141  0.018905   \n",
              "7   0.034141  0.078109 -0.036853  ...  0.086062 -0.007312  0.005437  0.073091   \n",
              "8   0.026514 -0.127686 -0.001961  ...  0.017386 -0.016188  0.018640  0.010524   \n",
              "9  -0.042845  0.092674  0.078472  ... -0.005325  0.106815  0.014020 -0.074019   \n",
              "10 -0.053325  0.046839  0.006819  ...  0.041650  0.170038 -0.044293  0.004112   \n",
              "11  0.042763 -0.056389 -0.043120  ...  0.042914 -0.032490  0.009025  0.004950   \n",
              "12  0.132660 -0.116526 -0.137380  ... -0.008512 -0.055690  0.034679  0.085180   \n",
              "13 -0.028790  0.080668 -0.064381  ... -0.020423  0.075944 -0.070604 -0.053503   \n",
              "14  0.006731 -0.044295  0.048296  ... -0.026215  0.104971  0.072772  0.041096   \n",
              "15  0.013864 -0.005772  0.182691  ... -0.027957 -0.073938 -0.020258 -0.038572   \n",
              "\n",
              "         140       141       142       143  layer  neuron_index  \n",
              "0  -0.033853  0.113257 -0.040305  0.088088  conv2             0  \n",
              "1   0.091032  0.005281 -0.026384  0.078543  conv2             1  \n",
              "2   0.002996  0.085109  0.008654  0.067731  conv2             2  \n",
              "3  -0.007134  0.030011 -0.008236 -0.030027  conv2             3  \n",
              "4   0.067527 -0.041539  0.057837  0.032542  conv2             4  \n",
              "5  -0.075360 -0.063744  0.026530  0.010399  conv2             5  \n",
              "6   0.031970 -0.055053  0.091992 -0.061894  conv2             6  \n",
              "7   0.030808 -0.011768  0.093748  0.089395  conv2             7  \n",
              "8  -0.089284  0.094356  0.033373  0.012280  conv2             8  \n",
              "9   0.011533 -0.078723  0.016492 -0.013510  conv2             9  \n",
              "10  0.017894 -0.050105  0.031401  0.045740  conv2            10  \n",
              "11  0.000354  0.074826  0.052800  0.083592  conv2            11  \n",
              "12  0.014633  0.005773  0.066127 -0.082251  conv2            12  \n",
              "13  0.020353 -0.001231  0.079536  0.032849  conv2            13  \n",
              "14  0.005841  0.030748 -0.013547 -0.054549  conv2            14  \n",
              "15 -0.075074  0.053019 -0.002690 -0.082208  conv2            15  \n",
              "\n",
              "[16 rows x 146 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e47deed9-f2bf-4227-ad53-6b1d77cc808a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>layer</th>\n",
              "      <th>neuron_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.053389</td>\n",
              "      <td>0.023715</td>\n",
              "      <td>-0.082655</td>\n",
              "      <td>-0.048249</td>\n",
              "      <td>-0.076035</td>\n",
              "      <td>-0.150770</td>\n",
              "      <td>0.021789</td>\n",
              "      <td>0.087136</td>\n",
              "      <td>0.136641</td>\n",
              "      <td>0.125749</td>\n",
              "      <td>...</td>\n",
              "      <td>0.105212</td>\n",
              "      <td>0.008460</td>\n",
              "      <td>0.086248</td>\n",
              "      <td>-0.005925</td>\n",
              "      <td>-0.033853</td>\n",
              "      <td>0.113257</td>\n",
              "      <td>-0.040305</td>\n",
              "      <td>0.088088</td>\n",
              "      <td>conv2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.047589</td>\n",
              "      <td>-0.093007</td>\n",
              "      <td>0.005160</td>\n",
              "      <td>0.069936</td>\n",
              "      <td>0.136776</td>\n",
              "      <td>0.046387</td>\n",
              "      <td>0.010860</td>\n",
              "      <td>0.068729</td>\n",
              "      <td>-0.004232</td>\n",
              "      <td>-0.004889</td>\n",
              "      <td>...</td>\n",
              "      <td>0.075859</td>\n",
              "      <td>0.051621</td>\n",
              "      <td>0.013238</td>\n",
              "      <td>0.045813</td>\n",
              "      <td>0.091032</td>\n",
              "      <td>0.005281</td>\n",
              "      <td>-0.026384</td>\n",
              "      <td>0.078543</td>\n",
              "      <td>conv2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.058461</td>\n",
              "      <td>0.100847</td>\n",
              "      <td>-0.038368</td>\n",
              "      <td>0.022629</td>\n",
              "      <td>0.103998</td>\n",
              "      <td>-0.064163</td>\n",
              "      <td>-0.030390</td>\n",
              "      <td>-0.050265</td>\n",
              "      <td>0.004671</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>...</td>\n",
              "      <td>0.094657</td>\n",
              "      <td>0.007707</td>\n",
              "      <td>0.022765</td>\n",
              "      <td>0.004527</td>\n",
              "      <td>0.002996</td>\n",
              "      <td>0.085109</td>\n",
              "      <td>0.008654</td>\n",
              "      <td>0.067731</td>\n",
              "      <td>conv2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.035365</td>\n",
              "      <td>0.023469</td>\n",
              "      <td>0.107092</td>\n",
              "      <td>0.101842</td>\n",
              "      <td>0.143467</td>\n",
              "      <td>0.217515</td>\n",
              "      <td>-0.012316</td>\n",
              "      <td>0.084479</td>\n",
              "      <td>0.127317</td>\n",
              "      <td>-0.104037</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.089539</td>\n",
              "      <td>-0.032844</td>\n",
              "      <td>0.050453</td>\n",
              "      <td>-0.007134</td>\n",
              "      <td>0.030011</td>\n",
              "      <td>-0.008236</td>\n",
              "      <td>-0.030027</td>\n",
              "      <td>conv2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.062249</td>\n",
              "      <td>-0.131436</td>\n",
              "      <td>-0.046978</td>\n",
              "      <td>0.021597</td>\n",
              "      <td>-0.080085</td>\n",
              "      <td>-0.078712</td>\n",
              "      <td>-0.103583</td>\n",
              "      <td>0.048199</td>\n",
              "      <td>0.034387</td>\n",
              "      <td>-0.074277</td>\n",
              "      <td>...</td>\n",
              "      <td>0.045809</td>\n",
              "      <td>-0.082850</td>\n",
              "      <td>0.027167</td>\n",
              "      <td>-0.046521</td>\n",
              "      <td>0.067527</td>\n",
              "      <td>-0.041539</td>\n",
              "      <td>0.057837</td>\n",
              "      <td>0.032542</td>\n",
              "      <td>conv2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.019309</td>\n",
              "      <td>-0.080785</td>\n",
              "      <td>0.035847</td>\n",
              "      <td>0.020273</td>\n",
              "      <td>0.009665</td>\n",
              "      <td>0.035786</td>\n",
              "      <td>-0.006190</td>\n",
              "      <td>-0.040837</td>\n",
              "      <td>-0.020941</td>\n",
              "      <td>0.074329</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027615</td>\n",
              "      <td>-0.036336</td>\n",
              "      <td>-0.055786</td>\n",
              "      <td>0.008235</td>\n",
              "      <td>-0.075360</td>\n",
              "      <td>-0.063744</td>\n",
              "      <td>0.026530</td>\n",
              "      <td>0.010399</td>\n",
              "      <td>conv2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.049547</td>\n",
              "      <td>0.038741</td>\n",
              "      <td>-0.036449</td>\n",
              "      <td>-0.002058</td>\n",
              "      <td>0.041931</td>\n",
              "      <td>0.053819</td>\n",
              "      <td>-0.005733</td>\n",
              "      <td>-0.068649</td>\n",
              "      <td>-0.066126</td>\n",
              "      <td>-0.024237</td>\n",
              "      <td>...</td>\n",
              "      <td>0.095926</td>\n",
              "      <td>0.090875</td>\n",
              "      <td>-0.025141</td>\n",
              "      <td>0.018905</td>\n",
              "      <td>0.031970</td>\n",
              "      <td>-0.055053</td>\n",
              "      <td>0.091992</td>\n",
              "      <td>-0.061894</td>\n",
              "      <td>conv2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.152914</td>\n",
              "      <td>-0.041600</td>\n",
              "      <td>0.115807</td>\n",
              "      <td>-0.041148</td>\n",
              "      <td>-0.039541</td>\n",
              "      <td>0.071635</td>\n",
              "      <td>-0.011336</td>\n",
              "      <td>0.034141</td>\n",
              "      <td>0.078109</td>\n",
              "      <td>-0.036853</td>\n",
              "      <td>...</td>\n",
              "      <td>0.086062</td>\n",
              "      <td>-0.007312</td>\n",
              "      <td>0.005437</td>\n",
              "      <td>0.073091</td>\n",
              "      <td>0.030808</td>\n",
              "      <td>-0.011768</td>\n",
              "      <td>0.093748</td>\n",
              "      <td>0.089395</td>\n",
              "      <td>conv2</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.125563</td>\n",
              "      <td>0.033356</td>\n",
              "      <td>-0.048423</td>\n",
              "      <td>0.110720</td>\n",
              "      <td>-0.018201</td>\n",
              "      <td>-0.147717</td>\n",
              "      <td>-0.042072</td>\n",
              "      <td>0.026514</td>\n",
              "      <td>-0.127686</td>\n",
              "      <td>-0.001961</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017386</td>\n",
              "      <td>-0.016188</td>\n",
              "      <td>0.018640</td>\n",
              "      <td>0.010524</td>\n",
              "      <td>-0.089284</td>\n",
              "      <td>0.094356</td>\n",
              "      <td>0.033373</td>\n",
              "      <td>0.012280</td>\n",
              "      <td>conv2</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.068213</td>\n",
              "      <td>-0.115001</td>\n",
              "      <td>-0.004743</td>\n",
              "      <td>-0.028494</td>\n",
              "      <td>-0.027653</td>\n",
              "      <td>0.079632</td>\n",
              "      <td>0.049930</td>\n",
              "      <td>-0.042845</td>\n",
              "      <td>0.092674</td>\n",
              "      <td>0.078472</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.005325</td>\n",
              "      <td>0.106815</td>\n",
              "      <td>0.014020</td>\n",
              "      <td>-0.074019</td>\n",
              "      <td>0.011533</td>\n",
              "      <td>-0.078723</td>\n",
              "      <td>0.016492</td>\n",
              "      <td>-0.013510</td>\n",
              "      <td>conv2</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-0.032338</td>\n",
              "      <td>-0.013617</td>\n",
              "      <td>0.151200</td>\n",
              "      <td>-0.075678</td>\n",
              "      <td>0.075957</td>\n",
              "      <td>0.207855</td>\n",
              "      <td>0.038870</td>\n",
              "      <td>-0.053325</td>\n",
              "      <td>0.046839</td>\n",
              "      <td>0.006819</td>\n",
              "      <td>...</td>\n",
              "      <td>0.041650</td>\n",
              "      <td>0.170038</td>\n",
              "      <td>-0.044293</td>\n",
              "      <td>0.004112</td>\n",
              "      <td>0.017894</td>\n",
              "      <td>-0.050105</td>\n",
              "      <td>0.031401</td>\n",
              "      <td>0.045740</td>\n",
              "      <td>conv2</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.051104</td>\n",
              "      <td>0.012186</td>\n",
              "      <td>0.086417</td>\n",
              "      <td>0.028352</td>\n",
              "      <td>0.156211</td>\n",
              "      <td>-0.002386</td>\n",
              "      <td>0.086014</td>\n",
              "      <td>0.042763</td>\n",
              "      <td>-0.056389</td>\n",
              "      <td>-0.043120</td>\n",
              "      <td>...</td>\n",
              "      <td>0.042914</td>\n",
              "      <td>-0.032490</td>\n",
              "      <td>0.009025</td>\n",
              "      <td>0.004950</td>\n",
              "      <td>0.000354</td>\n",
              "      <td>0.074826</td>\n",
              "      <td>0.052800</td>\n",
              "      <td>0.083592</td>\n",
              "      <td>conv2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.082138</td>\n",
              "      <td>0.005539</td>\n",
              "      <td>-0.044524</td>\n",
              "      <td>0.132945</td>\n",
              "      <td>0.106715</td>\n",
              "      <td>0.022446</td>\n",
              "      <td>0.116672</td>\n",
              "      <td>0.132660</td>\n",
              "      <td>-0.116526</td>\n",
              "      <td>-0.137380</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008512</td>\n",
              "      <td>-0.055690</td>\n",
              "      <td>0.034679</td>\n",
              "      <td>0.085180</td>\n",
              "      <td>0.014633</td>\n",
              "      <td>0.005773</td>\n",
              "      <td>0.066127</td>\n",
              "      <td>-0.082251</td>\n",
              "      <td>conv2</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>-0.043048</td>\n",
              "      <td>-0.140184</td>\n",
              "      <td>0.012286</td>\n",
              "      <td>-0.060239</td>\n",
              "      <td>-0.119047</td>\n",
              "      <td>0.023106</td>\n",
              "      <td>-0.004832</td>\n",
              "      <td>-0.028790</td>\n",
              "      <td>0.080668</td>\n",
              "      <td>-0.064381</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020423</td>\n",
              "      <td>0.075944</td>\n",
              "      <td>-0.070604</td>\n",
              "      <td>-0.053503</td>\n",
              "      <td>0.020353</td>\n",
              "      <td>-0.001231</td>\n",
              "      <td>0.079536</td>\n",
              "      <td>0.032849</td>\n",
              "      <td>conv2</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.021613</td>\n",
              "      <td>0.105474</td>\n",
              "      <td>0.025686</td>\n",
              "      <td>0.030302</td>\n",
              "      <td>0.136904</td>\n",
              "      <td>-0.020385</td>\n",
              "      <td>-0.046524</td>\n",
              "      <td>0.006731</td>\n",
              "      <td>-0.044295</td>\n",
              "      <td>0.048296</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.026215</td>\n",
              "      <td>0.104971</td>\n",
              "      <td>0.072772</td>\n",
              "      <td>0.041096</td>\n",
              "      <td>0.005841</td>\n",
              "      <td>0.030748</td>\n",
              "      <td>-0.013547</td>\n",
              "      <td>-0.054549</td>\n",
              "      <td>conv2</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.117869</td>\n",
              "      <td>0.126556</td>\n",
              "      <td>0.051337</td>\n",
              "      <td>0.002796</td>\n",
              "      <td>-0.029835</td>\n",
              "      <td>-0.061113</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.013864</td>\n",
              "      <td>-0.005772</td>\n",
              "      <td>0.182691</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027957</td>\n",
              "      <td>-0.073938</td>\n",
              "      <td>-0.020258</td>\n",
              "      <td>-0.038572</td>\n",
              "      <td>-0.075074</td>\n",
              "      <td>0.053019</td>\n",
              "      <td>-0.002690</td>\n",
              "      <td>-0.082208</td>\n",
              "      <td>conv2</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16 rows × 146 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e47deed9-f2bf-4227-ad53-6b1d77cc808a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e47deed9-f2bf-4227-ad53-6b1d77cc808a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e47deed9-f2bf-4227-ad53-6b1d77cc808a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8acdc681-e412-41e9-a844-14d1ab061b10\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8acdc681-e412-41e9-a844-14d1ab061b10')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8acdc681-e412-41e9-a844-14d1ab061b10 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results for each image\n",
        "for image_data in per_image_neuron_data:\n",
        "    print(f\"\\n--- Image {image_data['image_index']} ---\")\n",
        "    print(f\"True Label: {image_data['true_label']}\")\n",
        "    print(f\"Predicted Label: {image_data['predicted_label']}\")\n",
        "\n",
        "    # Display DataFrames for this image\n",
        "    for df_name, df in image_data['dataframes'].items():\n",
        "        print(f\"\\n{df_name.upper()} DataFrame:\")\n",
        "        print(df)\n",
        "        print(\"\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRIVKZFpfLTl",
        "outputId": "61f1073c-7fd7-4fc9-b96a-a52d060be0d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Image 0 ---\n",
            "True Label: 1\n",
            "Predicted Label: 1\n",
            "\n",
            "CONV1_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv1             0          0.362543\n",
            "1   conv1             1          0.347136\n",
            "2   conv1             2          0.154901\n",
            "3   conv1             3          0.090302\n",
            "4   conv1             4          0.160147\n",
            "5   conv1             5          0.258511\n",
            "6   conv1             6          0.025814\n",
            "7   conv1             7          0.376921\n",
            "8   conv1             8          0.182516\n",
            "9   conv1             9          0.182259\n",
            "10  conv1            10          0.160326\n",
            "11  conv1            11          0.252715\n",
            "12  conv1            12          0.014187\n",
            "13  conv1            13          0.150744\n",
            "14  conv1            14         -0.014882\n",
            "15  conv1            15          0.036231\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.400412  0.399371  0.071231 -0.067315  0.237489  0.328836 -0.150906   \n",
            "1   0.062670  0.420839  0.423627 -0.087857  0.270959 -0.074010  0.234360   \n",
            "2   0.077322  0.239514  0.362599  0.313162 -0.159267 -0.146957 -0.197186   \n",
            "3  -0.198520  0.416571  0.189112 -0.133381  0.282581  0.187163 -0.016161   \n",
            "4  -0.100635 -0.007484  0.208964  0.262549  0.069296  0.175801  0.311726   \n",
            "5  -0.273976 -0.003054  0.224042  0.069278  0.076746  0.157825  0.075656   \n",
            "6  -0.428757 -0.251183 -0.254338  0.081380  0.144253  0.244236  0.337603   \n",
            "7   0.378913  0.399986  0.071193  0.172418 -0.026089  0.130204 -0.220100   \n",
            "8   0.225943  0.192161  0.387705  0.135396 -0.048392 -0.038235 -0.115391   \n",
            "9   0.307195 -0.180329 -0.171382  0.319853  0.081811  0.181218  0.174932   \n",
            "10  0.130797  0.370466  0.380331  0.015307 -0.078076  0.016426 -0.083861   \n",
            "11  0.392175  0.247163  0.281231  0.259225 -0.270856 -0.165327  0.253949   \n",
            "12 -0.210740 -0.048877 -0.131766  0.335698  0.006075 -0.116493 -0.017947   \n",
            "13  0.119667  0.026561  0.093274  0.343750  0.204018 -0.236069  0.372014   \n",
            "14  0.198226 -0.245182  0.121569 -0.049056 -0.133100 -0.077453  0.442164   \n",
            "15  0.209107  0.173020 -0.301732  0.125146 -0.021763 -0.264287  0.322271   \n",
            "\n",
            "           7         8  layer  neuron_index  \n",
            "0   0.392861 -0.045586  conv1             0  \n",
            "1  -0.093755  0.006661  conv1             1  \n",
            "2   0.068652  0.012842  conv1             2  \n",
            "3  -0.195803  0.062113  conv1             3  \n",
            "4  -0.130634  0.326892  conv1             4  \n",
            "5   0.073296 -0.115009  conv1             5  \n",
            "6  -0.101311  0.121107  conv1             6  \n",
            "7   0.377975 -0.272754  conv1             7  \n",
            "8   0.228945  0.188063  conv1             8  \n",
            "9   0.345546  0.172957  conv1             9  \n",
            "10  0.056517  0.259929  conv1            10  \n",
            "11 -0.198929  0.022492  conv1            11  \n",
            "12  0.285077  0.004282  conv1            12  \n",
            "13  0.204587 -0.130169  conv1            13  \n",
            "14 -0.012433 -0.308704  conv1            14  \n",
            "15 -0.069897  0.135285  conv1            15  \n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv2             0          0.247010\n",
            "1   conv2             1          0.861713\n",
            "2   conv2             2          0.410245\n",
            "3   conv2             3          1.014889\n",
            "4   conv2             4         -0.287871\n",
            "5   conv2             5         -0.302608\n",
            "6   conv2             6          0.596905\n",
            "7   conv2             7          0.430440\n",
            "8   conv2             8          0.257072\n",
            "9   conv2             9          0.079023\n",
            "10  conv2            10          0.932231\n",
            "11  conv2            11          0.395869\n",
            "12  conv2            12          0.505099\n",
            "13  conv2            13         -0.011198\n",
            "14  conv2            14          0.597844\n",
            "15  conv2            15          0.355485\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.053389  0.023715 -0.082655 -0.048249 -0.076035 -0.150770  0.021789   \n",
            "1   0.047589 -0.093007  0.005160  0.069936  0.136776  0.046387  0.010860   \n",
            "2   0.058461  0.100847 -0.038368  0.022629  0.103998 -0.064163 -0.030390   \n",
            "3  -0.035365  0.023469  0.107092  0.101842  0.143467  0.217515 -0.012316   \n",
            "4   0.062249 -0.131436 -0.046978  0.021597 -0.080085 -0.078712 -0.103583   \n",
            "5  -0.019309 -0.080785  0.035847  0.020273  0.009665  0.035786 -0.006190   \n",
            "6   0.049547  0.038741 -0.036449 -0.002058  0.041931  0.053819 -0.005733   \n",
            "7  -0.152914 -0.041600  0.115807 -0.041148 -0.039541  0.071635 -0.011336   \n",
            "8   0.125563  0.033356 -0.048423  0.110720 -0.018201 -0.147717 -0.042072   \n",
            "9  -0.068213 -0.115001 -0.004743 -0.028494 -0.027653  0.079632  0.049930   \n",
            "10 -0.032338 -0.013617  0.151200 -0.075678  0.075957  0.207855  0.038870   \n",
            "11  0.051104  0.012186  0.086417  0.028352  0.156211 -0.002386  0.086014   \n",
            "12  0.082138  0.005539 -0.044524  0.132945  0.106715  0.022446  0.116672   \n",
            "13 -0.043048 -0.140184  0.012286 -0.060239 -0.119047  0.023106 -0.004832   \n",
            "14 -0.021613  0.105474  0.025686  0.030302  0.136904 -0.020385 -0.046524   \n",
            "15  0.117869  0.126556  0.051337  0.002796 -0.029835 -0.061113  0.004300   \n",
            "\n",
            "           7         8         9  ...       136       137       138       139  \\\n",
            "0   0.087136  0.136641  0.125749  ...  0.105212  0.008460  0.086248 -0.005925   \n",
            "1   0.068729 -0.004232 -0.004889  ...  0.075859  0.051621  0.013238  0.045813   \n",
            "2  -0.050265  0.004671  0.000319  ...  0.094657  0.007707  0.022765  0.004527   \n",
            "3   0.084479  0.127317 -0.104037  ...  0.000802  0.089539 -0.032844  0.050453   \n",
            "4   0.048199  0.034387 -0.074277  ...  0.045809 -0.082850  0.027167 -0.046521   \n",
            "5  -0.040837 -0.020941  0.074329  ... -0.027615 -0.036336 -0.055786  0.008235   \n",
            "6  -0.068649 -0.066126 -0.024237  ...  0.095926  0.090875 -0.025141  0.018905   \n",
            "7   0.034141  0.078109 -0.036853  ...  0.086062 -0.007312  0.005437  0.073091   \n",
            "8   0.026514 -0.127686 -0.001961  ...  0.017386 -0.016188  0.018640  0.010524   \n",
            "9  -0.042845  0.092674  0.078472  ... -0.005325  0.106815  0.014020 -0.074019   \n",
            "10 -0.053325  0.046839  0.006819  ...  0.041650  0.170038 -0.044293  0.004112   \n",
            "11  0.042763 -0.056389 -0.043120  ...  0.042914 -0.032490  0.009025  0.004950   \n",
            "12  0.132660 -0.116526 -0.137380  ... -0.008512 -0.055690  0.034679  0.085180   \n",
            "13 -0.028790  0.080668 -0.064381  ... -0.020423  0.075944 -0.070604 -0.053503   \n",
            "14  0.006731 -0.044295  0.048296  ... -0.026215  0.104971  0.072772  0.041096   \n",
            "15  0.013864 -0.005772  0.182691  ... -0.027957 -0.073938 -0.020258 -0.038572   \n",
            "\n",
            "         140       141       142       143  layer  neuron_index  \n",
            "0  -0.033853  0.113257 -0.040305  0.088088  conv2             0  \n",
            "1   0.091032  0.005281 -0.026384  0.078543  conv2             1  \n",
            "2   0.002996  0.085109  0.008654  0.067731  conv2             2  \n",
            "3  -0.007134  0.030011 -0.008236 -0.030027  conv2             3  \n",
            "4   0.067527 -0.041539  0.057837  0.032542  conv2             4  \n",
            "5  -0.075360 -0.063744  0.026530  0.010399  conv2             5  \n",
            "6   0.031970 -0.055053  0.091992 -0.061894  conv2             6  \n",
            "7   0.030808 -0.011768  0.093748  0.089395  conv2             7  \n",
            "8  -0.089284  0.094356  0.033373  0.012280  conv2             8  \n",
            "9   0.011533 -0.078723  0.016492 -0.013510  conv2             9  \n",
            "10  0.017894 -0.050105  0.031401  0.045740  conv2            10  \n",
            "11  0.000354  0.074826  0.052800  0.083592  conv2            11  \n",
            "12  0.014633  0.005773  0.066127 -0.082251  conv2            12  \n",
            "13  0.020353 -0.001231  0.079536  0.032849  conv2            13  \n",
            "14  0.005841  0.030748 -0.013547 -0.054549  conv2            14  \n",
            "15 -0.075074  0.053019 -0.002690 -0.082208  conv2            15  \n",
            "\n",
            "[16 rows x 146 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_ACTIVATIONS DataFrame:\n",
            "   layer  neuron_index  activation_value\n",
            "0    fc1             0         -4.456444\n",
            "1    fc1             1         -4.267042\n",
            "2    fc1             2        -11.516699\n",
            "3    fc1             3         -4.154439\n",
            "4    fc1             4         -5.867033\n",
            "5    fc1             5          6.849453\n",
            "6    fc1             6         -5.162141\n",
            "7    fc1             7          4.395796\n",
            "8    fc1             8         -0.609752\n",
            "9    fc1             9         12.551845\n",
            "10   fc1            10         -4.676147\n",
            "11   fc1            11         -6.131161\n",
            "12   fc1            12         17.422287\n",
            "13   fc1            13         31.487080\n",
            "14   fc1            14         -5.659576\n",
            "15   fc1            15        -13.189085\n",
            "16   fc1            16         -6.006960\n",
            "17   fc1            17         -4.983891\n",
            "18   fc1            18          3.676079\n",
            "19   fc1            19          9.194911\n",
            "20   fc1            20         26.456285\n",
            "21   fc1            21         -9.694846\n",
            "22   fc1            22        -12.955977\n",
            "23   fc1            23         11.109831\n",
            "24   fc1            24         -6.353340\n",
            "25   fc1            25         -4.115302\n",
            "26   fc1            26         -5.134408\n",
            "27   fc1            27         -3.119771\n",
            "28   fc1            28         -5.805737\n",
            "29   fc1            29         11.130728\n",
            "30   fc1            30         -5.984334\n",
            "31   fc1            31        -13.703977\n",
            "32   fc1            32         -7.644935\n",
            "33   fc1            33         12.571311\n",
            "34   fc1            34         -5.916789\n",
            "35   fc1            35         19.704357\n",
            "36   fc1            36         38.756954\n",
            "37   fc1            37         17.854876\n",
            "38   fc1            38          3.539701\n",
            "39   fc1            39        -15.611642\n",
            "40   fc1            40          6.480339\n",
            "41   fc1            41         -4.732583\n",
            "42   fc1            42         -4.810484\n",
            "43   fc1            43         -4.283399\n",
            "44   fc1            44         -5.013291\n",
            "45   fc1            45         -5.690193\n",
            "46   fc1            46         -4.625062\n",
            "47   fc1            47         -3.662933\n",
            "48   fc1            48         -3.836876\n",
            "49   fc1            49         39.360073\n",
            "50   fc1            50          9.538559\n",
            "51   fc1            51         -3.134629\n",
            "52   fc1            52         22.300701\n",
            "53   fc1            53         -4.558513\n",
            "54   fc1            54         -3.165549\n",
            "55   fc1            55         -6.995828\n",
            "56   fc1            56         -3.448275\n",
            "57   fc1            57          4.120663\n",
            "58   fc1            58         -3.984741\n",
            "59   fc1            59         -6.052259\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.012868  0.012967 -0.004651  0.004214  0.004450 -0.004286 -0.006892   \n",
            "1  -0.006668  0.001739  0.008233 -0.015734 -0.018150  0.006245 -0.009190   \n",
            "2  -0.004715 -0.008510 -0.027038 -0.012099 -0.017072 -0.001276 -0.004790   \n",
            "3  -0.022022  0.006158 -0.000070  0.005327 -0.010999 -0.013339 -0.000802   \n",
            "4  -0.021747  0.005062 -0.007141 -0.002509  0.002286 -0.004356 -0.013751   \n",
            "5   0.000488  0.012703  0.014533  0.016341 -0.004459  0.019696  0.002125   \n",
            "6   0.009300 -0.022666 -0.019488  0.000888 -0.000055 -0.011650 -0.015693   \n",
            "7   0.031482  0.014408  0.003062  0.002343  0.027174  0.003222  0.007536   \n",
            "8  -0.021455 -0.018184  0.010642 -0.009969 -0.002964 -0.022868  0.002633   \n",
            "9   0.008664  0.005595  0.018707  0.036951  0.012074  0.024569  0.035212   \n",
            "10 -0.015157 -0.015687 -0.020529  0.011342  0.001704 -0.014001 -0.023534   \n",
            "11  0.033904  0.009318  0.023171  0.003949  0.003715  0.000735  0.023219   \n",
            "12  0.024346  0.032305  0.026967  0.025876  0.010165  0.022697  0.011940   \n",
            "13  0.008851  0.010857  0.002167  0.024771  0.005659  0.021011  0.000578   \n",
            "14 -0.009209 -0.003457 -0.008076 -0.003801  0.010076 -0.013077 -0.018169   \n",
            "15 -0.022892 -0.007363 -0.001998 -0.025108 -0.005332 -0.008478 -0.006340   \n",
            "16 -0.011208 -0.008056 -0.020657 -0.002330  0.002901 -0.009348 -0.019556   \n",
            "17 -0.004303 -0.011543 -0.022747 -0.002148 -0.000974  0.003379  0.001518   \n",
            "18  0.014590  0.013593  0.026992  0.014942  0.002147  0.024546  0.023784   \n",
            "19  0.013653  0.015041  0.026944  0.017540  0.012362  0.005006  0.020771   \n",
            "20  0.000926 -0.002540  0.000976 -0.007426  0.008134  0.009121  0.006932   \n",
            "21 -0.008481 -0.009680 -0.015762 -0.008213  0.005569 -0.011419 -0.018695   \n",
            "22  0.002687 -0.004754  0.025822  0.022829  0.026687  0.024950  0.015143   \n",
            "23  0.012675 -0.008788  0.020707 -0.002437 -0.000562 -0.001121  0.009253   \n",
            "24 -0.021873 -0.006459  0.000207 -0.022165  0.008027 -0.006639  0.011209   \n",
            "25 -0.009614 -0.022043 -0.007360  0.004037 -0.017339 -0.001730 -0.005209   \n",
            "26 -0.003378 -0.006826 -0.019353 -0.008899  0.007771  0.007703 -0.008105   \n",
            "27 -0.009939 -0.008868  0.000785  0.008521 -0.017249  0.009543 -0.006277   \n",
            "28 -0.012323  0.002773 -0.002832  0.008380 -0.001049 -0.023035 -0.010383   \n",
            "29  0.014561  0.016938  0.017683 -0.001824  0.010139  0.014729  0.012067   \n",
            "30 -0.019001 -0.009498 -0.020953 -0.004602  0.011816  0.005326 -0.018519   \n",
            "31  0.000243  0.000025  0.021961 -0.006239  0.018972 -0.004205  0.002377   \n",
            "32 -0.003725 -0.003355 -0.020392  0.005503 -0.013929 -0.013021  0.001984   \n",
            "33  0.016569  0.002955  0.006740  0.025918  0.010518  0.006316  0.009447   \n",
            "34 -0.021455 -0.010343  0.011155  0.004942 -0.004358 -0.012782 -0.010427   \n",
            "35 -0.004436 -0.022906 -0.004501  0.005599  0.007464 -0.000397 -0.000361   \n",
            "36  0.028495  0.008049  0.033686  0.027099  0.001744  0.033556  0.005032   \n",
            "37  0.017607  0.014472  0.003794  0.007778  0.026253  0.026272  0.000011   \n",
            "38 -0.002645  0.017745  0.021209 -0.002706  0.004354  0.021500  0.015463   \n",
            "39  0.013660  0.008610  0.018474  0.003934  0.033713  0.001638  0.016512   \n",
            "40  0.030918  0.000622  0.009956  0.019433  0.000461 -0.000907  0.013701   \n",
            "41 -0.014202 -0.010007 -0.013808 -0.019121  0.001727 -0.010824  0.006965   \n",
            "42 -0.018572 -0.000763  0.011354 -0.020072 -0.000430  0.000737 -0.016534   \n",
            "43 -0.016530 -0.011671 -0.013214 -0.002635  0.008195 -0.009454 -0.014988   \n",
            "44 -0.004162  0.004954 -0.016971 -0.016766 -0.005005 -0.002056 -0.009664   \n",
            "45 -0.006126  0.000294 -0.010289 -0.013374 -0.021398 -0.016979 -0.005077   \n",
            "46 -0.003019  0.003975 -0.013475  0.004994 -0.012704 -0.002466 -0.018315   \n",
            "47  0.002745  0.005883 -0.017230  0.000825 -0.021160 -0.008062 -0.022425   \n",
            "48 -0.000349 -0.019200  0.004244  0.012784  0.002108 -0.011295 -0.014164   \n",
            "49  0.019498  0.027400  0.033523  0.031643  0.003375  0.024539  0.018732   \n",
            "50  0.002034  0.025831  0.021195  0.015002  0.011998  0.006490  0.015592   \n",
            "51  0.002681 -0.004163 -0.002470  0.002025  0.000847 -0.004836 -0.020047   \n",
            "52  0.031499  0.006817  0.030368  0.016483  0.009225  0.011744  0.023579   \n",
            "53 -0.002231 -0.016763 -0.012501  0.002132 -0.016919  0.000812  0.009893   \n",
            "54 -0.016136  0.002641  0.006465  0.005333  0.000611 -0.021958  0.005562   \n",
            "55  0.006558  0.001421 -0.006817 -0.021912 -0.021949 -0.012012  0.008985   \n",
            "56 -0.013326  0.005539 -0.001117  0.010829  0.002428 -0.019694 -0.017891   \n",
            "57  0.000068  0.003597  0.006211  0.004677 -0.000560  0.023743  0.012332   \n",
            "58  0.028027  0.022394  0.015621  0.022584  0.027930  0.013303  0.023036   \n",
            "59 -0.016706  0.001549  0.002360 -0.005833 -0.013946 -0.005872 -0.015406   \n",
            "\n",
            "           7         8         9  ...      3128      3129      3130      3131  \\\n",
            "0   0.007324 -0.009616 -0.005067  ... -0.006856  0.010652  0.010737 -0.005627   \n",
            "1  -0.018485 -0.011243 -0.014907  ...  0.003302 -0.023178 -0.002841 -0.002705   \n",
            "2   0.003348 -0.021924  0.002252  ... -0.004555 -0.022496 -0.027291  0.000060   \n",
            "3  -0.006648 -0.017124  0.011142  ... -0.008438  0.007751 -0.003895 -0.014379   \n",
            "4   0.009772 -0.011616 -0.003061  ... -0.013893 -0.023509  0.007052  0.009660   \n",
            "5   0.008894  0.007604  0.022646  ...  0.023266  0.041666  0.029488  0.033174   \n",
            "6  -0.003789  0.009797 -0.006847  ...  0.009045 -0.017039 -0.017816  0.009705   \n",
            "7   0.004008  0.015966  0.020602  ...  0.009192  0.034439  0.014094  0.028504   \n",
            "8  -0.003198 -0.022776  0.003764  ... -0.001452 -0.006950 -0.022771  0.003933   \n",
            "9   0.030930  0.004664  0.003770  ...  0.004832 -0.009257 -0.008188  0.001435   \n",
            "10 -0.013386 -0.013077 -0.018894  ... -0.001648  0.000762 -0.017888 -0.016899   \n",
            "11  0.006491  0.032536  0.000740  ...  0.003196  0.004032  0.010177  0.012247   \n",
            "12  0.025155  0.015860  0.000769  ...  0.006513  0.015600  0.012484  0.016776   \n",
            "13  0.029623  0.026839 -0.001954  ...  0.036832  0.027887  0.015517  0.010580   \n",
            "14 -0.016337  0.004727  0.010331  ... -0.012719 -0.016295  0.002682 -0.014291   \n",
            "15 -0.017671 -0.001590 -0.007524  ... -0.027042 -0.015744 -0.009734 -0.021105   \n",
            "16 -0.015554  0.001033  0.005761  ...  0.009729 -0.022332 -0.021350 -0.016759   \n",
            "17  0.010020 -0.019688 -0.004400  ...  0.007721 -0.004402 -0.016819 -0.015915   \n",
            "18  0.002784 -0.006211  0.013223  ...  0.024819  0.008977  0.035165  0.001899   \n",
            "19  0.014684  0.027412  0.018771  ...  0.027893  0.023726  0.017920  0.016699   \n",
            "20 -0.019416 -0.022032 -0.013255  ...  0.047160  0.036355  0.025181  0.022490   \n",
            "21  0.001027 -0.005196 -0.004424  ... -0.011853 -0.003937  0.003567  0.000304   \n",
            "22  0.004678  0.017792 -0.000148  ...  0.014265  0.008728 -0.017063  0.005836   \n",
            "23  0.005949 -0.008079  0.000984  ... -0.013547 -0.005859  0.002466  0.003944   \n",
            "24 -0.012757 -0.023823 -0.018663  ...  0.011678 -0.019435  0.003086 -0.019769   \n",
            "25 -0.010399  0.010380 -0.019753  ...  0.000844  0.010552 -0.019871 -0.019154   \n",
            "26  0.010319 -0.010890  0.007982  ...  0.000887  0.000956  0.001107 -0.014635   \n",
            "27 -0.004731 -0.009723 -0.003905  ...  0.005012 -0.016505 -0.011187 -0.001365   \n",
            "28 -0.015085 -0.001597 -0.002212  ... -0.009379  0.010932  0.000835  0.011187   \n",
            "29  0.009418  0.025151  0.020776  ... -0.004506 -0.015641 -0.013120 -0.010891   \n",
            "30 -0.016916  0.009760 -0.014054  ...  0.007682 -0.003761 -0.015353 -0.000961   \n",
            "31  0.012016 -0.000764 -0.007299  ... -0.012403 -0.009862 -0.011390 -0.003945   \n",
            "32 -0.024166  0.000700  0.009655  ... -0.019903 -0.006628 -0.020596  0.001379   \n",
            "33  0.010933  0.003280  0.013948  ... -0.003015  0.020623 -0.000906  0.015280   \n",
            "34 -0.017208  0.006274  0.003570  ... -0.013828 -0.008093 -0.019012 -0.001721   \n",
            "35  0.005101 -0.010963 -0.009127  ... -0.002773  0.011193  0.019547  0.002818   \n",
            "36  0.012425  0.009884  0.031366  ...  0.042560  0.039935  0.023204  0.029524   \n",
            "37  0.023419  0.006894  0.026621  ...  0.004115  0.000018  0.020470 -0.002871   \n",
            "38  0.018896  0.024307 -0.000390  ... -0.004216 -0.012399 -0.001571 -0.018753   \n",
            "39  0.010963  0.030405  0.032920  ... -0.013158  0.001995 -0.017575  0.000686   \n",
            "40  0.026295  0.013387 -0.001529  ...  0.043029  0.038009  0.020855  0.017207   \n",
            "41 -0.004680 -0.013764 -0.007863  ...  0.008035  0.010515 -0.006477  0.011474   \n",
            "42 -0.000678  0.009945  0.003108  ...  0.008813  0.006511 -0.001635 -0.017173   \n",
            "43 -0.011972 -0.016392  0.001932  ... -0.010337 -0.012674 -0.018367 -0.022919   \n",
            "44  0.003259 -0.000585  0.001807  ...  0.008413 -0.014572 -0.008983  0.005895   \n",
            "45  0.006896 -0.008086 -0.009275  ... -0.005778  0.007000  0.002406 -0.008275   \n",
            "46  0.010720 -0.002166 -0.014401  ... -0.013205 -0.022235 -0.002017 -0.000171   \n",
            "47 -0.005961 -0.003954  0.010139  ...  0.011701  0.001282 -0.018512 -0.003118   \n",
            "48 -0.013545  0.006565 -0.005001  ... -0.010218  0.010282 -0.019655  0.010408   \n",
            "49  0.018647  0.026510  0.016471  ...  0.039909  0.049060  0.028912  0.045582   \n",
            "50  0.000300  0.010269  0.008490  ...  0.023184 -0.001352  0.019045  0.003499   \n",
            "51  0.004892  0.007555  0.001219  ... -0.016053  0.001421 -0.013407 -0.006220   \n",
            "52  0.030600  0.026217  0.023260  ...  0.017529  0.006934  0.017406  0.020949   \n",
            "53 -0.010024 -0.009774 -0.018796  ... -0.015735 -0.020443  0.008829 -0.016565   \n",
            "54  0.011039 -0.000971 -0.004749  ... -0.011213  0.000786 -0.019802 -0.001971   \n",
            "55 -0.017711 -0.011881  0.008669  ...  0.003574  0.005007 -0.005511 -0.009074   \n",
            "56 -0.018805 -0.010618  0.002681  ... -0.014544 -0.013959  0.007968 -0.004889   \n",
            "57  0.013836  0.000618  0.024202  ...  0.020163  0.034249  0.023702  0.032346   \n",
            "58  0.020271 -0.000923  0.004961  ...  0.030553  0.023524  0.025896  0.012225   \n",
            "59  0.006506 -0.000810  0.000023  ... -0.021418 -0.022299 -0.008617 -0.017246   \n",
            "\n",
            "        3132      3133      3134      3135  layer  neuron_index  \n",
            "0  -0.006300  0.000761 -0.010077 -0.010894    fc1             0  \n",
            "1   0.007173  0.003588 -0.009852 -0.016465    fc1             1  \n",
            "2   0.005837 -0.015450 -0.013901  0.002791    fc1             2  \n",
            "3  -0.012918  0.007607 -0.013114 -0.016117    fc1             3  \n",
            "4   0.003922 -0.010699  0.009847 -0.021692    fc1             4  \n",
            "5   0.028678  0.014802  0.011530  0.006059    fc1             5  \n",
            "6  -0.014932  0.004915  0.002246 -0.007906    fc1             6  \n",
            "7   0.013501  0.022247  0.021532 -0.003630    fc1             7  \n",
            "8  -0.006297 -0.015515 -0.013881  0.004368    fc1             8  \n",
            "9  -0.001923  0.008458  0.021580  0.029945    fc1             9  \n",
            "10 -0.000172 -0.004697 -0.007486  0.003054    fc1            10  \n",
            "11 -0.007124 -0.002595 -0.013320 -0.004145    fc1            11  \n",
            "12 -0.001142  0.025493  0.015860  0.012417    fc1            12  \n",
            "13  0.008905  0.014851  0.010457  0.007798    fc1            13  \n",
            "14 -0.010744 -0.008016 -0.019082 -0.007001    fc1            14  \n",
            "15 -0.008899 -0.000829 -0.016586 -0.028002    fc1            15  \n",
            "16  0.011897 -0.018047 -0.002446 -0.017222    fc1            16  \n",
            "17 -0.020536  0.006682 -0.007786 -0.019112    fc1            17  \n",
            "18  0.014848  0.023633  0.009814 -0.009907    fc1            18  \n",
            "19  0.032436  0.014681  0.007802  0.015888    fc1            19  \n",
            "20  0.010047  0.026064  0.016482  0.030144    fc1            20  \n",
            "21  0.008994  0.010006 -0.018727 -0.003075    fc1            21  \n",
            "22 -0.000216 -0.005094  0.018505 -0.008893    fc1            22  \n",
            "23  0.027010  0.004446 -0.004685  0.011521    fc1            23  \n",
            "24 -0.006039  0.005686 -0.007776  0.007923    fc1            24  \n",
            "25 -0.013028 -0.005355 -0.013795  0.000189    fc1            25  \n",
            "26  0.001260  0.002817 -0.000567 -0.018858    fc1            26  \n",
            "27 -0.012066  0.005803  0.008898 -0.020256    fc1            27  \n",
            "28 -0.021011 -0.016277 -0.021642 -0.017041    fc1            28  \n",
            "29  0.015582 -0.001358  0.009005  0.020085    fc1            29  \n",
            "30  0.009142 -0.011048 -0.019065 -0.022964    fc1            30  \n",
            "31 -0.017578  0.018184 -0.014494  0.006414    fc1            31  \n",
            "32 -0.007214 -0.020134 -0.002980 -0.009061    fc1            32  \n",
            "33 -0.002171 -0.004688  0.024596  0.012860    fc1            33  \n",
            "34  0.006539  0.009605 -0.015453 -0.016520    fc1            34  \n",
            "35 -0.018720 -0.022744 -0.017856  0.009409    fc1            35  \n",
            "36  0.040159  0.015496  0.046070  0.038217    fc1            36  \n",
            "37  0.016869  0.005396  0.012609  0.013487    fc1            37  \n",
            "38 -0.000505  0.023441  0.003526  0.005332    fc1            38  \n",
            "39 -0.019023 -0.006869 -0.015280  0.005979    fc1            39  \n",
            "40  0.027607  0.004566  0.021181  0.012148    fc1            40  \n",
            "41  0.004879 -0.012685 -0.009628 -0.022963    fc1            41  \n",
            "42  0.007932 -0.012811 -0.015902 -0.008890    fc1            42  \n",
            "43 -0.016014  0.007799  0.005429 -0.008904    fc1            43  \n",
            "44  0.004329  0.007101 -0.003699 -0.012319    fc1            44  \n",
            "45  0.005780  0.009909 -0.004622 -0.021927    fc1            45  \n",
            "46 -0.013934 -0.011455 -0.007828 -0.010529    fc1            46  \n",
            "47 -0.016764 -0.023845 -0.003573  0.011422    fc1            47  \n",
            "48 -0.015247 -0.011081 -0.016139 -0.007430    fc1            48  \n",
            "49  0.037967  0.039943  0.047666  0.032830    fc1            49  \n",
            "50  0.002823 -0.002692 -0.008353  0.002865    fc1            50  \n",
            "51 -0.014761  0.009049 -0.008702 -0.023180    fc1            51  \n",
            "52  0.023632  0.022728  0.018886  0.025511    fc1            52  \n",
            "53 -0.018722  0.011494  0.004309 -0.023839    fc1            53  \n",
            "54  0.000339  0.006231  0.007703 -0.013026    fc1            54  \n",
            "55 -0.014757 -0.008799 -0.020483 -0.005706    fc1            55  \n",
            "56 -0.014795  0.005220 -0.021989 -0.015774    fc1            56  \n",
            "57 -0.000363  0.006102 -0.002093 -0.001637    fc1            57  \n",
            "58  0.026156  0.011964  0.002414  0.006485    fc1            58  \n",
            "59 -0.003636  0.008666 -0.018831 -0.015422    fc1            59  \n",
            "\n",
            "[60 rows x 3138 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_ACTIVATIONS DataFrame:\n",
            "  layer  neuron_index  activation_value\n",
            "0   fc2             0        -13.368244\n",
            "1   fc2             1         16.461422\n",
            "2   fc2             2         -3.920114\n",
            "3   fc2             3         -9.976067\n",
            "4   fc2             4        -12.454081\n",
            "5   fc2             5        -13.399258\n",
            "6   fc2             6        -13.740579\n",
            "7   fc2             7         -5.318144\n",
            "8   fc2             8        -14.564055\n",
            "9   fc2             9         15.788969\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_CONNECTIONS DataFrame:\n",
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.107950 -0.100333 -0.125853  0.091329 -0.090712 -0.024337 -0.035301   \n",
            "1  0.131501 -0.110936 -0.085158  0.066024 -0.037845 -0.071343 -0.052721   \n",
            "2  0.050709 -0.033907 -0.077746 -0.074659 -0.076966 -0.074498 -0.088701   \n",
            "3  0.049182 -0.027318 -0.033456 -0.035291 -0.043763 -0.003068  0.013614   \n",
            "4  0.042645  0.088435  0.038246  0.004014 -0.031166 -0.035241 -0.033445   \n",
            "5  0.086447 -0.126483 -0.040570  0.021679  0.066402 -0.114365  0.069705   \n",
            "6 -0.080449  0.085518 -0.090617 -0.053715 -0.023148 -0.005910  0.053310   \n",
            "7  0.092976 -0.108292 -0.003311 -0.020869  0.013422 -0.048318  0.010532   \n",
            "8 -0.123973 -0.038942  0.098014 -0.079174  0.093177  0.066027  0.115322   \n",
            "9  0.017517 -0.083782  0.037875 -0.062554 -0.064439  0.184223  0.006078   \n",
            "\n",
            "          7         8         9  ...        52        53        54        55  \\\n",
            "0  0.090453 -0.134711  0.053470  ... -0.070573  0.083195  0.095116  0.083355   \n",
            "1 -0.112283  0.045542  0.064597  ...  0.047196  0.127483  0.055454  0.038554   \n",
            "2  0.057485 -0.012508 -0.048672  ...  0.098187  0.072752 -0.079741 -0.097680   \n",
            "3 -0.134513 -0.108175  0.041074  ... -0.005931 -0.089947 -0.010503  0.077362   \n",
            "4 -0.040576  0.073950 -0.058569  ... -0.079893 -0.038126 -0.095315  0.112316   \n",
            "5 -0.126576 -0.152112 -0.069325  ... -0.028109  0.103588 -0.003243  0.043243   \n",
            "6  0.102617  0.096054  0.003210  ... -0.036933  0.010865  0.066728  0.112904   \n",
            "7  0.093271  0.076127 -0.004166  ... -0.105497  0.043385  0.102613  0.121523   \n",
            "8  0.016770 -0.150593  0.057095  ... -0.014134 -0.041150  0.043610 -0.033362   \n",
            "9  0.090238 -0.031772 -0.016623  ... -0.092037  0.067543 -0.015439  0.107121   \n",
            "\n",
            "         56        57        58        59  layer  neuron_index  \n",
            "0 -0.068317  0.057229 -0.010008 -0.046776    fc2             0  \n",
            "1  0.154287 -0.029886 -0.088726  0.017459    fc2             1  \n",
            "2 -0.002260 -0.142917  0.077585  0.014963    fc2             2  \n",
            "3 -0.088103 -0.110321  0.010353  0.046034    fc2             3  \n",
            "4  0.099081  0.096792  0.056523 -0.063165    fc2             4  \n",
            "5 -0.133819 -0.110977 -0.123005 -0.015103    fc2             5  \n",
            "6  0.097748 -0.037395  0.050451 -0.001918    fc2             6  \n",
            "7 -0.045865 -0.148744  0.003459 -0.012868    fc2             7  \n",
            "8 -0.111366 -0.025910  0.000013  0.098205    fc2             8  \n",
            "9 -0.079098  0.062496 -0.018937  0.063364    fc2             9  \n",
            "\n",
            "[10 rows x 62 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Image 1 ---\n",
            "True Label: 9\n",
            "Predicted Label: 9\n",
            "\n",
            "CONV1_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv1             0          0.400732\n",
            "1   conv1             1          0.375502\n",
            "2   conv1             2          0.168814\n",
            "3   conv1             3          0.104776\n",
            "4   conv1             4          0.187366\n",
            "5   conv1             5          0.265455\n",
            "6   conv1             6          0.023205\n",
            "7   conv1             7          0.401587\n",
            "8   conv1             8          0.210704\n",
            "9   conv1             9          0.212290\n",
            "10  conv1            10          0.186360\n",
            "11  conv1            11          0.272734\n",
            "12  conv1            12          0.016754\n",
            "13  conv1            13          0.175066\n",
            "14  conv1            14         -0.016441\n",
            "15  conv1            15          0.043719\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.400412  0.399371  0.071231 -0.067315  0.237489  0.328836 -0.150906   \n",
            "1   0.062670  0.420839  0.423627 -0.087857  0.270959 -0.074010  0.234360   \n",
            "2   0.077322  0.239514  0.362599  0.313162 -0.159267 -0.146957 -0.197186   \n",
            "3  -0.198520  0.416571  0.189112 -0.133381  0.282581  0.187163 -0.016161   \n",
            "4  -0.100635 -0.007484  0.208964  0.262549  0.069296  0.175801  0.311726   \n",
            "5  -0.273976 -0.003054  0.224042  0.069278  0.076746  0.157825  0.075656   \n",
            "6  -0.428757 -0.251183 -0.254338  0.081380  0.144253  0.244236  0.337603   \n",
            "7   0.378913  0.399986  0.071193  0.172418 -0.026089  0.130204 -0.220100   \n",
            "8   0.225943  0.192161  0.387705  0.135396 -0.048392 -0.038235 -0.115391   \n",
            "9   0.307195 -0.180329 -0.171382  0.319853  0.081811  0.181218  0.174932   \n",
            "10  0.130797  0.370466  0.380331  0.015307 -0.078076  0.016426 -0.083861   \n",
            "11  0.392175  0.247163  0.281231  0.259225 -0.270856 -0.165327  0.253949   \n",
            "12 -0.210740 -0.048877 -0.131766  0.335698  0.006075 -0.116493 -0.017947   \n",
            "13  0.119667  0.026561  0.093274  0.343750  0.204018 -0.236069  0.372014   \n",
            "14  0.198226 -0.245182  0.121569 -0.049056 -0.133100 -0.077453  0.442164   \n",
            "15  0.209107  0.173020 -0.301732  0.125146 -0.021763 -0.264287  0.322271   \n",
            "\n",
            "           7         8  layer  neuron_index  \n",
            "0   0.392861 -0.045586  conv1             0  \n",
            "1  -0.093755  0.006661  conv1             1  \n",
            "2   0.068652  0.012842  conv1             2  \n",
            "3  -0.195803  0.062113  conv1             3  \n",
            "4  -0.130634  0.326892  conv1             4  \n",
            "5   0.073296 -0.115009  conv1             5  \n",
            "6  -0.101311  0.121107  conv1             6  \n",
            "7   0.377975 -0.272754  conv1             7  \n",
            "8   0.228945  0.188063  conv1             8  \n",
            "9   0.345546  0.172957  conv1             9  \n",
            "10  0.056517  0.259929  conv1            10  \n",
            "11 -0.198929  0.022492  conv1            11  \n",
            "12  0.285077  0.004282  conv1            12  \n",
            "13  0.204587 -0.130169  conv1            13  \n",
            "14 -0.012433 -0.308704  conv1            14  \n",
            "15 -0.069897  0.135285  conv1            15  \n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv2             0          0.297024\n",
            "1   conv2             1          0.958960\n",
            "2   conv2             2          0.464407\n",
            "3   conv2             3          1.130801\n",
            "4   conv2             4         -0.309881\n",
            "5   conv2             5         -0.311059\n",
            "6   conv2             6          0.663883\n",
            "7   conv2             7          0.464707\n",
            "8   conv2             8          0.283155\n",
            "9   conv2             9          0.086225\n",
            "10  conv2            10          1.047050\n",
            "11  conv2            11          0.435380\n",
            "12  conv2            12          0.570321\n",
            "13  conv2            13         -0.020023\n",
            "14  conv2            14          0.678877\n",
            "15  conv2            15          0.403454\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.053389  0.023715 -0.082655 -0.048249 -0.076035 -0.150770  0.021789   \n",
            "1   0.047589 -0.093007  0.005160  0.069936  0.136776  0.046387  0.010860   \n",
            "2   0.058461  0.100847 -0.038368  0.022629  0.103998 -0.064163 -0.030390   \n",
            "3  -0.035365  0.023469  0.107092  0.101842  0.143467  0.217515 -0.012316   \n",
            "4   0.062249 -0.131436 -0.046978  0.021597 -0.080085 -0.078712 -0.103583   \n",
            "5  -0.019309 -0.080785  0.035847  0.020273  0.009665  0.035786 -0.006190   \n",
            "6   0.049547  0.038741 -0.036449 -0.002058  0.041931  0.053819 -0.005733   \n",
            "7  -0.152914 -0.041600  0.115807 -0.041148 -0.039541  0.071635 -0.011336   \n",
            "8   0.125563  0.033356 -0.048423  0.110720 -0.018201 -0.147717 -0.042072   \n",
            "9  -0.068213 -0.115001 -0.004743 -0.028494 -0.027653  0.079632  0.049930   \n",
            "10 -0.032338 -0.013617  0.151200 -0.075678  0.075957  0.207855  0.038870   \n",
            "11  0.051104  0.012186  0.086417  0.028352  0.156211 -0.002386  0.086014   \n",
            "12  0.082138  0.005539 -0.044524  0.132945  0.106715  0.022446  0.116672   \n",
            "13 -0.043048 -0.140184  0.012286 -0.060239 -0.119047  0.023106 -0.004832   \n",
            "14 -0.021613  0.105474  0.025686  0.030302  0.136904 -0.020385 -0.046524   \n",
            "15  0.117869  0.126556  0.051337  0.002796 -0.029835 -0.061113  0.004300   \n",
            "\n",
            "           7         8         9  ...       136       137       138       139  \\\n",
            "0   0.087136  0.136641  0.125749  ...  0.105212  0.008460  0.086248 -0.005925   \n",
            "1   0.068729 -0.004232 -0.004889  ...  0.075859  0.051621  0.013238  0.045813   \n",
            "2  -0.050265  0.004671  0.000319  ...  0.094657  0.007707  0.022765  0.004527   \n",
            "3   0.084479  0.127317 -0.104037  ...  0.000802  0.089539 -0.032844  0.050453   \n",
            "4   0.048199  0.034387 -0.074277  ...  0.045809 -0.082850  0.027167 -0.046521   \n",
            "5  -0.040837 -0.020941  0.074329  ... -0.027615 -0.036336 -0.055786  0.008235   \n",
            "6  -0.068649 -0.066126 -0.024237  ...  0.095926  0.090875 -0.025141  0.018905   \n",
            "7   0.034141  0.078109 -0.036853  ...  0.086062 -0.007312  0.005437  0.073091   \n",
            "8   0.026514 -0.127686 -0.001961  ...  0.017386 -0.016188  0.018640  0.010524   \n",
            "9  -0.042845  0.092674  0.078472  ... -0.005325  0.106815  0.014020 -0.074019   \n",
            "10 -0.053325  0.046839  0.006819  ...  0.041650  0.170038 -0.044293  0.004112   \n",
            "11  0.042763 -0.056389 -0.043120  ...  0.042914 -0.032490  0.009025  0.004950   \n",
            "12  0.132660 -0.116526 -0.137380  ... -0.008512 -0.055690  0.034679  0.085180   \n",
            "13 -0.028790  0.080668 -0.064381  ... -0.020423  0.075944 -0.070604 -0.053503   \n",
            "14  0.006731 -0.044295  0.048296  ... -0.026215  0.104971  0.072772  0.041096   \n",
            "15  0.013864 -0.005772  0.182691  ... -0.027957 -0.073938 -0.020258 -0.038572   \n",
            "\n",
            "         140       141       142       143  layer  neuron_index  \n",
            "0  -0.033853  0.113257 -0.040305  0.088088  conv2             0  \n",
            "1   0.091032  0.005281 -0.026384  0.078543  conv2             1  \n",
            "2   0.002996  0.085109  0.008654  0.067731  conv2             2  \n",
            "3  -0.007134  0.030011 -0.008236 -0.030027  conv2             3  \n",
            "4   0.067527 -0.041539  0.057837  0.032542  conv2             4  \n",
            "5  -0.075360 -0.063744  0.026530  0.010399  conv2             5  \n",
            "6   0.031970 -0.055053  0.091992 -0.061894  conv2             6  \n",
            "7   0.030808 -0.011768  0.093748  0.089395  conv2             7  \n",
            "8  -0.089284  0.094356  0.033373  0.012280  conv2             8  \n",
            "9   0.011533 -0.078723  0.016492 -0.013510  conv2             9  \n",
            "10  0.017894 -0.050105  0.031401  0.045740  conv2            10  \n",
            "11  0.000354  0.074826  0.052800  0.083592  conv2            11  \n",
            "12  0.014633  0.005773  0.066127 -0.082251  conv2            12  \n",
            "13  0.020353 -0.001231  0.079536  0.032849  conv2            13  \n",
            "14  0.005841  0.030748 -0.013547 -0.054549  conv2            14  \n",
            "15 -0.075074  0.053019 -0.002690 -0.082208  conv2            15  \n",
            "\n",
            "[16 rows x 146 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_ACTIVATIONS DataFrame:\n",
            "   layer  neuron_index  activation_value\n",
            "0    fc1             0         -4.014961\n",
            "1    fc1             1         -4.939095\n",
            "2    fc1             2        -12.609343\n",
            "3    fc1             3         -3.177641\n",
            "4    fc1             4         -6.337552\n",
            "5    fc1             5         22.497536\n",
            "6    fc1             6         -5.789864\n",
            "7    fc1             7         15.405884\n",
            "8    fc1             8         -1.017147\n",
            "9    fc1             9         12.133198\n",
            "10   fc1            10         -5.322656\n",
            "11   fc1            11        -11.300261\n",
            "12   fc1            12         15.102704\n",
            "13   fc1            13         27.388807\n",
            "14   fc1            14         -5.886049\n",
            "15   fc1            15        -14.939456\n",
            "16   fc1            16         -6.477428\n",
            "17   fc1            17         -5.259388\n",
            "18   fc1            18          2.521544\n",
            "19   fc1            19         22.410891\n",
            "20   fc1            20         25.927332\n",
            "21   fc1            21        -11.313374\n",
            "22   fc1            22        -23.356455\n",
            "23   fc1            23          4.156610\n",
            "24   fc1            24         -6.488601\n",
            "25   fc1            25         -3.970361\n",
            "26   fc1            26         -6.406602\n",
            "27   fc1            27         -3.421440\n",
            "28   fc1            28         -6.524096\n",
            "29   fc1            29         -2.829480\n",
            "30   fc1            30         -6.995428\n",
            "31   fc1            31        -23.961676\n",
            "32   fc1            32         -8.067002\n",
            "33   fc1            33          2.612532\n",
            "34   fc1            34         -6.338154\n",
            "35   fc1            35         19.799667\n",
            "36   fc1            36         37.394009\n",
            "37   fc1            37          6.711173\n",
            "38   fc1            38        -12.076859\n",
            "39   fc1            39        -25.640799\n",
            "40   fc1            40         22.279001\n",
            "41   fc1            41         -4.457060\n",
            "42   fc1            42         -5.324222\n",
            "43   fc1            43         -4.992780\n",
            "44   fc1            44         -5.916689\n",
            "45   fc1            45         -6.266454\n",
            "46   fc1            46         -4.114596\n",
            "47   fc1            47         -3.450431\n",
            "48   fc1            48         -4.237681\n",
            "49   fc1            49         48.895531\n",
            "50   fc1            50          9.303164\n",
            "51   fc1            51         -3.268939\n",
            "52   fc1            52         24.060097\n",
            "53   fc1            53         -4.785760\n",
            "54   fc1            54         -4.319663\n",
            "55   fc1            55         -7.292960\n",
            "56   fc1            56         -2.757726\n",
            "57   fc1            57          5.350139\n",
            "58   fc1            58          6.514144\n",
            "59   fc1            59         -6.821272\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.012868  0.012967 -0.004651  0.004214  0.004450 -0.004286 -0.006892   \n",
            "1  -0.006668  0.001739  0.008233 -0.015734 -0.018150  0.006245 -0.009190   \n",
            "2  -0.004715 -0.008510 -0.027038 -0.012099 -0.017072 -0.001276 -0.004790   \n",
            "3  -0.022022  0.006158 -0.000070  0.005327 -0.010999 -0.013339 -0.000802   \n",
            "4  -0.021747  0.005062 -0.007141 -0.002509  0.002286 -0.004356 -0.013751   \n",
            "5   0.000488  0.012703  0.014533  0.016341 -0.004459  0.019696  0.002125   \n",
            "6   0.009300 -0.022666 -0.019488  0.000888 -0.000055 -0.011650 -0.015693   \n",
            "7   0.031482  0.014408  0.003062  0.002343  0.027174  0.003222  0.007536   \n",
            "8  -0.021455 -0.018184  0.010642 -0.009969 -0.002964 -0.022868  0.002633   \n",
            "9   0.008664  0.005595  0.018707  0.036951  0.012074  0.024569  0.035212   \n",
            "10 -0.015157 -0.015687 -0.020529  0.011342  0.001704 -0.014001 -0.023534   \n",
            "11  0.033904  0.009318  0.023171  0.003949  0.003715  0.000735  0.023219   \n",
            "12  0.024346  0.032305  0.026967  0.025876  0.010165  0.022697  0.011940   \n",
            "13  0.008851  0.010857  0.002167  0.024771  0.005659  0.021011  0.000578   \n",
            "14 -0.009209 -0.003457 -0.008076 -0.003801  0.010076 -0.013077 -0.018169   \n",
            "15 -0.022892 -0.007363 -0.001998 -0.025108 -0.005332 -0.008478 -0.006340   \n",
            "16 -0.011208 -0.008056 -0.020657 -0.002330  0.002901 -0.009348 -0.019556   \n",
            "17 -0.004303 -0.011543 -0.022747 -0.002148 -0.000974  0.003379  0.001518   \n",
            "18  0.014590  0.013593  0.026992  0.014942  0.002147  0.024546  0.023784   \n",
            "19  0.013653  0.015041  0.026944  0.017540  0.012362  0.005006  0.020771   \n",
            "20  0.000926 -0.002540  0.000976 -0.007426  0.008134  0.009121  0.006932   \n",
            "21 -0.008481 -0.009680 -0.015762 -0.008213  0.005569 -0.011419 -0.018695   \n",
            "22  0.002687 -0.004754  0.025822  0.022829  0.026687  0.024950  0.015143   \n",
            "23  0.012675 -0.008788  0.020707 -0.002437 -0.000562 -0.001121  0.009253   \n",
            "24 -0.021873 -0.006459  0.000207 -0.022165  0.008027 -0.006639  0.011209   \n",
            "25 -0.009614 -0.022043 -0.007360  0.004037 -0.017339 -0.001730 -0.005209   \n",
            "26 -0.003378 -0.006826 -0.019353 -0.008899  0.007771  0.007703 -0.008105   \n",
            "27 -0.009939 -0.008868  0.000785  0.008521 -0.017249  0.009543 -0.006277   \n",
            "28 -0.012323  0.002773 -0.002832  0.008380 -0.001049 -0.023035 -0.010383   \n",
            "29  0.014561  0.016938  0.017683 -0.001824  0.010139  0.014729  0.012067   \n",
            "30 -0.019001 -0.009498 -0.020953 -0.004602  0.011816  0.005326 -0.018519   \n",
            "31  0.000243  0.000025  0.021961 -0.006239  0.018972 -0.004205  0.002377   \n",
            "32 -0.003725 -0.003355 -0.020392  0.005503 -0.013929 -0.013021  0.001984   \n",
            "33  0.016569  0.002955  0.006740  0.025918  0.010518  0.006316  0.009447   \n",
            "34 -0.021455 -0.010343  0.011155  0.004942 -0.004358 -0.012782 -0.010427   \n",
            "35 -0.004436 -0.022906 -0.004501  0.005599  0.007464 -0.000397 -0.000361   \n",
            "36  0.028495  0.008049  0.033686  0.027099  0.001744  0.033556  0.005032   \n",
            "37  0.017607  0.014472  0.003794  0.007778  0.026253  0.026272  0.000011   \n",
            "38 -0.002645  0.017745  0.021209 -0.002706  0.004354  0.021500  0.015463   \n",
            "39  0.013660  0.008610  0.018474  0.003934  0.033713  0.001638  0.016512   \n",
            "40  0.030918  0.000622  0.009956  0.019433  0.000461 -0.000907  0.013701   \n",
            "41 -0.014202 -0.010007 -0.013808 -0.019121  0.001727 -0.010824  0.006965   \n",
            "42 -0.018572 -0.000763  0.011354 -0.020072 -0.000430  0.000737 -0.016534   \n",
            "43 -0.016530 -0.011671 -0.013214 -0.002635  0.008195 -0.009454 -0.014988   \n",
            "44 -0.004162  0.004954 -0.016971 -0.016766 -0.005005 -0.002056 -0.009664   \n",
            "45 -0.006126  0.000294 -0.010289 -0.013374 -0.021398 -0.016979 -0.005077   \n",
            "46 -0.003019  0.003975 -0.013475  0.004994 -0.012704 -0.002466 -0.018315   \n",
            "47  0.002745  0.005883 -0.017230  0.000825 -0.021160 -0.008062 -0.022425   \n",
            "48 -0.000349 -0.019200  0.004244  0.012784  0.002108 -0.011295 -0.014164   \n",
            "49  0.019498  0.027400  0.033523  0.031643  0.003375  0.024539  0.018732   \n",
            "50  0.002034  0.025831  0.021195  0.015002  0.011998  0.006490  0.015592   \n",
            "51  0.002681 -0.004163 -0.002470  0.002025  0.000847 -0.004836 -0.020047   \n",
            "52  0.031499  0.006817  0.030368  0.016483  0.009225  0.011744  0.023579   \n",
            "53 -0.002231 -0.016763 -0.012501  0.002132 -0.016919  0.000812  0.009893   \n",
            "54 -0.016136  0.002641  0.006465  0.005333  0.000611 -0.021958  0.005562   \n",
            "55  0.006558  0.001421 -0.006817 -0.021912 -0.021949 -0.012012  0.008985   \n",
            "56 -0.013326  0.005539 -0.001117  0.010829  0.002428 -0.019694 -0.017891   \n",
            "57  0.000068  0.003597  0.006211  0.004677 -0.000560  0.023743  0.012332   \n",
            "58  0.028027  0.022394  0.015621  0.022584  0.027930  0.013303  0.023036   \n",
            "59 -0.016706  0.001549  0.002360 -0.005833 -0.013946 -0.005872 -0.015406   \n",
            "\n",
            "           7         8         9  ...      3128      3129      3130      3131  \\\n",
            "0   0.007324 -0.009616 -0.005067  ... -0.006856  0.010652  0.010737 -0.005627   \n",
            "1  -0.018485 -0.011243 -0.014907  ...  0.003302 -0.023178 -0.002841 -0.002705   \n",
            "2   0.003348 -0.021924  0.002252  ... -0.004555 -0.022496 -0.027291  0.000060   \n",
            "3  -0.006648 -0.017124  0.011142  ... -0.008438  0.007751 -0.003895 -0.014379   \n",
            "4   0.009772 -0.011616 -0.003061  ... -0.013893 -0.023509  0.007052  0.009660   \n",
            "5   0.008894  0.007604  0.022646  ...  0.023266  0.041666  0.029488  0.033174   \n",
            "6  -0.003789  0.009797 -0.006847  ...  0.009045 -0.017039 -0.017816  0.009705   \n",
            "7   0.004008  0.015966  0.020602  ...  0.009192  0.034439  0.014094  0.028504   \n",
            "8  -0.003198 -0.022776  0.003764  ... -0.001452 -0.006950 -0.022771  0.003933   \n",
            "9   0.030930  0.004664  0.003770  ...  0.004832 -0.009257 -0.008188  0.001435   \n",
            "10 -0.013386 -0.013077 -0.018894  ... -0.001648  0.000762 -0.017888 -0.016899   \n",
            "11  0.006491  0.032536  0.000740  ...  0.003196  0.004032  0.010177  0.012247   \n",
            "12  0.025155  0.015860  0.000769  ...  0.006513  0.015600  0.012484  0.016776   \n",
            "13  0.029623  0.026839 -0.001954  ...  0.036832  0.027887  0.015517  0.010580   \n",
            "14 -0.016337  0.004727  0.010331  ... -0.012719 -0.016295  0.002682 -0.014291   \n",
            "15 -0.017671 -0.001590 -0.007524  ... -0.027042 -0.015744 -0.009734 -0.021105   \n",
            "16 -0.015554  0.001033  0.005761  ...  0.009729 -0.022332 -0.021350 -0.016759   \n",
            "17  0.010020 -0.019688 -0.004400  ...  0.007721 -0.004402 -0.016819 -0.015915   \n",
            "18  0.002784 -0.006211  0.013223  ...  0.024819  0.008977  0.035165  0.001899   \n",
            "19  0.014684  0.027412  0.018771  ...  0.027893  0.023726  0.017920  0.016699   \n",
            "20 -0.019416 -0.022032 -0.013255  ...  0.047160  0.036355  0.025181  0.022490   \n",
            "21  0.001027 -0.005196 -0.004424  ... -0.011853 -0.003937  0.003567  0.000304   \n",
            "22  0.004678  0.017792 -0.000148  ...  0.014265  0.008728 -0.017063  0.005836   \n",
            "23  0.005949 -0.008079  0.000984  ... -0.013547 -0.005859  0.002466  0.003944   \n",
            "24 -0.012757 -0.023823 -0.018663  ...  0.011678 -0.019435  0.003086 -0.019769   \n",
            "25 -0.010399  0.010380 -0.019753  ...  0.000844  0.010552 -0.019871 -0.019154   \n",
            "26  0.010319 -0.010890  0.007982  ...  0.000887  0.000956  0.001107 -0.014635   \n",
            "27 -0.004731 -0.009723 -0.003905  ...  0.005012 -0.016505 -0.011187 -0.001365   \n",
            "28 -0.015085 -0.001597 -0.002212  ... -0.009379  0.010932  0.000835  0.011187   \n",
            "29  0.009418  0.025151  0.020776  ... -0.004506 -0.015641 -0.013120 -0.010891   \n",
            "30 -0.016916  0.009760 -0.014054  ...  0.007682 -0.003761 -0.015353 -0.000961   \n",
            "31  0.012016 -0.000764 -0.007299  ... -0.012403 -0.009862 -0.011390 -0.003945   \n",
            "32 -0.024166  0.000700  0.009655  ... -0.019903 -0.006628 -0.020596  0.001379   \n",
            "33  0.010933  0.003280  0.013948  ... -0.003015  0.020623 -0.000906  0.015280   \n",
            "34 -0.017208  0.006274  0.003570  ... -0.013828 -0.008093 -0.019012 -0.001721   \n",
            "35  0.005101 -0.010963 -0.009127  ... -0.002773  0.011193  0.019547  0.002818   \n",
            "36  0.012425  0.009884  0.031366  ...  0.042560  0.039935  0.023204  0.029524   \n",
            "37  0.023419  0.006894  0.026621  ...  0.004115  0.000018  0.020470 -0.002871   \n",
            "38  0.018896  0.024307 -0.000390  ... -0.004216 -0.012399 -0.001571 -0.018753   \n",
            "39  0.010963  0.030405  0.032920  ... -0.013158  0.001995 -0.017575  0.000686   \n",
            "40  0.026295  0.013387 -0.001529  ...  0.043029  0.038009  0.020855  0.017207   \n",
            "41 -0.004680 -0.013764 -0.007863  ...  0.008035  0.010515 -0.006477  0.011474   \n",
            "42 -0.000678  0.009945  0.003108  ...  0.008813  0.006511 -0.001635 -0.017173   \n",
            "43 -0.011972 -0.016392  0.001932  ... -0.010337 -0.012674 -0.018367 -0.022919   \n",
            "44  0.003259 -0.000585  0.001807  ...  0.008413 -0.014572 -0.008983  0.005895   \n",
            "45  0.006896 -0.008086 -0.009275  ... -0.005778  0.007000  0.002406 -0.008275   \n",
            "46  0.010720 -0.002166 -0.014401  ... -0.013205 -0.022235 -0.002017 -0.000171   \n",
            "47 -0.005961 -0.003954  0.010139  ...  0.011701  0.001282 -0.018512 -0.003118   \n",
            "48 -0.013545  0.006565 -0.005001  ... -0.010218  0.010282 -0.019655  0.010408   \n",
            "49  0.018647  0.026510  0.016471  ...  0.039909  0.049060  0.028912  0.045582   \n",
            "50  0.000300  0.010269  0.008490  ...  0.023184 -0.001352  0.019045  0.003499   \n",
            "51  0.004892  0.007555  0.001219  ... -0.016053  0.001421 -0.013407 -0.006220   \n",
            "52  0.030600  0.026217  0.023260  ...  0.017529  0.006934  0.017406  0.020949   \n",
            "53 -0.010024 -0.009774 -0.018796  ... -0.015735 -0.020443  0.008829 -0.016565   \n",
            "54  0.011039 -0.000971 -0.004749  ... -0.011213  0.000786 -0.019802 -0.001971   \n",
            "55 -0.017711 -0.011881  0.008669  ...  0.003574  0.005007 -0.005511 -0.009074   \n",
            "56 -0.018805 -0.010618  0.002681  ... -0.014544 -0.013959  0.007968 -0.004889   \n",
            "57  0.013836  0.000618  0.024202  ...  0.020163  0.034249  0.023702  0.032346   \n",
            "58  0.020271 -0.000923  0.004961  ...  0.030553  0.023524  0.025896  0.012225   \n",
            "59  0.006506 -0.000810  0.000023  ... -0.021418 -0.022299 -0.008617 -0.017246   \n",
            "\n",
            "        3132      3133      3134      3135  layer  neuron_index  \n",
            "0  -0.006300  0.000761 -0.010077 -0.010894    fc1             0  \n",
            "1   0.007173  0.003588 -0.009852 -0.016465    fc1             1  \n",
            "2   0.005837 -0.015450 -0.013901  0.002791    fc1             2  \n",
            "3  -0.012918  0.007607 -0.013114 -0.016117    fc1             3  \n",
            "4   0.003922 -0.010699  0.009847 -0.021692    fc1             4  \n",
            "5   0.028678  0.014802  0.011530  0.006059    fc1             5  \n",
            "6  -0.014932  0.004915  0.002246 -0.007906    fc1             6  \n",
            "7   0.013501  0.022247  0.021532 -0.003630    fc1             7  \n",
            "8  -0.006297 -0.015515 -0.013881  0.004368    fc1             8  \n",
            "9  -0.001923  0.008458  0.021580  0.029945    fc1             9  \n",
            "10 -0.000172 -0.004697 -0.007486  0.003054    fc1            10  \n",
            "11 -0.007124 -0.002595 -0.013320 -0.004145    fc1            11  \n",
            "12 -0.001142  0.025493  0.015860  0.012417    fc1            12  \n",
            "13  0.008905  0.014851  0.010457  0.007798    fc1            13  \n",
            "14 -0.010744 -0.008016 -0.019082 -0.007001    fc1            14  \n",
            "15 -0.008899 -0.000829 -0.016586 -0.028002    fc1            15  \n",
            "16  0.011897 -0.018047 -0.002446 -0.017222    fc1            16  \n",
            "17 -0.020536  0.006682 -0.007786 -0.019112    fc1            17  \n",
            "18  0.014848  0.023633  0.009814 -0.009907    fc1            18  \n",
            "19  0.032436  0.014681  0.007802  0.015888    fc1            19  \n",
            "20  0.010047  0.026064  0.016482  0.030144    fc1            20  \n",
            "21  0.008994  0.010006 -0.018727 -0.003075    fc1            21  \n",
            "22 -0.000216 -0.005094  0.018505 -0.008893    fc1            22  \n",
            "23  0.027010  0.004446 -0.004685  0.011521    fc1            23  \n",
            "24 -0.006039  0.005686 -0.007776  0.007923    fc1            24  \n",
            "25 -0.013028 -0.005355 -0.013795  0.000189    fc1            25  \n",
            "26  0.001260  0.002817 -0.000567 -0.018858    fc1            26  \n",
            "27 -0.012066  0.005803  0.008898 -0.020256    fc1            27  \n",
            "28 -0.021011 -0.016277 -0.021642 -0.017041    fc1            28  \n",
            "29  0.015582 -0.001358  0.009005  0.020085    fc1            29  \n",
            "30  0.009142 -0.011048 -0.019065 -0.022964    fc1            30  \n",
            "31 -0.017578  0.018184 -0.014494  0.006414    fc1            31  \n",
            "32 -0.007214 -0.020134 -0.002980 -0.009061    fc1            32  \n",
            "33 -0.002171 -0.004688  0.024596  0.012860    fc1            33  \n",
            "34  0.006539  0.009605 -0.015453 -0.016520    fc1            34  \n",
            "35 -0.018720 -0.022744 -0.017856  0.009409    fc1            35  \n",
            "36  0.040159  0.015496  0.046070  0.038217    fc1            36  \n",
            "37  0.016869  0.005396  0.012609  0.013487    fc1            37  \n",
            "38 -0.000505  0.023441  0.003526  0.005332    fc1            38  \n",
            "39 -0.019023 -0.006869 -0.015280  0.005979    fc1            39  \n",
            "40  0.027607  0.004566  0.021181  0.012148    fc1            40  \n",
            "41  0.004879 -0.012685 -0.009628 -0.022963    fc1            41  \n",
            "42  0.007932 -0.012811 -0.015902 -0.008890    fc1            42  \n",
            "43 -0.016014  0.007799  0.005429 -0.008904    fc1            43  \n",
            "44  0.004329  0.007101 -0.003699 -0.012319    fc1            44  \n",
            "45  0.005780  0.009909 -0.004622 -0.021927    fc1            45  \n",
            "46 -0.013934 -0.011455 -0.007828 -0.010529    fc1            46  \n",
            "47 -0.016764 -0.023845 -0.003573  0.011422    fc1            47  \n",
            "48 -0.015247 -0.011081 -0.016139 -0.007430    fc1            48  \n",
            "49  0.037967  0.039943  0.047666  0.032830    fc1            49  \n",
            "50  0.002823 -0.002692 -0.008353  0.002865    fc1            50  \n",
            "51 -0.014761  0.009049 -0.008702 -0.023180    fc1            51  \n",
            "52  0.023632  0.022728  0.018886  0.025511    fc1            52  \n",
            "53 -0.018722  0.011494  0.004309 -0.023839    fc1            53  \n",
            "54  0.000339  0.006231  0.007703 -0.013026    fc1            54  \n",
            "55 -0.014757 -0.008799 -0.020483 -0.005706    fc1            55  \n",
            "56 -0.014795  0.005220 -0.021989 -0.015774    fc1            56  \n",
            "57 -0.000363  0.006102 -0.002093 -0.001637    fc1            57  \n",
            "58  0.026156  0.011964  0.002414  0.006485    fc1            58  \n",
            "59 -0.003636  0.008666 -0.018831 -0.015422    fc1            59  \n",
            "\n",
            "[60 rows x 3138 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_ACTIVATIONS DataFrame:\n",
            "  layer  neuron_index  activation_value\n",
            "0   fc2             0        -14.909154\n",
            "1   fc2             1          5.598890\n",
            "2   fc2             2         -3.607817\n",
            "3   fc2             3         -8.763968\n",
            "4   fc2             4        -15.308965\n",
            "5   fc2             5        -15.411082\n",
            "6   fc2             6        -14.035707\n",
            "7   fc2             7         -5.430692\n",
            "8   fc2             8        -11.139443\n",
            "9   fc2             9         25.107201\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_CONNECTIONS DataFrame:\n",
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.107950 -0.100333 -0.125853  0.091329 -0.090712 -0.024337 -0.035301   \n",
            "1  0.131501 -0.110936 -0.085158  0.066024 -0.037845 -0.071343 -0.052721   \n",
            "2  0.050709 -0.033907 -0.077746 -0.074659 -0.076966 -0.074498 -0.088701   \n",
            "3  0.049182 -0.027318 -0.033456 -0.035291 -0.043763 -0.003068  0.013614   \n",
            "4  0.042645  0.088435  0.038246  0.004014 -0.031166 -0.035241 -0.033445   \n",
            "5  0.086447 -0.126483 -0.040570  0.021679  0.066402 -0.114365  0.069705   \n",
            "6 -0.080449  0.085518 -0.090617 -0.053715 -0.023148 -0.005910  0.053310   \n",
            "7  0.092976 -0.108292 -0.003311 -0.020869  0.013422 -0.048318  0.010532   \n",
            "8 -0.123973 -0.038942  0.098014 -0.079174  0.093177  0.066027  0.115322   \n",
            "9  0.017517 -0.083782  0.037875 -0.062554 -0.064439  0.184223  0.006078   \n",
            "\n",
            "          7         8         9  ...        52        53        54        55  \\\n",
            "0  0.090453 -0.134711  0.053470  ... -0.070573  0.083195  0.095116  0.083355   \n",
            "1 -0.112283  0.045542  0.064597  ...  0.047196  0.127483  0.055454  0.038554   \n",
            "2  0.057485 -0.012508 -0.048672  ...  0.098187  0.072752 -0.079741 -0.097680   \n",
            "3 -0.134513 -0.108175  0.041074  ... -0.005931 -0.089947 -0.010503  0.077362   \n",
            "4 -0.040576  0.073950 -0.058569  ... -0.079893 -0.038126 -0.095315  0.112316   \n",
            "5 -0.126576 -0.152112 -0.069325  ... -0.028109  0.103588 -0.003243  0.043243   \n",
            "6  0.102617  0.096054  0.003210  ... -0.036933  0.010865  0.066728  0.112904   \n",
            "7  0.093271  0.076127 -0.004166  ... -0.105497  0.043385  0.102613  0.121523   \n",
            "8  0.016770 -0.150593  0.057095  ... -0.014134 -0.041150  0.043610 -0.033362   \n",
            "9  0.090238 -0.031772 -0.016623  ... -0.092037  0.067543 -0.015439  0.107121   \n",
            "\n",
            "         56        57        58        59  layer  neuron_index  \n",
            "0 -0.068317  0.057229 -0.010008 -0.046776    fc2             0  \n",
            "1  0.154287 -0.029886 -0.088726  0.017459    fc2             1  \n",
            "2 -0.002260 -0.142917  0.077585  0.014963    fc2             2  \n",
            "3 -0.088103 -0.110321  0.010353  0.046034    fc2             3  \n",
            "4  0.099081  0.096792  0.056523 -0.063165    fc2             4  \n",
            "5 -0.133819 -0.110977 -0.123005 -0.015103    fc2             5  \n",
            "6  0.097748 -0.037395  0.050451 -0.001918    fc2             6  \n",
            "7 -0.045865 -0.148744  0.003459 -0.012868    fc2             7  \n",
            "8 -0.111366 -0.025910  0.000013  0.098205    fc2             8  \n",
            "9 -0.079098  0.062496 -0.018937  0.063364    fc2             9  \n",
            "\n",
            "[10 rows x 62 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Image 2 ---\n",
            "True Label: 9\n",
            "Predicted Label: 9\n",
            "\n",
            "CONV1_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv1             0          0.317554\n",
            "1   conv1             1          0.313719\n",
            "2   conv1             2          0.138510\n",
            "3   conv1             3          0.073251\n",
            "4   conv1             4          0.128080\n",
            "5   conv1             5          0.250331\n",
            "6   conv1             6          0.028887\n",
            "7   conv1             7          0.347862\n",
            "8   conv1             8          0.149309\n",
            "9   conv1             9          0.146880\n",
            "10  conv1            10          0.129656\n",
            "11  conv1            11          0.229132\n",
            "12  conv1            12          0.011162\n",
            "13  conv1            13          0.122090\n",
            "14  conv1            14         -0.013045\n",
            "15  conv1            15          0.027409\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.400412  0.399371  0.071231 -0.067315  0.237489  0.328836 -0.150906   \n",
            "1   0.062670  0.420839  0.423627 -0.087857  0.270959 -0.074010  0.234360   \n",
            "2   0.077322  0.239514  0.362599  0.313162 -0.159267 -0.146957 -0.197186   \n",
            "3  -0.198520  0.416571  0.189112 -0.133381  0.282581  0.187163 -0.016161   \n",
            "4  -0.100635 -0.007484  0.208964  0.262549  0.069296  0.175801  0.311726   \n",
            "5  -0.273976 -0.003054  0.224042  0.069278  0.076746  0.157825  0.075656   \n",
            "6  -0.428757 -0.251183 -0.254338  0.081380  0.144253  0.244236  0.337603   \n",
            "7   0.378913  0.399986  0.071193  0.172418 -0.026089  0.130204 -0.220100   \n",
            "8   0.225943  0.192161  0.387705  0.135396 -0.048392 -0.038235 -0.115391   \n",
            "9   0.307195 -0.180329 -0.171382  0.319853  0.081811  0.181218  0.174932   \n",
            "10  0.130797  0.370466  0.380331  0.015307 -0.078076  0.016426 -0.083861   \n",
            "11  0.392175  0.247163  0.281231  0.259225 -0.270856 -0.165327  0.253949   \n",
            "12 -0.210740 -0.048877 -0.131766  0.335698  0.006075 -0.116493 -0.017947   \n",
            "13  0.119667  0.026561  0.093274  0.343750  0.204018 -0.236069  0.372014   \n",
            "14  0.198226 -0.245182  0.121569 -0.049056 -0.133100 -0.077453  0.442164   \n",
            "15  0.209107  0.173020 -0.301732  0.125146 -0.021763 -0.264287  0.322271   \n",
            "\n",
            "           7         8  layer  neuron_index  \n",
            "0   0.392861 -0.045586  conv1             0  \n",
            "1  -0.093755  0.006661  conv1             1  \n",
            "2   0.068652  0.012842  conv1             2  \n",
            "3  -0.195803  0.062113  conv1             3  \n",
            "4  -0.130634  0.326892  conv1             4  \n",
            "5   0.073296 -0.115009  conv1             5  \n",
            "6  -0.101311  0.121107  conv1             6  \n",
            "7   0.377975 -0.272754  conv1             7  \n",
            "8   0.228945  0.188063  conv1             8  \n",
            "9   0.345546  0.172957  conv1             9  \n",
            "10  0.056517  0.259929  conv1            10  \n",
            "11 -0.198929  0.022492  conv1            11  \n",
            "12  0.285077  0.004282  conv1            12  \n",
            "13  0.204587 -0.130169  conv1            13  \n",
            "14 -0.012433 -0.308704  conv1            14  \n",
            "15 -0.069897  0.135285  conv1            15  \n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv2             0          0.196069\n",
            "1   conv2             1          0.750793\n",
            "2   conv2             2          0.337213\n",
            "3   conv2             3          0.897991\n",
            "4   conv2             4         -0.253787\n",
            "5   conv2             5         -0.274717\n",
            "6   conv2             6          0.526959\n",
            "7   conv2             7          0.385343\n",
            "8   conv2             8          0.227153\n",
            "9   conv2             9          0.071072\n",
            "10  conv2            10          0.810812\n",
            "11  conv2            11          0.325627\n",
            "12  conv2            12          0.459374\n",
            "13  conv2            13          0.002668\n",
            "14  conv2            14          0.514260\n",
            "15  conv2            15          0.307381\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.053389  0.023715 -0.082655 -0.048249 -0.076035 -0.150770  0.021789   \n",
            "1   0.047589 -0.093007  0.005160  0.069936  0.136776  0.046387  0.010860   \n",
            "2   0.058461  0.100847 -0.038368  0.022629  0.103998 -0.064163 -0.030390   \n",
            "3  -0.035365  0.023469  0.107092  0.101842  0.143467  0.217515 -0.012316   \n",
            "4   0.062249 -0.131436 -0.046978  0.021597 -0.080085 -0.078712 -0.103583   \n",
            "5  -0.019309 -0.080785  0.035847  0.020273  0.009665  0.035786 -0.006190   \n",
            "6   0.049547  0.038741 -0.036449 -0.002058  0.041931  0.053819 -0.005733   \n",
            "7  -0.152914 -0.041600  0.115807 -0.041148 -0.039541  0.071635 -0.011336   \n",
            "8   0.125563  0.033356 -0.048423  0.110720 -0.018201 -0.147717 -0.042072   \n",
            "9  -0.068213 -0.115001 -0.004743 -0.028494 -0.027653  0.079632  0.049930   \n",
            "10 -0.032338 -0.013617  0.151200 -0.075678  0.075957  0.207855  0.038870   \n",
            "11  0.051104  0.012186  0.086417  0.028352  0.156211 -0.002386  0.086014   \n",
            "12  0.082138  0.005539 -0.044524  0.132945  0.106715  0.022446  0.116672   \n",
            "13 -0.043048 -0.140184  0.012286 -0.060239 -0.119047  0.023106 -0.004832   \n",
            "14 -0.021613  0.105474  0.025686  0.030302  0.136904 -0.020385 -0.046524   \n",
            "15  0.117869  0.126556  0.051337  0.002796 -0.029835 -0.061113  0.004300   \n",
            "\n",
            "           7         8         9  ...       136       137       138       139  \\\n",
            "0   0.087136  0.136641  0.125749  ...  0.105212  0.008460  0.086248 -0.005925   \n",
            "1   0.068729 -0.004232 -0.004889  ...  0.075859  0.051621  0.013238  0.045813   \n",
            "2  -0.050265  0.004671  0.000319  ...  0.094657  0.007707  0.022765  0.004527   \n",
            "3   0.084479  0.127317 -0.104037  ...  0.000802  0.089539 -0.032844  0.050453   \n",
            "4   0.048199  0.034387 -0.074277  ...  0.045809 -0.082850  0.027167 -0.046521   \n",
            "5  -0.040837 -0.020941  0.074329  ... -0.027615 -0.036336 -0.055786  0.008235   \n",
            "6  -0.068649 -0.066126 -0.024237  ...  0.095926  0.090875 -0.025141  0.018905   \n",
            "7   0.034141  0.078109 -0.036853  ...  0.086062 -0.007312  0.005437  0.073091   \n",
            "8   0.026514 -0.127686 -0.001961  ...  0.017386 -0.016188  0.018640  0.010524   \n",
            "9  -0.042845  0.092674  0.078472  ... -0.005325  0.106815  0.014020 -0.074019   \n",
            "10 -0.053325  0.046839  0.006819  ...  0.041650  0.170038 -0.044293  0.004112   \n",
            "11  0.042763 -0.056389 -0.043120  ...  0.042914 -0.032490  0.009025  0.004950   \n",
            "12  0.132660 -0.116526 -0.137380  ... -0.008512 -0.055690  0.034679  0.085180   \n",
            "13 -0.028790  0.080668 -0.064381  ... -0.020423  0.075944 -0.070604 -0.053503   \n",
            "14  0.006731 -0.044295  0.048296  ... -0.026215  0.104971  0.072772  0.041096   \n",
            "15  0.013864 -0.005772  0.182691  ... -0.027957 -0.073938 -0.020258 -0.038572   \n",
            "\n",
            "         140       141       142       143  layer  neuron_index  \n",
            "0  -0.033853  0.113257 -0.040305  0.088088  conv2             0  \n",
            "1   0.091032  0.005281 -0.026384  0.078543  conv2             1  \n",
            "2   0.002996  0.085109  0.008654  0.067731  conv2             2  \n",
            "3  -0.007134  0.030011 -0.008236 -0.030027  conv2             3  \n",
            "4   0.067527 -0.041539  0.057837  0.032542  conv2             4  \n",
            "5  -0.075360 -0.063744  0.026530  0.010399  conv2             5  \n",
            "6   0.031970 -0.055053  0.091992 -0.061894  conv2             6  \n",
            "7   0.030808 -0.011768  0.093748  0.089395  conv2             7  \n",
            "8  -0.089284  0.094356  0.033373  0.012280  conv2             8  \n",
            "9   0.011533 -0.078723  0.016492 -0.013510  conv2             9  \n",
            "10  0.017894 -0.050105  0.031401  0.045740  conv2            10  \n",
            "11  0.000354  0.074826  0.052800  0.083592  conv2            11  \n",
            "12  0.014633  0.005773  0.066127 -0.082251  conv2            12  \n",
            "13  0.020353 -0.001231  0.079536  0.032849  conv2            13  \n",
            "14  0.005841  0.030748 -0.013547 -0.054549  conv2            14  \n",
            "15 -0.075074  0.053019 -0.002690 -0.082208  conv2            15  \n",
            "\n",
            "[16 rows x 146 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_ACTIVATIONS DataFrame:\n",
            "   layer  neuron_index  activation_value\n",
            "0    fc1             0         -2.628375\n",
            "1    fc1             1         -3.315416\n",
            "2    fc1             2         -9.309978\n",
            "3    fc1             3         -2.831021\n",
            "4    fc1             4         -4.635664\n",
            "5    fc1             5         19.549892\n",
            "6    fc1             6         -5.090425\n",
            "7    fc1             7         11.944546\n",
            "8    fc1             8         -1.275163\n",
            "9    fc1             9          7.318295\n",
            "10   fc1            10         -4.138322\n",
            "11   fc1            11         -8.309507\n",
            "12   fc1            12          9.892439\n",
            "13   fc1            13         20.720623\n",
            "14   fc1            14         -3.919506\n",
            "15   fc1            15        -10.726952\n",
            "16   fc1            16         -5.099340\n",
            "17   fc1            17         -4.343733\n",
            "18   fc1            18          3.737906\n",
            "19   fc1            19         18.336639\n",
            "20   fc1            20         12.120880\n",
            "21   fc1            21         -8.739196\n",
            "22   fc1            22        -19.514563\n",
            "23   fc1            23          5.395497\n",
            "24   fc1            24         -5.175546\n",
            "25   fc1            25         -2.880133\n",
            "26   fc1            26         -5.130176\n",
            "27   fc1            27         -3.386496\n",
            "28   fc1            28         -5.230077\n",
            "29   fc1            29         -2.866092\n",
            "30   fc1            30         -5.160532\n",
            "31   fc1            31        -19.518284\n",
            "32   fc1            32         -6.915961\n",
            "33   fc1            33          1.759902\n",
            "34   fc1            34         -4.920274\n",
            "35   fc1            35         13.456693\n",
            "36   fc1            36         29.990736\n",
            "37   fc1            37          4.445462\n",
            "38   fc1            38        -10.662566\n",
            "39   fc1            39        -20.528700\n",
            "40   fc1            40         19.060373\n",
            "41   fc1            41         -3.829919\n",
            "42   fc1            42         -3.974333\n",
            "43   fc1            43         -4.174093\n",
            "44   fc1            44         -4.947381\n",
            "45   fc1            45         -5.110422\n",
            "46   fc1            46         -2.829086\n",
            "47   fc1            47         -2.500293\n",
            "48   fc1            48         -3.910422\n",
            "49   fc1            49         37.694946\n",
            "50   fc1            50          8.011145\n",
            "51   fc1            51         -2.574083\n",
            "52   fc1            52         17.624670\n",
            "53   fc1            53         -3.507314\n",
            "54   fc1            54         -3.416935\n",
            "55   fc1            55         -5.791512\n",
            "56   fc1            56         -2.713559\n",
            "57   fc1            57          5.391152\n",
            "58   fc1            58          7.059021\n",
            "59   fc1            59         -5.039438\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.012868  0.012967 -0.004651  0.004214  0.004450 -0.004286 -0.006892   \n",
            "1  -0.006668  0.001739  0.008233 -0.015734 -0.018150  0.006245 -0.009190   \n",
            "2  -0.004715 -0.008510 -0.027038 -0.012099 -0.017072 -0.001276 -0.004790   \n",
            "3  -0.022022  0.006158 -0.000070  0.005327 -0.010999 -0.013339 -0.000802   \n",
            "4  -0.021747  0.005062 -0.007141 -0.002509  0.002286 -0.004356 -0.013751   \n",
            "5   0.000488  0.012703  0.014533  0.016341 -0.004459  0.019696  0.002125   \n",
            "6   0.009300 -0.022666 -0.019488  0.000888 -0.000055 -0.011650 -0.015693   \n",
            "7   0.031482  0.014408  0.003062  0.002343  0.027174  0.003222  0.007536   \n",
            "8  -0.021455 -0.018184  0.010642 -0.009969 -0.002964 -0.022868  0.002633   \n",
            "9   0.008664  0.005595  0.018707  0.036951  0.012074  0.024569  0.035212   \n",
            "10 -0.015157 -0.015687 -0.020529  0.011342  0.001704 -0.014001 -0.023534   \n",
            "11  0.033904  0.009318  0.023171  0.003949  0.003715  0.000735  0.023219   \n",
            "12  0.024346  0.032305  0.026967  0.025876  0.010165  0.022697  0.011940   \n",
            "13  0.008851  0.010857  0.002167  0.024771  0.005659  0.021011  0.000578   \n",
            "14 -0.009209 -0.003457 -0.008076 -0.003801  0.010076 -0.013077 -0.018169   \n",
            "15 -0.022892 -0.007363 -0.001998 -0.025108 -0.005332 -0.008478 -0.006340   \n",
            "16 -0.011208 -0.008056 -0.020657 -0.002330  0.002901 -0.009348 -0.019556   \n",
            "17 -0.004303 -0.011543 -0.022747 -0.002148 -0.000974  0.003379  0.001518   \n",
            "18  0.014590  0.013593  0.026992  0.014942  0.002147  0.024546  0.023784   \n",
            "19  0.013653  0.015041  0.026944  0.017540  0.012362  0.005006  0.020771   \n",
            "20  0.000926 -0.002540  0.000976 -0.007426  0.008134  0.009121  0.006932   \n",
            "21 -0.008481 -0.009680 -0.015762 -0.008213  0.005569 -0.011419 -0.018695   \n",
            "22  0.002687 -0.004754  0.025822  0.022829  0.026687  0.024950  0.015143   \n",
            "23  0.012675 -0.008788  0.020707 -0.002437 -0.000562 -0.001121  0.009253   \n",
            "24 -0.021873 -0.006459  0.000207 -0.022165  0.008027 -0.006639  0.011209   \n",
            "25 -0.009614 -0.022043 -0.007360  0.004037 -0.017339 -0.001730 -0.005209   \n",
            "26 -0.003378 -0.006826 -0.019353 -0.008899  0.007771  0.007703 -0.008105   \n",
            "27 -0.009939 -0.008868  0.000785  0.008521 -0.017249  0.009543 -0.006277   \n",
            "28 -0.012323  0.002773 -0.002832  0.008380 -0.001049 -0.023035 -0.010383   \n",
            "29  0.014561  0.016938  0.017683 -0.001824  0.010139  0.014729  0.012067   \n",
            "30 -0.019001 -0.009498 -0.020953 -0.004602  0.011816  0.005326 -0.018519   \n",
            "31  0.000243  0.000025  0.021961 -0.006239  0.018972 -0.004205  0.002377   \n",
            "32 -0.003725 -0.003355 -0.020392  0.005503 -0.013929 -0.013021  0.001984   \n",
            "33  0.016569  0.002955  0.006740  0.025918  0.010518  0.006316  0.009447   \n",
            "34 -0.021455 -0.010343  0.011155  0.004942 -0.004358 -0.012782 -0.010427   \n",
            "35 -0.004436 -0.022906 -0.004501  0.005599  0.007464 -0.000397 -0.000361   \n",
            "36  0.028495  0.008049  0.033686  0.027099  0.001744  0.033556  0.005032   \n",
            "37  0.017607  0.014472  0.003794  0.007778  0.026253  0.026272  0.000011   \n",
            "38 -0.002645  0.017745  0.021209 -0.002706  0.004354  0.021500  0.015463   \n",
            "39  0.013660  0.008610  0.018474  0.003934  0.033713  0.001638  0.016512   \n",
            "40  0.030918  0.000622  0.009956  0.019433  0.000461 -0.000907  0.013701   \n",
            "41 -0.014202 -0.010007 -0.013808 -0.019121  0.001727 -0.010824  0.006965   \n",
            "42 -0.018572 -0.000763  0.011354 -0.020072 -0.000430  0.000737 -0.016534   \n",
            "43 -0.016530 -0.011671 -0.013214 -0.002635  0.008195 -0.009454 -0.014988   \n",
            "44 -0.004162  0.004954 -0.016971 -0.016766 -0.005005 -0.002056 -0.009664   \n",
            "45 -0.006126  0.000294 -0.010289 -0.013374 -0.021398 -0.016979 -0.005077   \n",
            "46 -0.003019  0.003975 -0.013475  0.004994 -0.012704 -0.002466 -0.018315   \n",
            "47  0.002745  0.005883 -0.017230  0.000825 -0.021160 -0.008062 -0.022425   \n",
            "48 -0.000349 -0.019200  0.004244  0.012784  0.002108 -0.011295 -0.014164   \n",
            "49  0.019498  0.027400  0.033523  0.031643  0.003375  0.024539  0.018732   \n",
            "50  0.002034  0.025831  0.021195  0.015002  0.011998  0.006490  0.015592   \n",
            "51  0.002681 -0.004163 -0.002470  0.002025  0.000847 -0.004836 -0.020047   \n",
            "52  0.031499  0.006817  0.030368  0.016483  0.009225  0.011744  0.023579   \n",
            "53 -0.002231 -0.016763 -0.012501  0.002132 -0.016919  0.000812  0.009893   \n",
            "54 -0.016136  0.002641  0.006465  0.005333  0.000611 -0.021958  0.005562   \n",
            "55  0.006558  0.001421 -0.006817 -0.021912 -0.021949 -0.012012  0.008985   \n",
            "56 -0.013326  0.005539 -0.001117  0.010829  0.002428 -0.019694 -0.017891   \n",
            "57  0.000068  0.003597  0.006211  0.004677 -0.000560  0.023743  0.012332   \n",
            "58  0.028027  0.022394  0.015621  0.022584  0.027930  0.013303  0.023036   \n",
            "59 -0.016706  0.001549  0.002360 -0.005833 -0.013946 -0.005872 -0.015406   \n",
            "\n",
            "           7         8         9  ...      3128      3129      3130      3131  \\\n",
            "0   0.007324 -0.009616 -0.005067  ... -0.006856  0.010652  0.010737 -0.005627   \n",
            "1  -0.018485 -0.011243 -0.014907  ...  0.003302 -0.023178 -0.002841 -0.002705   \n",
            "2   0.003348 -0.021924  0.002252  ... -0.004555 -0.022496 -0.027291  0.000060   \n",
            "3  -0.006648 -0.017124  0.011142  ... -0.008438  0.007751 -0.003895 -0.014379   \n",
            "4   0.009772 -0.011616 -0.003061  ... -0.013893 -0.023509  0.007052  0.009660   \n",
            "5   0.008894  0.007604  0.022646  ...  0.023266  0.041666  0.029488  0.033174   \n",
            "6  -0.003789  0.009797 -0.006847  ...  0.009045 -0.017039 -0.017816  0.009705   \n",
            "7   0.004008  0.015966  0.020602  ...  0.009192  0.034439  0.014094  0.028504   \n",
            "8  -0.003198 -0.022776  0.003764  ... -0.001452 -0.006950 -0.022771  0.003933   \n",
            "9   0.030930  0.004664  0.003770  ...  0.004832 -0.009257 -0.008188  0.001435   \n",
            "10 -0.013386 -0.013077 -0.018894  ... -0.001648  0.000762 -0.017888 -0.016899   \n",
            "11  0.006491  0.032536  0.000740  ...  0.003196  0.004032  0.010177  0.012247   \n",
            "12  0.025155  0.015860  0.000769  ...  0.006513  0.015600  0.012484  0.016776   \n",
            "13  0.029623  0.026839 -0.001954  ...  0.036832  0.027887  0.015517  0.010580   \n",
            "14 -0.016337  0.004727  0.010331  ... -0.012719 -0.016295  0.002682 -0.014291   \n",
            "15 -0.017671 -0.001590 -0.007524  ... -0.027042 -0.015744 -0.009734 -0.021105   \n",
            "16 -0.015554  0.001033  0.005761  ...  0.009729 -0.022332 -0.021350 -0.016759   \n",
            "17  0.010020 -0.019688 -0.004400  ...  0.007721 -0.004402 -0.016819 -0.015915   \n",
            "18  0.002784 -0.006211  0.013223  ...  0.024819  0.008977  0.035165  0.001899   \n",
            "19  0.014684  0.027412  0.018771  ...  0.027893  0.023726  0.017920  0.016699   \n",
            "20 -0.019416 -0.022032 -0.013255  ...  0.047160  0.036355  0.025181  0.022490   \n",
            "21  0.001027 -0.005196 -0.004424  ... -0.011853 -0.003937  0.003567  0.000304   \n",
            "22  0.004678  0.017792 -0.000148  ...  0.014265  0.008728 -0.017063  0.005836   \n",
            "23  0.005949 -0.008079  0.000984  ... -0.013547 -0.005859  0.002466  0.003944   \n",
            "24 -0.012757 -0.023823 -0.018663  ...  0.011678 -0.019435  0.003086 -0.019769   \n",
            "25 -0.010399  0.010380 -0.019753  ...  0.000844  0.010552 -0.019871 -0.019154   \n",
            "26  0.010319 -0.010890  0.007982  ...  0.000887  0.000956  0.001107 -0.014635   \n",
            "27 -0.004731 -0.009723 -0.003905  ...  0.005012 -0.016505 -0.011187 -0.001365   \n",
            "28 -0.015085 -0.001597 -0.002212  ... -0.009379  0.010932  0.000835  0.011187   \n",
            "29  0.009418  0.025151  0.020776  ... -0.004506 -0.015641 -0.013120 -0.010891   \n",
            "30 -0.016916  0.009760 -0.014054  ...  0.007682 -0.003761 -0.015353 -0.000961   \n",
            "31  0.012016 -0.000764 -0.007299  ... -0.012403 -0.009862 -0.011390 -0.003945   \n",
            "32 -0.024166  0.000700  0.009655  ... -0.019903 -0.006628 -0.020596  0.001379   \n",
            "33  0.010933  0.003280  0.013948  ... -0.003015  0.020623 -0.000906  0.015280   \n",
            "34 -0.017208  0.006274  0.003570  ... -0.013828 -0.008093 -0.019012 -0.001721   \n",
            "35  0.005101 -0.010963 -0.009127  ... -0.002773  0.011193  0.019547  0.002818   \n",
            "36  0.012425  0.009884  0.031366  ...  0.042560  0.039935  0.023204  0.029524   \n",
            "37  0.023419  0.006894  0.026621  ...  0.004115  0.000018  0.020470 -0.002871   \n",
            "38  0.018896  0.024307 -0.000390  ... -0.004216 -0.012399 -0.001571 -0.018753   \n",
            "39  0.010963  0.030405  0.032920  ... -0.013158  0.001995 -0.017575  0.000686   \n",
            "40  0.026295  0.013387 -0.001529  ...  0.043029  0.038009  0.020855  0.017207   \n",
            "41 -0.004680 -0.013764 -0.007863  ...  0.008035  0.010515 -0.006477  0.011474   \n",
            "42 -0.000678  0.009945  0.003108  ...  0.008813  0.006511 -0.001635 -0.017173   \n",
            "43 -0.011972 -0.016392  0.001932  ... -0.010337 -0.012674 -0.018367 -0.022919   \n",
            "44  0.003259 -0.000585  0.001807  ...  0.008413 -0.014572 -0.008983  0.005895   \n",
            "45  0.006896 -0.008086 -0.009275  ... -0.005778  0.007000  0.002406 -0.008275   \n",
            "46  0.010720 -0.002166 -0.014401  ... -0.013205 -0.022235 -0.002017 -0.000171   \n",
            "47 -0.005961 -0.003954  0.010139  ...  0.011701  0.001282 -0.018512 -0.003118   \n",
            "48 -0.013545  0.006565 -0.005001  ... -0.010218  0.010282 -0.019655  0.010408   \n",
            "49  0.018647  0.026510  0.016471  ...  0.039909  0.049060  0.028912  0.045582   \n",
            "50  0.000300  0.010269  0.008490  ...  0.023184 -0.001352  0.019045  0.003499   \n",
            "51  0.004892  0.007555  0.001219  ... -0.016053  0.001421 -0.013407 -0.006220   \n",
            "52  0.030600  0.026217  0.023260  ...  0.017529  0.006934  0.017406  0.020949   \n",
            "53 -0.010024 -0.009774 -0.018796  ... -0.015735 -0.020443  0.008829 -0.016565   \n",
            "54  0.011039 -0.000971 -0.004749  ... -0.011213  0.000786 -0.019802 -0.001971   \n",
            "55 -0.017711 -0.011881  0.008669  ...  0.003574  0.005007 -0.005511 -0.009074   \n",
            "56 -0.018805 -0.010618  0.002681  ... -0.014544 -0.013959  0.007968 -0.004889   \n",
            "57  0.013836  0.000618  0.024202  ...  0.020163  0.034249  0.023702  0.032346   \n",
            "58  0.020271 -0.000923  0.004961  ...  0.030553  0.023524  0.025896  0.012225   \n",
            "59  0.006506 -0.000810  0.000023  ... -0.021418 -0.022299 -0.008617 -0.017246   \n",
            "\n",
            "        3132      3133      3134      3135  layer  neuron_index  \n",
            "0  -0.006300  0.000761 -0.010077 -0.010894    fc1             0  \n",
            "1   0.007173  0.003588 -0.009852 -0.016465    fc1             1  \n",
            "2   0.005837 -0.015450 -0.013901  0.002791    fc1             2  \n",
            "3  -0.012918  0.007607 -0.013114 -0.016117    fc1             3  \n",
            "4   0.003922 -0.010699  0.009847 -0.021692    fc1             4  \n",
            "5   0.028678  0.014802  0.011530  0.006059    fc1             5  \n",
            "6  -0.014932  0.004915  0.002246 -0.007906    fc1             6  \n",
            "7   0.013501  0.022247  0.021532 -0.003630    fc1             7  \n",
            "8  -0.006297 -0.015515 -0.013881  0.004368    fc1             8  \n",
            "9  -0.001923  0.008458  0.021580  0.029945    fc1             9  \n",
            "10 -0.000172 -0.004697 -0.007486  0.003054    fc1            10  \n",
            "11 -0.007124 -0.002595 -0.013320 -0.004145    fc1            11  \n",
            "12 -0.001142  0.025493  0.015860  0.012417    fc1            12  \n",
            "13  0.008905  0.014851  0.010457  0.007798    fc1            13  \n",
            "14 -0.010744 -0.008016 -0.019082 -0.007001    fc1            14  \n",
            "15 -0.008899 -0.000829 -0.016586 -0.028002    fc1            15  \n",
            "16  0.011897 -0.018047 -0.002446 -0.017222    fc1            16  \n",
            "17 -0.020536  0.006682 -0.007786 -0.019112    fc1            17  \n",
            "18  0.014848  0.023633  0.009814 -0.009907    fc1            18  \n",
            "19  0.032436  0.014681  0.007802  0.015888    fc1            19  \n",
            "20  0.010047  0.026064  0.016482  0.030144    fc1            20  \n",
            "21  0.008994  0.010006 -0.018727 -0.003075    fc1            21  \n",
            "22 -0.000216 -0.005094  0.018505 -0.008893    fc1            22  \n",
            "23  0.027010  0.004446 -0.004685  0.011521    fc1            23  \n",
            "24 -0.006039  0.005686 -0.007776  0.007923    fc1            24  \n",
            "25 -0.013028 -0.005355 -0.013795  0.000189    fc1            25  \n",
            "26  0.001260  0.002817 -0.000567 -0.018858    fc1            26  \n",
            "27 -0.012066  0.005803  0.008898 -0.020256    fc1            27  \n",
            "28 -0.021011 -0.016277 -0.021642 -0.017041    fc1            28  \n",
            "29  0.015582 -0.001358  0.009005  0.020085    fc1            29  \n",
            "30  0.009142 -0.011048 -0.019065 -0.022964    fc1            30  \n",
            "31 -0.017578  0.018184 -0.014494  0.006414    fc1            31  \n",
            "32 -0.007214 -0.020134 -0.002980 -0.009061    fc1            32  \n",
            "33 -0.002171 -0.004688  0.024596  0.012860    fc1            33  \n",
            "34  0.006539  0.009605 -0.015453 -0.016520    fc1            34  \n",
            "35 -0.018720 -0.022744 -0.017856  0.009409    fc1            35  \n",
            "36  0.040159  0.015496  0.046070  0.038217    fc1            36  \n",
            "37  0.016869  0.005396  0.012609  0.013487    fc1            37  \n",
            "38 -0.000505  0.023441  0.003526  0.005332    fc1            38  \n",
            "39 -0.019023 -0.006869 -0.015280  0.005979    fc1            39  \n",
            "40  0.027607  0.004566  0.021181  0.012148    fc1            40  \n",
            "41  0.004879 -0.012685 -0.009628 -0.022963    fc1            41  \n",
            "42  0.007932 -0.012811 -0.015902 -0.008890    fc1            42  \n",
            "43 -0.016014  0.007799  0.005429 -0.008904    fc1            43  \n",
            "44  0.004329  0.007101 -0.003699 -0.012319    fc1            44  \n",
            "45  0.005780  0.009909 -0.004622 -0.021927    fc1            45  \n",
            "46 -0.013934 -0.011455 -0.007828 -0.010529    fc1            46  \n",
            "47 -0.016764 -0.023845 -0.003573  0.011422    fc1            47  \n",
            "48 -0.015247 -0.011081 -0.016139 -0.007430    fc1            48  \n",
            "49  0.037967  0.039943  0.047666  0.032830    fc1            49  \n",
            "50  0.002823 -0.002692 -0.008353  0.002865    fc1            50  \n",
            "51 -0.014761  0.009049 -0.008702 -0.023180    fc1            51  \n",
            "52  0.023632  0.022728  0.018886  0.025511    fc1            52  \n",
            "53 -0.018722  0.011494  0.004309 -0.023839    fc1            53  \n",
            "54  0.000339  0.006231  0.007703 -0.013026    fc1            54  \n",
            "55 -0.014757 -0.008799 -0.020483 -0.005706    fc1            55  \n",
            "56 -0.014795  0.005220 -0.021989 -0.015774    fc1            56  \n",
            "57 -0.000363  0.006102 -0.002093 -0.001637    fc1            57  \n",
            "58  0.026156  0.011964  0.002414  0.006485    fc1            58  \n",
            "59 -0.003636  0.008666 -0.018831 -0.015422    fc1            59  \n",
            "\n",
            "[60 rows x 3138 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_ACTIVATIONS DataFrame:\n",
            "  layer  neuron_index  activation_value\n",
            "0   fc2             0        -10.811419\n",
            "1   fc2             1          4.075020\n",
            "2   fc2             2         -3.624119\n",
            "3   fc2             3         -6.470073\n",
            "4   fc2             4        -11.139791\n",
            "5   fc2             5        -11.084295\n",
            "6   fc2             6        -10.644231\n",
            "7   fc2             7         -5.183127\n",
            "8   fc2             8         -7.644820\n",
            "9   fc2             9         19.820395\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_CONNECTIONS DataFrame:\n",
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.107950 -0.100333 -0.125853  0.091329 -0.090712 -0.024337 -0.035301   \n",
            "1  0.131501 -0.110936 -0.085158  0.066024 -0.037845 -0.071343 -0.052721   \n",
            "2  0.050709 -0.033907 -0.077746 -0.074659 -0.076966 -0.074498 -0.088701   \n",
            "3  0.049182 -0.027318 -0.033456 -0.035291 -0.043763 -0.003068  0.013614   \n",
            "4  0.042645  0.088435  0.038246  0.004014 -0.031166 -0.035241 -0.033445   \n",
            "5  0.086447 -0.126483 -0.040570  0.021679  0.066402 -0.114365  0.069705   \n",
            "6 -0.080449  0.085518 -0.090617 -0.053715 -0.023148 -0.005910  0.053310   \n",
            "7  0.092976 -0.108292 -0.003311 -0.020869  0.013422 -0.048318  0.010532   \n",
            "8 -0.123973 -0.038942  0.098014 -0.079174  0.093177  0.066027  0.115322   \n",
            "9  0.017517 -0.083782  0.037875 -0.062554 -0.064439  0.184223  0.006078   \n",
            "\n",
            "          7         8         9  ...        52        53        54        55  \\\n",
            "0  0.090453 -0.134711  0.053470  ... -0.070573  0.083195  0.095116  0.083355   \n",
            "1 -0.112283  0.045542  0.064597  ...  0.047196  0.127483  0.055454  0.038554   \n",
            "2  0.057485 -0.012508 -0.048672  ...  0.098187  0.072752 -0.079741 -0.097680   \n",
            "3 -0.134513 -0.108175  0.041074  ... -0.005931 -0.089947 -0.010503  0.077362   \n",
            "4 -0.040576  0.073950 -0.058569  ... -0.079893 -0.038126 -0.095315  0.112316   \n",
            "5 -0.126576 -0.152112 -0.069325  ... -0.028109  0.103588 -0.003243  0.043243   \n",
            "6  0.102617  0.096054  0.003210  ... -0.036933  0.010865  0.066728  0.112904   \n",
            "7  0.093271  0.076127 -0.004166  ... -0.105497  0.043385  0.102613  0.121523   \n",
            "8  0.016770 -0.150593  0.057095  ... -0.014134 -0.041150  0.043610 -0.033362   \n",
            "9  0.090238 -0.031772 -0.016623  ... -0.092037  0.067543 -0.015439  0.107121   \n",
            "\n",
            "         56        57        58        59  layer  neuron_index  \n",
            "0 -0.068317  0.057229 -0.010008 -0.046776    fc2             0  \n",
            "1  0.154287 -0.029886 -0.088726  0.017459    fc2             1  \n",
            "2 -0.002260 -0.142917  0.077585  0.014963    fc2             2  \n",
            "3 -0.088103 -0.110321  0.010353  0.046034    fc2             3  \n",
            "4  0.099081  0.096792  0.056523 -0.063165    fc2             4  \n",
            "5 -0.133819 -0.110977 -0.123005 -0.015103    fc2             5  \n",
            "6  0.097748 -0.037395  0.050451 -0.001918    fc2             6  \n",
            "7 -0.045865 -0.148744  0.003459 -0.012868    fc2             7  \n",
            "8 -0.111366 -0.025910  0.000013  0.098205    fc2             8  \n",
            "9 -0.079098  0.062496 -0.018937  0.063364    fc2             9  \n",
            "\n",
            "[10 rows x 62 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Image 3 ---\n",
            "True Label: 1\n",
            "Predicted Label: 1\n",
            "\n",
            "CONV1_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv1             0          0.200850\n",
            "1   conv1             1          0.227033\n",
            "2   conv1             2          0.095992\n",
            "3   conv1             3          0.029019\n",
            "4   conv1             4          0.044897\n",
            "5   conv1             5          0.229112\n",
            "6   conv1             6          0.036860\n",
            "7   conv1             7          0.272482\n",
            "8   conv1             8          0.063167\n",
            "9   conv1             9          0.055105\n",
            "10  conv1            10          0.050097\n",
            "11  conv1            11          0.167954\n",
            "12  conv1            12          0.003316\n",
            "13  conv1            13          0.047762\n",
            "14  conv1            14         -0.008279\n",
            "15  conv1            15          0.004525\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.400412  0.399371  0.071231 -0.067315  0.237489  0.328836 -0.150906   \n",
            "1   0.062670  0.420839  0.423627 -0.087857  0.270959 -0.074010  0.234360   \n",
            "2   0.077322  0.239514  0.362599  0.313162 -0.159267 -0.146957 -0.197186   \n",
            "3  -0.198520  0.416571  0.189112 -0.133381  0.282581  0.187163 -0.016161   \n",
            "4  -0.100635 -0.007484  0.208964  0.262549  0.069296  0.175801  0.311726   \n",
            "5  -0.273976 -0.003054  0.224042  0.069278  0.076746  0.157825  0.075656   \n",
            "6  -0.428757 -0.251183 -0.254338  0.081380  0.144253  0.244236  0.337603   \n",
            "7   0.378913  0.399986  0.071193  0.172418 -0.026089  0.130204 -0.220100   \n",
            "8   0.225943  0.192161  0.387705  0.135396 -0.048392 -0.038235 -0.115391   \n",
            "9   0.307195 -0.180329 -0.171382  0.319853  0.081811  0.181218  0.174932   \n",
            "10  0.130797  0.370466  0.380331  0.015307 -0.078076  0.016426 -0.083861   \n",
            "11  0.392175  0.247163  0.281231  0.259225 -0.270856 -0.165327  0.253949   \n",
            "12 -0.210740 -0.048877 -0.131766  0.335698  0.006075 -0.116493 -0.017947   \n",
            "13  0.119667  0.026561  0.093274  0.343750  0.204018 -0.236069  0.372014   \n",
            "14  0.198226 -0.245182  0.121569 -0.049056 -0.133100 -0.077453  0.442164   \n",
            "15  0.209107  0.173020 -0.301732  0.125146 -0.021763 -0.264287  0.322271   \n",
            "\n",
            "           7         8  layer  neuron_index  \n",
            "0   0.392861 -0.045586  conv1             0  \n",
            "1  -0.093755  0.006661  conv1             1  \n",
            "2   0.068652  0.012842  conv1             2  \n",
            "3  -0.195803  0.062113  conv1             3  \n",
            "4  -0.130634  0.326892  conv1             4  \n",
            "5   0.073296 -0.115009  conv1             5  \n",
            "6  -0.101311  0.121107  conv1             6  \n",
            "7   0.377975 -0.272754  conv1             7  \n",
            "8   0.228945  0.188063  conv1             8  \n",
            "9   0.345546  0.172957  conv1             9  \n",
            "10  0.056517  0.259929  conv1            10  \n",
            "11 -0.198929  0.022492  conv1            11  \n",
            "12  0.285077  0.004282  conv1            12  \n",
            "13  0.204587 -0.130169  conv1            13  \n",
            "14 -0.012433 -0.308704  conv1            14  \n",
            "15 -0.069897  0.135285  conv1            15  \n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv2             0          0.070487\n",
            "1   conv2             1          0.445887\n",
            "2   conv2             2          0.160948\n",
            "3   conv2             3          0.557783\n",
            "4   conv2             4         -0.166093\n",
            "5   conv2             5         -0.234721\n",
            "6   conv2             6          0.333487\n",
            "7   conv2             7          0.272461\n",
            "8   conv2             8          0.156724\n",
            "9   conv2             9          0.061776\n",
            "10  conv2            10          0.475569\n",
            "11  conv2            11          0.157494\n",
            "12  conv2            12          0.291480\n",
            "13  conv2            13          0.029561\n",
            "14  conv2            14          0.280187\n",
            "15  conv2            15          0.192662\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.053389  0.023715 -0.082655 -0.048249 -0.076035 -0.150770  0.021789   \n",
            "1   0.047589 -0.093007  0.005160  0.069936  0.136776  0.046387  0.010860   \n",
            "2   0.058461  0.100847 -0.038368  0.022629  0.103998 -0.064163 -0.030390   \n",
            "3  -0.035365  0.023469  0.107092  0.101842  0.143467  0.217515 -0.012316   \n",
            "4   0.062249 -0.131436 -0.046978  0.021597 -0.080085 -0.078712 -0.103583   \n",
            "5  -0.019309 -0.080785  0.035847  0.020273  0.009665  0.035786 -0.006190   \n",
            "6   0.049547  0.038741 -0.036449 -0.002058  0.041931  0.053819 -0.005733   \n",
            "7  -0.152914 -0.041600  0.115807 -0.041148 -0.039541  0.071635 -0.011336   \n",
            "8   0.125563  0.033356 -0.048423  0.110720 -0.018201 -0.147717 -0.042072   \n",
            "9  -0.068213 -0.115001 -0.004743 -0.028494 -0.027653  0.079632  0.049930   \n",
            "10 -0.032338 -0.013617  0.151200 -0.075678  0.075957  0.207855  0.038870   \n",
            "11  0.051104  0.012186  0.086417  0.028352  0.156211 -0.002386  0.086014   \n",
            "12  0.082138  0.005539 -0.044524  0.132945  0.106715  0.022446  0.116672   \n",
            "13 -0.043048 -0.140184  0.012286 -0.060239 -0.119047  0.023106 -0.004832   \n",
            "14 -0.021613  0.105474  0.025686  0.030302  0.136904 -0.020385 -0.046524   \n",
            "15  0.117869  0.126556  0.051337  0.002796 -0.029835 -0.061113  0.004300   \n",
            "\n",
            "           7         8         9  ...       136       137       138       139  \\\n",
            "0   0.087136  0.136641  0.125749  ...  0.105212  0.008460  0.086248 -0.005925   \n",
            "1   0.068729 -0.004232 -0.004889  ...  0.075859  0.051621  0.013238  0.045813   \n",
            "2  -0.050265  0.004671  0.000319  ...  0.094657  0.007707  0.022765  0.004527   \n",
            "3   0.084479  0.127317 -0.104037  ...  0.000802  0.089539 -0.032844  0.050453   \n",
            "4   0.048199  0.034387 -0.074277  ...  0.045809 -0.082850  0.027167 -0.046521   \n",
            "5  -0.040837 -0.020941  0.074329  ... -0.027615 -0.036336 -0.055786  0.008235   \n",
            "6  -0.068649 -0.066126 -0.024237  ...  0.095926  0.090875 -0.025141  0.018905   \n",
            "7   0.034141  0.078109 -0.036853  ...  0.086062 -0.007312  0.005437  0.073091   \n",
            "8   0.026514 -0.127686 -0.001961  ...  0.017386 -0.016188  0.018640  0.010524   \n",
            "9  -0.042845  0.092674  0.078472  ... -0.005325  0.106815  0.014020 -0.074019   \n",
            "10 -0.053325  0.046839  0.006819  ...  0.041650  0.170038 -0.044293  0.004112   \n",
            "11  0.042763 -0.056389 -0.043120  ...  0.042914 -0.032490  0.009025  0.004950   \n",
            "12  0.132660 -0.116526 -0.137380  ... -0.008512 -0.055690  0.034679  0.085180   \n",
            "13 -0.028790  0.080668 -0.064381  ... -0.020423  0.075944 -0.070604 -0.053503   \n",
            "14  0.006731 -0.044295  0.048296  ... -0.026215  0.104971  0.072772  0.041096   \n",
            "15  0.013864 -0.005772  0.182691  ... -0.027957 -0.073938 -0.020258 -0.038572   \n",
            "\n",
            "         140       141       142       143  layer  neuron_index  \n",
            "0  -0.033853  0.113257 -0.040305  0.088088  conv2             0  \n",
            "1   0.091032  0.005281 -0.026384  0.078543  conv2             1  \n",
            "2   0.002996  0.085109  0.008654  0.067731  conv2             2  \n",
            "3  -0.007134  0.030011 -0.008236 -0.030027  conv2             3  \n",
            "4   0.067527 -0.041539  0.057837  0.032542  conv2             4  \n",
            "5  -0.075360 -0.063744  0.026530  0.010399  conv2             5  \n",
            "6   0.031970 -0.055053  0.091992 -0.061894  conv2             6  \n",
            "7   0.030808 -0.011768  0.093748  0.089395  conv2             7  \n",
            "8  -0.089284  0.094356  0.033373  0.012280  conv2             8  \n",
            "9   0.011533 -0.078723  0.016492 -0.013510  conv2             9  \n",
            "10  0.017894 -0.050105  0.031401  0.045740  conv2            10  \n",
            "11  0.000354  0.074826  0.052800  0.083592  conv2            11  \n",
            "12  0.014633  0.005773  0.066127 -0.082251  conv2            12  \n",
            "13  0.020353 -0.001231  0.079536  0.032849  conv2            13  \n",
            "14  0.005841  0.030748 -0.013547 -0.054549  conv2            14  \n",
            "15 -0.075074  0.053019 -0.002690 -0.082208  conv2            15  \n",
            "\n",
            "[16 rows x 146 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_ACTIVATIONS DataFrame:\n",
            "   layer  neuron_index  activation_value\n",
            "0    fc1             0         -2.165279\n",
            "1    fc1             1         -2.088221\n",
            "2    fc1             2         -5.934769\n",
            "3    fc1             3         -2.360328\n",
            "4    fc1             4         -2.850434\n",
            "5    fc1             5          1.000971\n",
            "6    fc1             6         -2.696477\n",
            "7    fc1             7         -0.438763\n",
            "8    fc1             8         -0.914782\n",
            "9    fc1             9          5.792916\n",
            "10   fc1            10         -2.649843\n",
            "11   fc1            11         -3.127541\n",
            "12   fc1            12          9.574037\n",
            "13   fc1            13         18.319092\n",
            "14   fc1            14         -2.423701\n",
            "15   fc1            15         -6.739137\n",
            "16   fc1            16         -3.404199\n",
            "17   fc1            17         -2.196233\n",
            "18   fc1            18          2.634432\n",
            "19   fc1            19          2.410805\n",
            "20   fc1            20          8.990967\n",
            "21   fc1            21         -5.509514\n",
            "22   fc1            22         -4.717367\n",
            "23   fc1            23          8.951365\n",
            "24   fc1            24         -3.153265\n",
            "25   fc1            25         -1.931523\n",
            "26   fc1            26         -2.855277\n",
            "27   fc1            27         -1.465812\n",
            "28   fc1            28         -2.786105\n",
            "29   fc1            29          9.764580\n",
            "30   fc1            30         -2.638684\n",
            "31   fc1            31         -3.977245\n",
            "32   fc1            32         -3.832541\n",
            "33   fc1            33          8.773963\n",
            "34   fc1            34         -2.736166\n",
            "35   fc1            35         12.052728\n",
            "36   fc1            36         22.065014\n",
            "37   fc1            37         11.900796\n",
            "38   fc1            38          5.701588\n",
            "39   fc1            39         -6.931377\n",
            "40   fc1            40          0.176237\n",
            "41   fc1            41         -2.613035\n",
            "42   fc1            42         -2.514101\n",
            "43   fc1            43         -2.748724\n",
            "44   fc1            44         -2.629490\n",
            "45   fc1            45         -2.775944\n",
            "46   fc1            46         -2.213656\n",
            "47   fc1            47         -2.114663\n",
            "48   fc1            48         -1.921541\n",
            "49   fc1            49         19.841887\n",
            "50   fc1            50          6.344415\n",
            "51   fc1            51         -1.807569\n",
            "52   fc1            52         11.699365\n",
            "53   fc1            53         -2.165532\n",
            "54   fc1            54         -1.454028\n",
            "55   fc1            55         -3.881118\n",
            "56   fc1            56         -2.012558\n",
            "57   fc1            57          2.832031\n",
            "58   fc1            58         -4.586135\n",
            "59   fc1            59         -2.680660\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.012868  0.012967 -0.004651  0.004214  0.004450 -0.004286 -0.006892   \n",
            "1  -0.006668  0.001739  0.008233 -0.015734 -0.018150  0.006245 -0.009190   \n",
            "2  -0.004715 -0.008510 -0.027038 -0.012099 -0.017072 -0.001276 -0.004790   \n",
            "3  -0.022022  0.006158 -0.000070  0.005327 -0.010999 -0.013339 -0.000802   \n",
            "4  -0.021747  0.005062 -0.007141 -0.002509  0.002286 -0.004356 -0.013751   \n",
            "5   0.000488  0.012703  0.014533  0.016341 -0.004459  0.019696  0.002125   \n",
            "6   0.009300 -0.022666 -0.019488  0.000888 -0.000055 -0.011650 -0.015693   \n",
            "7   0.031482  0.014408  0.003062  0.002343  0.027174  0.003222  0.007536   \n",
            "8  -0.021455 -0.018184  0.010642 -0.009969 -0.002964 -0.022868  0.002633   \n",
            "9   0.008664  0.005595  0.018707  0.036951  0.012074  0.024569  0.035212   \n",
            "10 -0.015157 -0.015687 -0.020529  0.011342  0.001704 -0.014001 -0.023534   \n",
            "11  0.033904  0.009318  0.023171  0.003949  0.003715  0.000735  0.023219   \n",
            "12  0.024346  0.032305  0.026967  0.025876  0.010165  0.022697  0.011940   \n",
            "13  0.008851  0.010857  0.002167  0.024771  0.005659  0.021011  0.000578   \n",
            "14 -0.009209 -0.003457 -0.008076 -0.003801  0.010076 -0.013077 -0.018169   \n",
            "15 -0.022892 -0.007363 -0.001998 -0.025108 -0.005332 -0.008478 -0.006340   \n",
            "16 -0.011208 -0.008056 -0.020657 -0.002330  0.002901 -0.009348 -0.019556   \n",
            "17 -0.004303 -0.011543 -0.022747 -0.002148 -0.000974  0.003379  0.001518   \n",
            "18  0.014590  0.013593  0.026992  0.014942  0.002147  0.024546  0.023784   \n",
            "19  0.013653  0.015041  0.026944  0.017540  0.012362  0.005006  0.020771   \n",
            "20  0.000926 -0.002540  0.000976 -0.007426  0.008134  0.009121  0.006932   \n",
            "21 -0.008481 -0.009680 -0.015762 -0.008213  0.005569 -0.011419 -0.018695   \n",
            "22  0.002687 -0.004754  0.025822  0.022829  0.026687  0.024950  0.015143   \n",
            "23  0.012675 -0.008788  0.020707 -0.002437 -0.000562 -0.001121  0.009253   \n",
            "24 -0.021873 -0.006459  0.000207 -0.022165  0.008027 -0.006639  0.011209   \n",
            "25 -0.009614 -0.022043 -0.007360  0.004037 -0.017339 -0.001730 -0.005209   \n",
            "26 -0.003378 -0.006826 -0.019353 -0.008899  0.007771  0.007703 -0.008105   \n",
            "27 -0.009939 -0.008868  0.000785  0.008521 -0.017249  0.009543 -0.006277   \n",
            "28 -0.012323  0.002773 -0.002832  0.008380 -0.001049 -0.023035 -0.010383   \n",
            "29  0.014561  0.016938  0.017683 -0.001824  0.010139  0.014729  0.012067   \n",
            "30 -0.019001 -0.009498 -0.020953 -0.004602  0.011816  0.005326 -0.018519   \n",
            "31  0.000243  0.000025  0.021961 -0.006239  0.018972 -0.004205  0.002377   \n",
            "32 -0.003725 -0.003355 -0.020392  0.005503 -0.013929 -0.013021  0.001984   \n",
            "33  0.016569  0.002955  0.006740  0.025918  0.010518  0.006316  0.009447   \n",
            "34 -0.021455 -0.010343  0.011155  0.004942 -0.004358 -0.012782 -0.010427   \n",
            "35 -0.004436 -0.022906 -0.004501  0.005599  0.007464 -0.000397 -0.000361   \n",
            "36  0.028495  0.008049  0.033686  0.027099  0.001744  0.033556  0.005032   \n",
            "37  0.017607  0.014472  0.003794  0.007778  0.026253  0.026272  0.000011   \n",
            "38 -0.002645  0.017745  0.021209 -0.002706  0.004354  0.021500  0.015463   \n",
            "39  0.013660  0.008610  0.018474  0.003934  0.033713  0.001638  0.016512   \n",
            "40  0.030918  0.000622  0.009956  0.019433  0.000461 -0.000907  0.013701   \n",
            "41 -0.014202 -0.010007 -0.013808 -0.019121  0.001727 -0.010824  0.006965   \n",
            "42 -0.018572 -0.000763  0.011354 -0.020072 -0.000430  0.000737 -0.016534   \n",
            "43 -0.016530 -0.011671 -0.013214 -0.002635  0.008195 -0.009454 -0.014988   \n",
            "44 -0.004162  0.004954 -0.016971 -0.016766 -0.005005 -0.002056 -0.009664   \n",
            "45 -0.006126  0.000294 -0.010289 -0.013374 -0.021398 -0.016979 -0.005077   \n",
            "46 -0.003019  0.003975 -0.013475  0.004994 -0.012704 -0.002466 -0.018315   \n",
            "47  0.002745  0.005883 -0.017230  0.000825 -0.021160 -0.008062 -0.022425   \n",
            "48 -0.000349 -0.019200  0.004244  0.012784  0.002108 -0.011295 -0.014164   \n",
            "49  0.019498  0.027400  0.033523  0.031643  0.003375  0.024539  0.018732   \n",
            "50  0.002034  0.025831  0.021195  0.015002  0.011998  0.006490  0.015592   \n",
            "51  0.002681 -0.004163 -0.002470  0.002025  0.000847 -0.004836 -0.020047   \n",
            "52  0.031499  0.006817  0.030368  0.016483  0.009225  0.011744  0.023579   \n",
            "53 -0.002231 -0.016763 -0.012501  0.002132 -0.016919  0.000812  0.009893   \n",
            "54 -0.016136  0.002641  0.006465  0.005333  0.000611 -0.021958  0.005562   \n",
            "55  0.006558  0.001421 -0.006817 -0.021912 -0.021949 -0.012012  0.008985   \n",
            "56 -0.013326  0.005539 -0.001117  0.010829  0.002428 -0.019694 -0.017891   \n",
            "57  0.000068  0.003597  0.006211  0.004677 -0.000560  0.023743  0.012332   \n",
            "58  0.028027  0.022394  0.015621  0.022584  0.027930  0.013303  0.023036   \n",
            "59 -0.016706  0.001549  0.002360 -0.005833 -0.013946 -0.005872 -0.015406   \n",
            "\n",
            "           7         8         9  ...      3128      3129      3130      3131  \\\n",
            "0   0.007324 -0.009616 -0.005067  ... -0.006856  0.010652  0.010737 -0.005627   \n",
            "1  -0.018485 -0.011243 -0.014907  ...  0.003302 -0.023178 -0.002841 -0.002705   \n",
            "2   0.003348 -0.021924  0.002252  ... -0.004555 -0.022496 -0.027291  0.000060   \n",
            "3  -0.006648 -0.017124  0.011142  ... -0.008438  0.007751 -0.003895 -0.014379   \n",
            "4   0.009772 -0.011616 -0.003061  ... -0.013893 -0.023509  0.007052  0.009660   \n",
            "5   0.008894  0.007604  0.022646  ...  0.023266  0.041666  0.029488  0.033174   \n",
            "6  -0.003789  0.009797 -0.006847  ...  0.009045 -0.017039 -0.017816  0.009705   \n",
            "7   0.004008  0.015966  0.020602  ...  0.009192  0.034439  0.014094  0.028504   \n",
            "8  -0.003198 -0.022776  0.003764  ... -0.001452 -0.006950 -0.022771  0.003933   \n",
            "9   0.030930  0.004664  0.003770  ...  0.004832 -0.009257 -0.008188  0.001435   \n",
            "10 -0.013386 -0.013077 -0.018894  ... -0.001648  0.000762 -0.017888 -0.016899   \n",
            "11  0.006491  0.032536  0.000740  ...  0.003196  0.004032  0.010177  0.012247   \n",
            "12  0.025155  0.015860  0.000769  ...  0.006513  0.015600  0.012484  0.016776   \n",
            "13  0.029623  0.026839 -0.001954  ...  0.036832  0.027887  0.015517  0.010580   \n",
            "14 -0.016337  0.004727  0.010331  ... -0.012719 -0.016295  0.002682 -0.014291   \n",
            "15 -0.017671 -0.001590 -0.007524  ... -0.027042 -0.015744 -0.009734 -0.021105   \n",
            "16 -0.015554  0.001033  0.005761  ...  0.009729 -0.022332 -0.021350 -0.016759   \n",
            "17  0.010020 -0.019688 -0.004400  ...  0.007721 -0.004402 -0.016819 -0.015915   \n",
            "18  0.002784 -0.006211  0.013223  ...  0.024819  0.008977  0.035165  0.001899   \n",
            "19  0.014684  0.027412  0.018771  ...  0.027893  0.023726  0.017920  0.016699   \n",
            "20 -0.019416 -0.022032 -0.013255  ...  0.047160  0.036355  0.025181  0.022490   \n",
            "21  0.001027 -0.005196 -0.004424  ... -0.011853 -0.003937  0.003567  0.000304   \n",
            "22  0.004678  0.017792 -0.000148  ...  0.014265  0.008728 -0.017063  0.005836   \n",
            "23  0.005949 -0.008079  0.000984  ... -0.013547 -0.005859  0.002466  0.003944   \n",
            "24 -0.012757 -0.023823 -0.018663  ...  0.011678 -0.019435  0.003086 -0.019769   \n",
            "25 -0.010399  0.010380 -0.019753  ...  0.000844  0.010552 -0.019871 -0.019154   \n",
            "26  0.010319 -0.010890  0.007982  ...  0.000887  0.000956  0.001107 -0.014635   \n",
            "27 -0.004731 -0.009723 -0.003905  ...  0.005012 -0.016505 -0.011187 -0.001365   \n",
            "28 -0.015085 -0.001597 -0.002212  ... -0.009379  0.010932  0.000835  0.011187   \n",
            "29  0.009418  0.025151  0.020776  ... -0.004506 -0.015641 -0.013120 -0.010891   \n",
            "30 -0.016916  0.009760 -0.014054  ...  0.007682 -0.003761 -0.015353 -0.000961   \n",
            "31  0.012016 -0.000764 -0.007299  ... -0.012403 -0.009862 -0.011390 -0.003945   \n",
            "32 -0.024166  0.000700  0.009655  ... -0.019903 -0.006628 -0.020596  0.001379   \n",
            "33  0.010933  0.003280  0.013948  ... -0.003015  0.020623 -0.000906  0.015280   \n",
            "34 -0.017208  0.006274  0.003570  ... -0.013828 -0.008093 -0.019012 -0.001721   \n",
            "35  0.005101 -0.010963 -0.009127  ... -0.002773  0.011193  0.019547  0.002818   \n",
            "36  0.012425  0.009884  0.031366  ...  0.042560  0.039935  0.023204  0.029524   \n",
            "37  0.023419  0.006894  0.026621  ...  0.004115  0.000018  0.020470 -0.002871   \n",
            "38  0.018896  0.024307 -0.000390  ... -0.004216 -0.012399 -0.001571 -0.018753   \n",
            "39  0.010963  0.030405  0.032920  ... -0.013158  0.001995 -0.017575  0.000686   \n",
            "40  0.026295  0.013387 -0.001529  ...  0.043029  0.038009  0.020855  0.017207   \n",
            "41 -0.004680 -0.013764 -0.007863  ...  0.008035  0.010515 -0.006477  0.011474   \n",
            "42 -0.000678  0.009945  0.003108  ...  0.008813  0.006511 -0.001635 -0.017173   \n",
            "43 -0.011972 -0.016392  0.001932  ... -0.010337 -0.012674 -0.018367 -0.022919   \n",
            "44  0.003259 -0.000585  0.001807  ...  0.008413 -0.014572 -0.008983  0.005895   \n",
            "45  0.006896 -0.008086 -0.009275  ... -0.005778  0.007000  0.002406 -0.008275   \n",
            "46  0.010720 -0.002166 -0.014401  ... -0.013205 -0.022235 -0.002017 -0.000171   \n",
            "47 -0.005961 -0.003954  0.010139  ...  0.011701  0.001282 -0.018512 -0.003118   \n",
            "48 -0.013545  0.006565 -0.005001  ... -0.010218  0.010282 -0.019655  0.010408   \n",
            "49  0.018647  0.026510  0.016471  ...  0.039909  0.049060  0.028912  0.045582   \n",
            "50  0.000300  0.010269  0.008490  ...  0.023184 -0.001352  0.019045  0.003499   \n",
            "51  0.004892  0.007555  0.001219  ... -0.016053  0.001421 -0.013407 -0.006220   \n",
            "52  0.030600  0.026217  0.023260  ...  0.017529  0.006934  0.017406  0.020949   \n",
            "53 -0.010024 -0.009774 -0.018796  ... -0.015735 -0.020443  0.008829 -0.016565   \n",
            "54  0.011039 -0.000971 -0.004749  ... -0.011213  0.000786 -0.019802 -0.001971   \n",
            "55 -0.017711 -0.011881  0.008669  ...  0.003574  0.005007 -0.005511 -0.009074   \n",
            "56 -0.018805 -0.010618  0.002681  ... -0.014544 -0.013959  0.007968 -0.004889   \n",
            "57  0.013836  0.000618  0.024202  ...  0.020163  0.034249  0.023702  0.032346   \n",
            "58  0.020271 -0.000923  0.004961  ...  0.030553  0.023524  0.025896  0.012225   \n",
            "59  0.006506 -0.000810  0.000023  ... -0.021418 -0.022299 -0.008617 -0.017246   \n",
            "\n",
            "        3132      3133      3134      3135  layer  neuron_index  \n",
            "0  -0.006300  0.000761 -0.010077 -0.010894    fc1             0  \n",
            "1   0.007173  0.003588 -0.009852 -0.016465    fc1             1  \n",
            "2   0.005837 -0.015450 -0.013901  0.002791    fc1             2  \n",
            "3  -0.012918  0.007607 -0.013114 -0.016117    fc1             3  \n",
            "4   0.003922 -0.010699  0.009847 -0.021692    fc1             4  \n",
            "5   0.028678  0.014802  0.011530  0.006059    fc1             5  \n",
            "6  -0.014932  0.004915  0.002246 -0.007906    fc1             6  \n",
            "7   0.013501  0.022247  0.021532 -0.003630    fc1             7  \n",
            "8  -0.006297 -0.015515 -0.013881  0.004368    fc1             8  \n",
            "9  -0.001923  0.008458  0.021580  0.029945    fc1             9  \n",
            "10 -0.000172 -0.004697 -0.007486  0.003054    fc1            10  \n",
            "11 -0.007124 -0.002595 -0.013320 -0.004145    fc1            11  \n",
            "12 -0.001142  0.025493  0.015860  0.012417    fc1            12  \n",
            "13  0.008905  0.014851  0.010457  0.007798    fc1            13  \n",
            "14 -0.010744 -0.008016 -0.019082 -0.007001    fc1            14  \n",
            "15 -0.008899 -0.000829 -0.016586 -0.028002    fc1            15  \n",
            "16  0.011897 -0.018047 -0.002446 -0.017222    fc1            16  \n",
            "17 -0.020536  0.006682 -0.007786 -0.019112    fc1            17  \n",
            "18  0.014848  0.023633  0.009814 -0.009907    fc1            18  \n",
            "19  0.032436  0.014681  0.007802  0.015888    fc1            19  \n",
            "20  0.010047  0.026064  0.016482  0.030144    fc1            20  \n",
            "21  0.008994  0.010006 -0.018727 -0.003075    fc1            21  \n",
            "22 -0.000216 -0.005094  0.018505 -0.008893    fc1            22  \n",
            "23  0.027010  0.004446 -0.004685  0.011521    fc1            23  \n",
            "24 -0.006039  0.005686 -0.007776  0.007923    fc1            24  \n",
            "25 -0.013028 -0.005355 -0.013795  0.000189    fc1            25  \n",
            "26  0.001260  0.002817 -0.000567 -0.018858    fc1            26  \n",
            "27 -0.012066  0.005803  0.008898 -0.020256    fc1            27  \n",
            "28 -0.021011 -0.016277 -0.021642 -0.017041    fc1            28  \n",
            "29  0.015582 -0.001358  0.009005  0.020085    fc1            29  \n",
            "30  0.009142 -0.011048 -0.019065 -0.022964    fc1            30  \n",
            "31 -0.017578  0.018184 -0.014494  0.006414    fc1            31  \n",
            "32 -0.007214 -0.020134 -0.002980 -0.009061    fc1            32  \n",
            "33 -0.002171 -0.004688  0.024596  0.012860    fc1            33  \n",
            "34  0.006539  0.009605 -0.015453 -0.016520    fc1            34  \n",
            "35 -0.018720 -0.022744 -0.017856  0.009409    fc1            35  \n",
            "36  0.040159  0.015496  0.046070  0.038217    fc1            36  \n",
            "37  0.016869  0.005396  0.012609  0.013487    fc1            37  \n",
            "38 -0.000505  0.023441  0.003526  0.005332    fc1            38  \n",
            "39 -0.019023 -0.006869 -0.015280  0.005979    fc1            39  \n",
            "40  0.027607  0.004566  0.021181  0.012148    fc1            40  \n",
            "41  0.004879 -0.012685 -0.009628 -0.022963    fc1            41  \n",
            "42  0.007932 -0.012811 -0.015902 -0.008890    fc1            42  \n",
            "43 -0.016014  0.007799  0.005429 -0.008904    fc1            43  \n",
            "44  0.004329  0.007101 -0.003699 -0.012319    fc1            44  \n",
            "45  0.005780  0.009909 -0.004622 -0.021927    fc1            45  \n",
            "46 -0.013934 -0.011455 -0.007828 -0.010529    fc1            46  \n",
            "47 -0.016764 -0.023845 -0.003573  0.011422    fc1            47  \n",
            "48 -0.015247 -0.011081 -0.016139 -0.007430    fc1            48  \n",
            "49  0.037967  0.039943  0.047666  0.032830    fc1            49  \n",
            "50  0.002823 -0.002692 -0.008353  0.002865    fc1            50  \n",
            "51 -0.014761  0.009049 -0.008702 -0.023180    fc1            51  \n",
            "52  0.023632  0.022728  0.018886  0.025511    fc1            52  \n",
            "53 -0.018722  0.011494  0.004309 -0.023839    fc1            53  \n",
            "54  0.000339  0.006231  0.007703 -0.013026    fc1            54  \n",
            "55 -0.014757 -0.008799 -0.020483 -0.005706    fc1            55  \n",
            "56 -0.014795  0.005220 -0.021989 -0.015774    fc1            56  \n",
            "57 -0.000363  0.006102 -0.002093 -0.001637    fc1            57  \n",
            "58  0.026156  0.011964  0.002414  0.006485    fc1            58  \n",
            "59 -0.003636  0.008666 -0.018831 -0.015422    fc1            59  \n",
            "\n",
            "[60 rows x 3138 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_ACTIVATIONS DataFrame:\n",
            "  layer  neuron_index  activation_value\n",
            "0   fc2             0         -6.284803\n",
            "1   fc2             1         11.974081\n",
            "2   fc2             2         -2.527557\n",
            "3   fc2             3         -5.618510\n",
            "4   fc2             4         -5.871741\n",
            "5   fc2             5         -6.956254\n",
            "6   fc2             6         -7.518332\n",
            "7   fc2             7         -3.801505\n",
            "8   fc2             8         -8.468379\n",
            "9   fc2             9          6.178788\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_CONNECTIONS DataFrame:\n",
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.107950 -0.100333 -0.125853  0.091329 -0.090712 -0.024337 -0.035301   \n",
            "1  0.131501 -0.110936 -0.085158  0.066024 -0.037845 -0.071343 -0.052721   \n",
            "2  0.050709 -0.033907 -0.077746 -0.074659 -0.076966 -0.074498 -0.088701   \n",
            "3  0.049182 -0.027318 -0.033456 -0.035291 -0.043763 -0.003068  0.013614   \n",
            "4  0.042645  0.088435  0.038246  0.004014 -0.031166 -0.035241 -0.033445   \n",
            "5  0.086447 -0.126483 -0.040570  0.021679  0.066402 -0.114365  0.069705   \n",
            "6 -0.080449  0.085518 -0.090617 -0.053715 -0.023148 -0.005910  0.053310   \n",
            "7  0.092976 -0.108292 -0.003311 -0.020869  0.013422 -0.048318  0.010532   \n",
            "8 -0.123973 -0.038942  0.098014 -0.079174  0.093177  0.066027  0.115322   \n",
            "9  0.017517 -0.083782  0.037875 -0.062554 -0.064439  0.184223  0.006078   \n",
            "\n",
            "          7         8         9  ...        52        53        54        55  \\\n",
            "0  0.090453 -0.134711  0.053470  ... -0.070573  0.083195  0.095116  0.083355   \n",
            "1 -0.112283  0.045542  0.064597  ...  0.047196  0.127483  0.055454  0.038554   \n",
            "2  0.057485 -0.012508 -0.048672  ...  0.098187  0.072752 -0.079741 -0.097680   \n",
            "3 -0.134513 -0.108175  0.041074  ... -0.005931 -0.089947 -0.010503  0.077362   \n",
            "4 -0.040576  0.073950 -0.058569  ... -0.079893 -0.038126 -0.095315  0.112316   \n",
            "5 -0.126576 -0.152112 -0.069325  ... -0.028109  0.103588 -0.003243  0.043243   \n",
            "6  0.102617  0.096054  0.003210  ... -0.036933  0.010865  0.066728  0.112904   \n",
            "7  0.093271  0.076127 -0.004166  ... -0.105497  0.043385  0.102613  0.121523   \n",
            "8  0.016770 -0.150593  0.057095  ... -0.014134 -0.041150  0.043610 -0.033362   \n",
            "9  0.090238 -0.031772 -0.016623  ... -0.092037  0.067543 -0.015439  0.107121   \n",
            "\n",
            "         56        57        58        59  layer  neuron_index  \n",
            "0 -0.068317  0.057229 -0.010008 -0.046776    fc2             0  \n",
            "1  0.154287 -0.029886 -0.088726  0.017459    fc2             1  \n",
            "2 -0.002260 -0.142917  0.077585  0.014963    fc2             2  \n",
            "3 -0.088103 -0.110321  0.010353  0.046034    fc2             3  \n",
            "4  0.099081  0.096792  0.056523 -0.063165    fc2             4  \n",
            "5 -0.133819 -0.110977 -0.123005 -0.015103    fc2             5  \n",
            "6  0.097748 -0.037395  0.050451 -0.001918    fc2             6  \n",
            "7 -0.045865 -0.148744  0.003459 -0.012868    fc2             7  \n",
            "8 -0.111366 -0.025910  0.000013  0.098205    fc2             8  \n",
            "9 -0.079098  0.062496 -0.018937  0.063364    fc2             9  \n",
            "\n",
            "[10 rows x 62 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Image 4 ---\n",
            "True Label: 1\n",
            "Predicted Label: 1\n",
            "\n",
            "CONV1_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv1             0          0.202786\n",
            "1   conv1             1          0.228470\n",
            "2   conv1             2          0.096697\n",
            "3   conv1             3          0.029753\n",
            "4   conv1             4          0.046277\n",
            "5   conv1             5          0.229464\n",
            "6   conv1             6          0.036728\n",
            "7   conv1             7          0.273732\n",
            "8   conv1             8          0.064595\n",
            "9   conv1             9          0.056627\n",
            "10  conv1            10          0.051417\n",
            "11  conv1            11          0.168968\n",
            "12  conv1            12          0.003446\n",
            "13  conv1            13          0.048994\n",
            "14  conv1            14         -0.008358\n",
            "15  conv1            15          0.004905\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.400412  0.399371  0.071231 -0.067315  0.237489  0.328836 -0.150906   \n",
            "1   0.062670  0.420839  0.423627 -0.087857  0.270959 -0.074010  0.234360   \n",
            "2   0.077322  0.239514  0.362599  0.313162 -0.159267 -0.146957 -0.197186   \n",
            "3  -0.198520  0.416571  0.189112 -0.133381  0.282581  0.187163 -0.016161   \n",
            "4  -0.100635 -0.007484  0.208964  0.262549  0.069296  0.175801  0.311726   \n",
            "5  -0.273976 -0.003054  0.224042  0.069278  0.076746  0.157825  0.075656   \n",
            "6  -0.428757 -0.251183 -0.254338  0.081380  0.144253  0.244236  0.337603   \n",
            "7   0.378913  0.399986  0.071193  0.172418 -0.026089  0.130204 -0.220100   \n",
            "8   0.225943  0.192161  0.387705  0.135396 -0.048392 -0.038235 -0.115391   \n",
            "9   0.307195 -0.180329 -0.171382  0.319853  0.081811  0.181218  0.174932   \n",
            "10  0.130797  0.370466  0.380331  0.015307 -0.078076  0.016426 -0.083861   \n",
            "11  0.392175  0.247163  0.281231  0.259225 -0.270856 -0.165327  0.253949   \n",
            "12 -0.210740 -0.048877 -0.131766  0.335698  0.006075 -0.116493 -0.017947   \n",
            "13  0.119667  0.026561  0.093274  0.343750  0.204018 -0.236069  0.372014   \n",
            "14  0.198226 -0.245182  0.121569 -0.049056 -0.133100 -0.077453  0.442164   \n",
            "15  0.209107  0.173020 -0.301732  0.125146 -0.021763 -0.264287  0.322271   \n",
            "\n",
            "           7         8  layer  neuron_index  \n",
            "0   0.392861 -0.045586  conv1             0  \n",
            "1  -0.093755  0.006661  conv1             1  \n",
            "2   0.068652  0.012842  conv1             2  \n",
            "3  -0.195803  0.062113  conv1             3  \n",
            "4  -0.130634  0.326892  conv1             4  \n",
            "5   0.073296 -0.115009  conv1             5  \n",
            "6  -0.101311  0.121107  conv1             6  \n",
            "7   0.377975 -0.272754  conv1             7  \n",
            "8   0.228945  0.188063  conv1             8  \n",
            "9   0.345546  0.172957  conv1             9  \n",
            "10  0.056517  0.259929  conv1            10  \n",
            "11 -0.198929  0.022492  conv1            11  \n",
            "12  0.285077  0.004282  conv1            12  \n",
            "13  0.204587 -0.130169  conv1            13  \n",
            "14 -0.012433 -0.308704  conv1            14  \n",
            "15 -0.069897  0.135285  conv1            15  \n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv2             0          0.071940\n",
            "1   conv2             1          0.443661\n",
            "2   conv2             2          0.162161\n",
            "3   conv2             3          0.556482\n",
            "4   conv2             4         -0.164623\n",
            "5   conv2             5         -0.237601\n",
            "6   conv2             6          0.334133\n",
            "7   conv2             7          0.273973\n",
            "8   conv2             8          0.157335\n",
            "9   conv2             9          0.065807\n",
            "10  conv2            10          0.475794\n",
            "11  conv2            11          0.155512\n",
            "12  conv2            12          0.288310\n",
            "13  conv2            13          0.029698\n",
            "14  conv2            14          0.280480\n",
            "15  conv2            15          0.192537\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.053389  0.023715 -0.082655 -0.048249 -0.076035 -0.150770  0.021789   \n",
            "1   0.047589 -0.093007  0.005160  0.069936  0.136776  0.046387  0.010860   \n",
            "2   0.058461  0.100847 -0.038368  0.022629  0.103998 -0.064163 -0.030390   \n",
            "3  -0.035365  0.023469  0.107092  0.101842  0.143467  0.217515 -0.012316   \n",
            "4   0.062249 -0.131436 -0.046978  0.021597 -0.080085 -0.078712 -0.103583   \n",
            "5  -0.019309 -0.080785  0.035847  0.020273  0.009665  0.035786 -0.006190   \n",
            "6   0.049547  0.038741 -0.036449 -0.002058  0.041931  0.053819 -0.005733   \n",
            "7  -0.152914 -0.041600  0.115807 -0.041148 -0.039541  0.071635 -0.011336   \n",
            "8   0.125563  0.033356 -0.048423  0.110720 -0.018201 -0.147717 -0.042072   \n",
            "9  -0.068213 -0.115001 -0.004743 -0.028494 -0.027653  0.079632  0.049930   \n",
            "10 -0.032338 -0.013617  0.151200 -0.075678  0.075957  0.207855  0.038870   \n",
            "11  0.051104  0.012186  0.086417  0.028352  0.156211 -0.002386  0.086014   \n",
            "12  0.082138  0.005539 -0.044524  0.132945  0.106715  0.022446  0.116672   \n",
            "13 -0.043048 -0.140184  0.012286 -0.060239 -0.119047  0.023106 -0.004832   \n",
            "14 -0.021613  0.105474  0.025686  0.030302  0.136904 -0.020385 -0.046524   \n",
            "15  0.117869  0.126556  0.051337  0.002796 -0.029835 -0.061113  0.004300   \n",
            "\n",
            "           7         8         9  ...       136       137       138       139  \\\n",
            "0   0.087136  0.136641  0.125749  ...  0.105212  0.008460  0.086248 -0.005925   \n",
            "1   0.068729 -0.004232 -0.004889  ...  0.075859  0.051621  0.013238  0.045813   \n",
            "2  -0.050265  0.004671  0.000319  ...  0.094657  0.007707  0.022765  0.004527   \n",
            "3   0.084479  0.127317 -0.104037  ...  0.000802  0.089539 -0.032844  0.050453   \n",
            "4   0.048199  0.034387 -0.074277  ...  0.045809 -0.082850  0.027167 -0.046521   \n",
            "5  -0.040837 -0.020941  0.074329  ... -0.027615 -0.036336 -0.055786  0.008235   \n",
            "6  -0.068649 -0.066126 -0.024237  ...  0.095926  0.090875 -0.025141  0.018905   \n",
            "7   0.034141  0.078109 -0.036853  ...  0.086062 -0.007312  0.005437  0.073091   \n",
            "8   0.026514 -0.127686 -0.001961  ...  0.017386 -0.016188  0.018640  0.010524   \n",
            "9  -0.042845  0.092674  0.078472  ... -0.005325  0.106815  0.014020 -0.074019   \n",
            "10 -0.053325  0.046839  0.006819  ...  0.041650  0.170038 -0.044293  0.004112   \n",
            "11  0.042763 -0.056389 -0.043120  ...  0.042914 -0.032490  0.009025  0.004950   \n",
            "12  0.132660 -0.116526 -0.137380  ... -0.008512 -0.055690  0.034679  0.085180   \n",
            "13 -0.028790  0.080668 -0.064381  ... -0.020423  0.075944 -0.070604 -0.053503   \n",
            "14  0.006731 -0.044295  0.048296  ... -0.026215  0.104971  0.072772  0.041096   \n",
            "15  0.013864 -0.005772  0.182691  ... -0.027957 -0.073938 -0.020258 -0.038572   \n",
            "\n",
            "         140       141       142       143  layer  neuron_index  \n",
            "0  -0.033853  0.113257 -0.040305  0.088088  conv2             0  \n",
            "1   0.091032  0.005281 -0.026384  0.078543  conv2             1  \n",
            "2   0.002996  0.085109  0.008654  0.067731  conv2             2  \n",
            "3  -0.007134  0.030011 -0.008236 -0.030027  conv2             3  \n",
            "4   0.067527 -0.041539  0.057837  0.032542  conv2             4  \n",
            "5  -0.075360 -0.063744  0.026530  0.010399  conv2             5  \n",
            "6   0.031970 -0.055053  0.091992 -0.061894  conv2             6  \n",
            "7   0.030808 -0.011768  0.093748  0.089395  conv2             7  \n",
            "8  -0.089284  0.094356  0.033373  0.012280  conv2             8  \n",
            "9   0.011533 -0.078723  0.016492 -0.013510  conv2             9  \n",
            "10  0.017894 -0.050105  0.031401  0.045740  conv2            10  \n",
            "11  0.000354  0.074826  0.052800  0.083592  conv2            11  \n",
            "12  0.014633  0.005773  0.066127 -0.082251  conv2            12  \n",
            "13  0.020353 -0.001231  0.079536  0.032849  conv2            13  \n",
            "14  0.005841  0.030748 -0.013547 -0.054549  conv2            14  \n",
            "15 -0.075074  0.053019 -0.002690 -0.082208  conv2            15  \n",
            "\n",
            "[16 rows x 146 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_ACTIVATIONS DataFrame:\n",
            "   layer  neuron_index  activation_value\n",
            "0    fc1             0         -2.374377\n",
            "1    fc1             1         -2.065357\n",
            "2    fc1             2         -6.169060\n",
            "3    fc1             3         -2.350203\n",
            "4    fc1             4         -2.859513\n",
            "5    fc1             5         -0.981580\n",
            "6    fc1             6         -2.894818\n",
            "7    fc1             7         -0.658419\n",
            "8    fc1             8         -0.736814\n",
            "9    fc1             9          6.843378\n",
            "10   fc1            10         -2.336852\n",
            "11   fc1            11         -2.599482\n",
            "12   fc1            12         10.727160\n",
            "13   fc1            13         19.140759\n",
            "14   fc1            14         -2.871072\n",
            "15   fc1            15         -6.848743\n",
            "16   fc1            16         -3.324179\n",
            "17   fc1            17         -1.996545\n",
            "18   fc1            18          2.451436\n",
            "19   fc1            19          1.082092\n",
            "20   fc1            20         12.959995\n",
            "21   fc1            21         -5.189202\n",
            "22   fc1            22         -3.753217\n",
            "23   fc1            23          9.446451\n",
            "24   fc1            24         -2.990185\n",
            "25   fc1            25         -2.365512\n",
            "26   fc1            26         -2.343669\n",
            "27   fc1            27         -1.444633\n",
            "28   fc1            28         -3.006880\n",
            "29   fc1            29         10.897516\n",
            "30   fc1            30         -2.901538\n",
            "31   fc1            31         -3.627739\n",
            "32   fc1            32         -4.071685\n",
            "33   fc1            33         10.292527\n",
            "34   fc1            34         -2.803623\n",
            "35   fc1            35         10.006472\n",
            "36   fc1            36         22.525322\n",
            "37   fc1            37         13.412393\n",
            "38   fc1            38          7.477548\n",
            "39   fc1            39         -6.025717\n",
            "40   fc1            40         -1.277579\n",
            "41   fc1            41         -2.818062\n",
            "42   fc1            42         -2.445110\n",
            "43   fc1            43         -2.337788\n",
            "44   fc1            44         -2.619355\n",
            "45   fc1            45         -2.986756\n",
            "46   fc1            46         -1.914032\n",
            "47   fc1            47         -2.003424\n",
            "48   fc1            48         -1.948035\n",
            "49   fc1            49         19.220636\n",
            "50   fc1            50          6.128818\n",
            "51   fc1            51         -1.386397\n",
            "52   fc1            52         11.983940\n",
            "53   fc1            53         -2.171559\n",
            "54   fc1            54         -1.723909\n",
            "55   fc1            55         -3.847883\n",
            "56   fc1            56         -1.869305\n",
            "57   fc1            57          2.092607\n",
            "58   fc1            58         -5.502591\n",
            "59   fc1            59         -2.898663\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.012868  0.012967 -0.004651  0.004214  0.004450 -0.004286 -0.006892   \n",
            "1  -0.006668  0.001739  0.008233 -0.015734 -0.018150  0.006245 -0.009190   \n",
            "2  -0.004715 -0.008510 -0.027038 -0.012099 -0.017072 -0.001276 -0.004790   \n",
            "3  -0.022022  0.006158 -0.000070  0.005327 -0.010999 -0.013339 -0.000802   \n",
            "4  -0.021747  0.005062 -0.007141 -0.002509  0.002286 -0.004356 -0.013751   \n",
            "5   0.000488  0.012703  0.014533  0.016341 -0.004459  0.019696  0.002125   \n",
            "6   0.009300 -0.022666 -0.019488  0.000888 -0.000055 -0.011650 -0.015693   \n",
            "7   0.031482  0.014408  0.003062  0.002343  0.027174  0.003222  0.007536   \n",
            "8  -0.021455 -0.018184  0.010642 -0.009969 -0.002964 -0.022868  0.002633   \n",
            "9   0.008664  0.005595  0.018707  0.036951  0.012074  0.024569  0.035212   \n",
            "10 -0.015157 -0.015687 -0.020529  0.011342  0.001704 -0.014001 -0.023534   \n",
            "11  0.033904  0.009318  0.023171  0.003949  0.003715  0.000735  0.023219   \n",
            "12  0.024346  0.032305  0.026967  0.025876  0.010165  0.022697  0.011940   \n",
            "13  0.008851  0.010857  0.002167  0.024771  0.005659  0.021011  0.000578   \n",
            "14 -0.009209 -0.003457 -0.008076 -0.003801  0.010076 -0.013077 -0.018169   \n",
            "15 -0.022892 -0.007363 -0.001998 -0.025108 -0.005332 -0.008478 -0.006340   \n",
            "16 -0.011208 -0.008056 -0.020657 -0.002330  0.002901 -0.009348 -0.019556   \n",
            "17 -0.004303 -0.011543 -0.022747 -0.002148 -0.000974  0.003379  0.001518   \n",
            "18  0.014590  0.013593  0.026992  0.014942  0.002147  0.024546  0.023784   \n",
            "19  0.013653  0.015041  0.026944  0.017540  0.012362  0.005006  0.020771   \n",
            "20  0.000926 -0.002540  0.000976 -0.007426  0.008134  0.009121  0.006932   \n",
            "21 -0.008481 -0.009680 -0.015762 -0.008213  0.005569 -0.011419 -0.018695   \n",
            "22  0.002687 -0.004754  0.025822  0.022829  0.026687  0.024950  0.015143   \n",
            "23  0.012675 -0.008788  0.020707 -0.002437 -0.000562 -0.001121  0.009253   \n",
            "24 -0.021873 -0.006459  0.000207 -0.022165  0.008027 -0.006639  0.011209   \n",
            "25 -0.009614 -0.022043 -0.007360  0.004037 -0.017339 -0.001730 -0.005209   \n",
            "26 -0.003378 -0.006826 -0.019353 -0.008899  0.007771  0.007703 -0.008105   \n",
            "27 -0.009939 -0.008868  0.000785  0.008521 -0.017249  0.009543 -0.006277   \n",
            "28 -0.012323  0.002773 -0.002832  0.008380 -0.001049 -0.023035 -0.010383   \n",
            "29  0.014561  0.016938  0.017683 -0.001824  0.010139  0.014729  0.012067   \n",
            "30 -0.019001 -0.009498 -0.020953 -0.004602  0.011816  0.005326 -0.018519   \n",
            "31  0.000243  0.000025  0.021961 -0.006239  0.018972 -0.004205  0.002377   \n",
            "32 -0.003725 -0.003355 -0.020392  0.005503 -0.013929 -0.013021  0.001984   \n",
            "33  0.016569  0.002955  0.006740  0.025918  0.010518  0.006316  0.009447   \n",
            "34 -0.021455 -0.010343  0.011155  0.004942 -0.004358 -0.012782 -0.010427   \n",
            "35 -0.004436 -0.022906 -0.004501  0.005599  0.007464 -0.000397 -0.000361   \n",
            "36  0.028495  0.008049  0.033686  0.027099  0.001744  0.033556  0.005032   \n",
            "37  0.017607  0.014472  0.003794  0.007778  0.026253  0.026272  0.000011   \n",
            "38 -0.002645  0.017745  0.021209 -0.002706  0.004354  0.021500  0.015463   \n",
            "39  0.013660  0.008610  0.018474  0.003934  0.033713  0.001638  0.016512   \n",
            "40  0.030918  0.000622  0.009956  0.019433  0.000461 -0.000907  0.013701   \n",
            "41 -0.014202 -0.010007 -0.013808 -0.019121  0.001727 -0.010824  0.006965   \n",
            "42 -0.018572 -0.000763  0.011354 -0.020072 -0.000430  0.000737 -0.016534   \n",
            "43 -0.016530 -0.011671 -0.013214 -0.002635  0.008195 -0.009454 -0.014988   \n",
            "44 -0.004162  0.004954 -0.016971 -0.016766 -0.005005 -0.002056 -0.009664   \n",
            "45 -0.006126  0.000294 -0.010289 -0.013374 -0.021398 -0.016979 -0.005077   \n",
            "46 -0.003019  0.003975 -0.013475  0.004994 -0.012704 -0.002466 -0.018315   \n",
            "47  0.002745  0.005883 -0.017230  0.000825 -0.021160 -0.008062 -0.022425   \n",
            "48 -0.000349 -0.019200  0.004244  0.012784  0.002108 -0.011295 -0.014164   \n",
            "49  0.019498  0.027400  0.033523  0.031643  0.003375  0.024539  0.018732   \n",
            "50  0.002034  0.025831  0.021195  0.015002  0.011998  0.006490  0.015592   \n",
            "51  0.002681 -0.004163 -0.002470  0.002025  0.000847 -0.004836 -0.020047   \n",
            "52  0.031499  0.006817  0.030368  0.016483  0.009225  0.011744  0.023579   \n",
            "53 -0.002231 -0.016763 -0.012501  0.002132 -0.016919  0.000812  0.009893   \n",
            "54 -0.016136  0.002641  0.006465  0.005333  0.000611 -0.021958  0.005562   \n",
            "55  0.006558  0.001421 -0.006817 -0.021912 -0.021949 -0.012012  0.008985   \n",
            "56 -0.013326  0.005539 -0.001117  0.010829  0.002428 -0.019694 -0.017891   \n",
            "57  0.000068  0.003597  0.006211  0.004677 -0.000560  0.023743  0.012332   \n",
            "58  0.028027  0.022394  0.015621  0.022584  0.027930  0.013303  0.023036   \n",
            "59 -0.016706  0.001549  0.002360 -0.005833 -0.013946 -0.005872 -0.015406   \n",
            "\n",
            "           7         8         9  ...      3128      3129      3130      3131  \\\n",
            "0   0.007324 -0.009616 -0.005067  ... -0.006856  0.010652  0.010737 -0.005627   \n",
            "1  -0.018485 -0.011243 -0.014907  ...  0.003302 -0.023178 -0.002841 -0.002705   \n",
            "2   0.003348 -0.021924  0.002252  ... -0.004555 -0.022496 -0.027291  0.000060   \n",
            "3  -0.006648 -0.017124  0.011142  ... -0.008438  0.007751 -0.003895 -0.014379   \n",
            "4   0.009772 -0.011616 -0.003061  ... -0.013893 -0.023509  0.007052  0.009660   \n",
            "5   0.008894  0.007604  0.022646  ...  0.023266  0.041666  0.029488  0.033174   \n",
            "6  -0.003789  0.009797 -0.006847  ...  0.009045 -0.017039 -0.017816  0.009705   \n",
            "7   0.004008  0.015966  0.020602  ...  0.009192  0.034439  0.014094  0.028504   \n",
            "8  -0.003198 -0.022776  0.003764  ... -0.001452 -0.006950 -0.022771  0.003933   \n",
            "9   0.030930  0.004664  0.003770  ...  0.004832 -0.009257 -0.008188  0.001435   \n",
            "10 -0.013386 -0.013077 -0.018894  ... -0.001648  0.000762 -0.017888 -0.016899   \n",
            "11  0.006491  0.032536  0.000740  ...  0.003196  0.004032  0.010177  0.012247   \n",
            "12  0.025155  0.015860  0.000769  ...  0.006513  0.015600  0.012484  0.016776   \n",
            "13  0.029623  0.026839 -0.001954  ...  0.036832  0.027887  0.015517  0.010580   \n",
            "14 -0.016337  0.004727  0.010331  ... -0.012719 -0.016295  0.002682 -0.014291   \n",
            "15 -0.017671 -0.001590 -0.007524  ... -0.027042 -0.015744 -0.009734 -0.021105   \n",
            "16 -0.015554  0.001033  0.005761  ...  0.009729 -0.022332 -0.021350 -0.016759   \n",
            "17  0.010020 -0.019688 -0.004400  ...  0.007721 -0.004402 -0.016819 -0.015915   \n",
            "18  0.002784 -0.006211  0.013223  ...  0.024819  0.008977  0.035165  0.001899   \n",
            "19  0.014684  0.027412  0.018771  ...  0.027893  0.023726  0.017920  0.016699   \n",
            "20 -0.019416 -0.022032 -0.013255  ...  0.047160  0.036355  0.025181  0.022490   \n",
            "21  0.001027 -0.005196 -0.004424  ... -0.011853 -0.003937  0.003567  0.000304   \n",
            "22  0.004678  0.017792 -0.000148  ...  0.014265  0.008728 -0.017063  0.005836   \n",
            "23  0.005949 -0.008079  0.000984  ... -0.013547 -0.005859  0.002466  0.003944   \n",
            "24 -0.012757 -0.023823 -0.018663  ...  0.011678 -0.019435  0.003086 -0.019769   \n",
            "25 -0.010399  0.010380 -0.019753  ...  0.000844  0.010552 -0.019871 -0.019154   \n",
            "26  0.010319 -0.010890  0.007982  ...  0.000887  0.000956  0.001107 -0.014635   \n",
            "27 -0.004731 -0.009723 -0.003905  ...  0.005012 -0.016505 -0.011187 -0.001365   \n",
            "28 -0.015085 -0.001597 -0.002212  ... -0.009379  0.010932  0.000835  0.011187   \n",
            "29  0.009418  0.025151  0.020776  ... -0.004506 -0.015641 -0.013120 -0.010891   \n",
            "30 -0.016916  0.009760 -0.014054  ...  0.007682 -0.003761 -0.015353 -0.000961   \n",
            "31  0.012016 -0.000764 -0.007299  ... -0.012403 -0.009862 -0.011390 -0.003945   \n",
            "32 -0.024166  0.000700  0.009655  ... -0.019903 -0.006628 -0.020596  0.001379   \n",
            "33  0.010933  0.003280  0.013948  ... -0.003015  0.020623 -0.000906  0.015280   \n",
            "34 -0.017208  0.006274  0.003570  ... -0.013828 -0.008093 -0.019012 -0.001721   \n",
            "35  0.005101 -0.010963 -0.009127  ... -0.002773  0.011193  0.019547  0.002818   \n",
            "36  0.012425  0.009884  0.031366  ...  0.042560  0.039935  0.023204  0.029524   \n",
            "37  0.023419  0.006894  0.026621  ...  0.004115  0.000018  0.020470 -0.002871   \n",
            "38  0.018896  0.024307 -0.000390  ... -0.004216 -0.012399 -0.001571 -0.018753   \n",
            "39  0.010963  0.030405  0.032920  ... -0.013158  0.001995 -0.017575  0.000686   \n",
            "40  0.026295  0.013387 -0.001529  ...  0.043029  0.038009  0.020855  0.017207   \n",
            "41 -0.004680 -0.013764 -0.007863  ...  0.008035  0.010515 -0.006477  0.011474   \n",
            "42 -0.000678  0.009945  0.003108  ...  0.008813  0.006511 -0.001635 -0.017173   \n",
            "43 -0.011972 -0.016392  0.001932  ... -0.010337 -0.012674 -0.018367 -0.022919   \n",
            "44  0.003259 -0.000585  0.001807  ...  0.008413 -0.014572 -0.008983  0.005895   \n",
            "45  0.006896 -0.008086 -0.009275  ... -0.005778  0.007000  0.002406 -0.008275   \n",
            "46  0.010720 -0.002166 -0.014401  ... -0.013205 -0.022235 -0.002017 -0.000171   \n",
            "47 -0.005961 -0.003954  0.010139  ...  0.011701  0.001282 -0.018512 -0.003118   \n",
            "48 -0.013545  0.006565 -0.005001  ... -0.010218  0.010282 -0.019655  0.010408   \n",
            "49  0.018647  0.026510  0.016471  ...  0.039909  0.049060  0.028912  0.045582   \n",
            "50  0.000300  0.010269  0.008490  ...  0.023184 -0.001352  0.019045  0.003499   \n",
            "51  0.004892  0.007555  0.001219  ... -0.016053  0.001421 -0.013407 -0.006220   \n",
            "52  0.030600  0.026217  0.023260  ...  0.017529  0.006934  0.017406  0.020949   \n",
            "53 -0.010024 -0.009774 -0.018796  ... -0.015735 -0.020443  0.008829 -0.016565   \n",
            "54  0.011039 -0.000971 -0.004749  ... -0.011213  0.000786 -0.019802 -0.001971   \n",
            "55 -0.017711 -0.011881  0.008669  ...  0.003574  0.005007 -0.005511 -0.009074   \n",
            "56 -0.018805 -0.010618  0.002681  ... -0.014544 -0.013959  0.007968 -0.004889   \n",
            "57  0.013836  0.000618  0.024202  ...  0.020163  0.034249  0.023702  0.032346   \n",
            "58  0.020271 -0.000923  0.004961  ...  0.030553  0.023524  0.025896  0.012225   \n",
            "59  0.006506 -0.000810  0.000023  ... -0.021418 -0.022299 -0.008617 -0.017246   \n",
            "\n",
            "        3132      3133      3134      3135  layer  neuron_index  \n",
            "0  -0.006300  0.000761 -0.010077 -0.010894    fc1             0  \n",
            "1   0.007173  0.003588 -0.009852 -0.016465    fc1             1  \n",
            "2   0.005837 -0.015450 -0.013901  0.002791    fc1             2  \n",
            "3  -0.012918  0.007607 -0.013114 -0.016117    fc1             3  \n",
            "4   0.003922 -0.010699  0.009847 -0.021692    fc1             4  \n",
            "5   0.028678  0.014802  0.011530  0.006059    fc1             5  \n",
            "6  -0.014932  0.004915  0.002246 -0.007906    fc1             6  \n",
            "7   0.013501  0.022247  0.021532 -0.003630    fc1             7  \n",
            "8  -0.006297 -0.015515 -0.013881  0.004368    fc1             8  \n",
            "9  -0.001923  0.008458  0.021580  0.029945    fc1             9  \n",
            "10 -0.000172 -0.004697 -0.007486  0.003054    fc1            10  \n",
            "11 -0.007124 -0.002595 -0.013320 -0.004145    fc1            11  \n",
            "12 -0.001142  0.025493  0.015860  0.012417    fc1            12  \n",
            "13  0.008905  0.014851  0.010457  0.007798    fc1            13  \n",
            "14 -0.010744 -0.008016 -0.019082 -0.007001    fc1            14  \n",
            "15 -0.008899 -0.000829 -0.016586 -0.028002    fc1            15  \n",
            "16  0.011897 -0.018047 -0.002446 -0.017222    fc1            16  \n",
            "17 -0.020536  0.006682 -0.007786 -0.019112    fc1            17  \n",
            "18  0.014848  0.023633  0.009814 -0.009907    fc1            18  \n",
            "19  0.032436  0.014681  0.007802  0.015888    fc1            19  \n",
            "20  0.010047  0.026064  0.016482  0.030144    fc1            20  \n",
            "21  0.008994  0.010006 -0.018727 -0.003075    fc1            21  \n",
            "22 -0.000216 -0.005094  0.018505 -0.008893    fc1            22  \n",
            "23  0.027010  0.004446 -0.004685  0.011521    fc1            23  \n",
            "24 -0.006039  0.005686 -0.007776  0.007923    fc1            24  \n",
            "25 -0.013028 -0.005355 -0.013795  0.000189    fc1            25  \n",
            "26  0.001260  0.002817 -0.000567 -0.018858    fc1            26  \n",
            "27 -0.012066  0.005803  0.008898 -0.020256    fc1            27  \n",
            "28 -0.021011 -0.016277 -0.021642 -0.017041    fc1            28  \n",
            "29  0.015582 -0.001358  0.009005  0.020085    fc1            29  \n",
            "30  0.009142 -0.011048 -0.019065 -0.022964    fc1            30  \n",
            "31 -0.017578  0.018184 -0.014494  0.006414    fc1            31  \n",
            "32 -0.007214 -0.020134 -0.002980 -0.009061    fc1            32  \n",
            "33 -0.002171 -0.004688  0.024596  0.012860    fc1            33  \n",
            "34  0.006539  0.009605 -0.015453 -0.016520    fc1            34  \n",
            "35 -0.018720 -0.022744 -0.017856  0.009409    fc1            35  \n",
            "36  0.040159  0.015496  0.046070  0.038217    fc1            36  \n",
            "37  0.016869  0.005396  0.012609  0.013487    fc1            37  \n",
            "38 -0.000505  0.023441  0.003526  0.005332    fc1            38  \n",
            "39 -0.019023 -0.006869 -0.015280  0.005979    fc1            39  \n",
            "40  0.027607  0.004566  0.021181  0.012148    fc1            40  \n",
            "41  0.004879 -0.012685 -0.009628 -0.022963    fc1            41  \n",
            "42  0.007932 -0.012811 -0.015902 -0.008890    fc1            42  \n",
            "43 -0.016014  0.007799  0.005429 -0.008904    fc1            43  \n",
            "44  0.004329  0.007101 -0.003699 -0.012319    fc1            44  \n",
            "45  0.005780  0.009909 -0.004622 -0.021927    fc1            45  \n",
            "46 -0.013934 -0.011455 -0.007828 -0.010529    fc1            46  \n",
            "47 -0.016764 -0.023845 -0.003573  0.011422    fc1            47  \n",
            "48 -0.015247 -0.011081 -0.016139 -0.007430    fc1            48  \n",
            "49  0.037967  0.039943  0.047666  0.032830    fc1            49  \n",
            "50  0.002823 -0.002692 -0.008353  0.002865    fc1            50  \n",
            "51 -0.014761  0.009049 -0.008702 -0.023180    fc1            51  \n",
            "52  0.023632  0.022728  0.018886  0.025511    fc1            52  \n",
            "53 -0.018722  0.011494  0.004309 -0.023839    fc1            53  \n",
            "54  0.000339  0.006231  0.007703 -0.013026    fc1            54  \n",
            "55 -0.014757 -0.008799 -0.020483 -0.005706    fc1            55  \n",
            "56 -0.014795  0.005220 -0.021989 -0.015774    fc1            56  \n",
            "57 -0.000363  0.006102 -0.002093 -0.001637    fc1            57  \n",
            "58  0.026156  0.011964  0.002414  0.006485    fc1            58  \n",
            "59 -0.003636  0.008666 -0.018831 -0.015422    fc1            59  \n",
            "\n",
            "[60 rows x 3138 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_ACTIVATIONS DataFrame:\n",
            "  layer  neuron_index  activation_value\n",
            "0   fc2             0         -6.652503\n",
            "1   fc2             1         12.782240\n",
            "2   fc2             2         -1.835445\n",
            "3   fc2             3         -6.027697\n",
            "4   fc2             4         -5.707763\n",
            "5   fc2             5         -7.418067\n",
            "6   fc2             6         -7.733752\n",
            "7   fc2             7         -3.690409\n",
            "8   fc2             8         -9.035890\n",
            "9   fc2             9          5.839262\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_CONNECTIONS DataFrame:\n",
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.107950 -0.100333 -0.125853  0.091329 -0.090712 -0.024337 -0.035301   \n",
            "1  0.131501 -0.110936 -0.085158  0.066024 -0.037845 -0.071343 -0.052721   \n",
            "2  0.050709 -0.033907 -0.077746 -0.074659 -0.076966 -0.074498 -0.088701   \n",
            "3  0.049182 -0.027318 -0.033456 -0.035291 -0.043763 -0.003068  0.013614   \n",
            "4  0.042645  0.088435  0.038246  0.004014 -0.031166 -0.035241 -0.033445   \n",
            "5  0.086447 -0.126483 -0.040570  0.021679  0.066402 -0.114365  0.069705   \n",
            "6 -0.080449  0.085518 -0.090617 -0.053715 -0.023148 -0.005910  0.053310   \n",
            "7  0.092976 -0.108292 -0.003311 -0.020869  0.013422 -0.048318  0.010532   \n",
            "8 -0.123973 -0.038942  0.098014 -0.079174  0.093177  0.066027  0.115322   \n",
            "9  0.017517 -0.083782  0.037875 -0.062554 -0.064439  0.184223  0.006078   \n",
            "\n",
            "          7         8         9  ...        52        53        54        55  \\\n",
            "0  0.090453 -0.134711  0.053470  ... -0.070573  0.083195  0.095116  0.083355   \n",
            "1 -0.112283  0.045542  0.064597  ...  0.047196  0.127483  0.055454  0.038554   \n",
            "2  0.057485 -0.012508 -0.048672  ...  0.098187  0.072752 -0.079741 -0.097680   \n",
            "3 -0.134513 -0.108175  0.041074  ... -0.005931 -0.089947 -0.010503  0.077362   \n",
            "4 -0.040576  0.073950 -0.058569  ... -0.079893 -0.038126 -0.095315  0.112316   \n",
            "5 -0.126576 -0.152112 -0.069325  ... -0.028109  0.103588 -0.003243  0.043243   \n",
            "6  0.102617  0.096054  0.003210  ... -0.036933  0.010865  0.066728  0.112904   \n",
            "7  0.093271  0.076127 -0.004166  ... -0.105497  0.043385  0.102613  0.121523   \n",
            "8  0.016770 -0.150593  0.057095  ... -0.014134 -0.041150  0.043610 -0.033362   \n",
            "9  0.090238 -0.031772 -0.016623  ... -0.092037  0.067543 -0.015439  0.107121   \n",
            "\n",
            "         56        57        58        59  layer  neuron_index  \n",
            "0 -0.068317  0.057229 -0.010008 -0.046776    fc2             0  \n",
            "1  0.154287 -0.029886 -0.088726  0.017459    fc2             1  \n",
            "2 -0.002260 -0.142917  0.077585  0.014963    fc2             2  \n",
            "3 -0.088103 -0.110321  0.010353  0.046034    fc2             3  \n",
            "4  0.099081  0.096792  0.056523 -0.063165    fc2             4  \n",
            "5 -0.133819 -0.110977 -0.123005 -0.015103    fc2             5  \n",
            "6  0.097748 -0.037395  0.050451 -0.001918    fc2             6  \n",
            "7 -0.045865 -0.148744  0.003459 -0.012868    fc2             7  \n",
            "8 -0.111366 -0.025910  0.000013  0.098205    fc2             8  \n",
            "9 -0.079098  0.062496 -0.018937  0.063364    fc2             9  \n",
            "\n",
            "[10 rows x 62 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Image 5 ---\n",
            "True Label: 1\n",
            "Predicted Label: 1\n",
            "\n",
            "CONV1_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv1             0          0.256903\n",
            "1   conv1             1          0.268667\n",
            "2   conv1             2          0.116413\n",
            "3   conv1             3          0.050264\n",
            "4   conv1             4          0.084849\n",
            "5   conv1             5          0.239303\n",
            "6   conv1             6          0.033031\n",
            "7   conv1             7          0.308687\n",
            "8   conv1             8          0.104540\n",
            "9   conv1             9          0.099184\n",
            "10  conv1            10          0.088309\n",
            "11  conv1            11          0.197337\n",
            "12  conv1            12          0.007084\n",
            "13  conv1            13          0.083461\n",
            "14  conv1            14         -0.010568\n",
            "15  conv1            15          0.015516\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.400412  0.399371  0.071231 -0.067315  0.237489  0.328836 -0.150906   \n",
            "1   0.062670  0.420839  0.423627 -0.087857  0.270959 -0.074010  0.234360   \n",
            "2   0.077322  0.239514  0.362599  0.313162 -0.159267 -0.146957 -0.197186   \n",
            "3  -0.198520  0.416571  0.189112 -0.133381  0.282581  0.187163 -0.016161   \n",
            "4  -0.100635 -0.007484  0.208964  0.262549  0.069296  0.175801  0.311726   \n",
            "5  -0.273976 -0.003054  0.224042  0.069278  0.076746  0.157825  0.075656   \n",
            "6  -0.428757 -0.251183 -0.254338  0.081380  0.144253  0.244236  0.337603   \n",
            "7   0.378913  0.399986  0.071193  0.172418 -0.026089  0.130204 -0.220100   \n",
            "8   0.225943  0.192161  0.387705  0.135396 -0.048392 -0.038235 -0.115391   \n",
            "9   0.307195 -0.180329 -0.171382  0.319853  0.081811  0.181218  0.174932   \n",
            "10  0.130797  0.370466  0.380331  0.015307 -0.078076  0.016426 -0.083861   \n",
            "11  0.392175  0.247163  0.281231  0.259225 -0.270856 -0.165327  0.253949   \n",
            "12 -0.210740 -0.048877 -0.131766  0.335698  0.006075 -0.116493 -0.017947   \n",
            "13  0.119667  0.026561  0.093274  0.343750  0.204018 -0.236069  0.372014   \n",
            "14  0.198226 -0.245182  0.121569 -0.049056 -0.133100 -0.077453  0.442164   \n",
            "15  0.209107  0.173020 -0.301732  0.125146 -0.021763 -0.264287  0.322271   \n",
            "\n",
            "           7         8  layer  neuron_index  \n",
            "0   0.392861 -0.045586  conv1             0  \n",
            "1  -0.093755  0.006661  conv1             1  \n",
            "2   0.068652  0.012842  conv1             2  \n",
            "3  -0.195803  0.062113  conv1             3  \n",
            "4  -0.130634  0.326892  conv1             4  \n",
            "5   0.073296 -0.115009  conv1             5  \n",
            "6  -0.101311  0.121107  conv1             6  \n",
            "7   0.377975 -0.272754  conv1             7  \n",
            "8   0.228945  0.188063  conv1             8  \n",
            "9   0.345546  0.172957  conv1             9  \n",
            "10  0.056517  0.259929  conv1            10  \n",
            "11 -0.198929  0.022492  conv1            11  \n",
            "12  0.285077  0.004282  conv1            12  \n",
            "13  0.204587 -0.130169  conv1            13  \n",
            "14 -0.012433 -0.308704  conv1            14  \n",
            "15 -0.069897  0.135285  conv1            15  \n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv2             0          0.128907\n",
            "1   conv2             1          0.579641\n",
            "2   conv2             2          0.243793\n",
            "3   conv2             3          0.711194\n",
            "4   conv2             4         -0.203085\n",
            "5   conv2             5         -0.257309\n",
            "6   conv2             6          0.424128\n",
            "7   conv2             7          0.324716\n",
            "8   conv2             8          0.189164\n",
            "9   conv2             9          0.072802\n",
            "10  conv2            10          0.627651\n",
            "11  conv2            11          0.232772\n",
            "12  conv2            12          0.361196\n",
            "13  conv2            13          0.017888\n",
            "14  conv2            14          0.385643\n",
            "15  conv2            15          0.247360\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.053389  0.023715 -0.082655 -0.048249 -0.076035 -0.150770  0.021789   \n",
            "1   0.047589 -0.093007  0.005160  0.069936  0.136776  0.046387  0.010860   \n",
            "2   0.058461  0.100847 -0.038368  0.022629  0.103998 -0.064163 -0.030390   \n",
            "3  -0.035365  0.023469  0.107092  0.101842  0.143467  0.217515 -0.012316   \n",
            "4   0.062249 -0.131436 -0.046978  0.021597 -0.080085 -0.078712 -0.103583   \n",
            "5  -0.019309 -0.080785  0.035847  0.020273  0.009665  0.035786 -0.006190   \n",
            "6   0.049547  0.038741 -0.036449 -0.002058  0.041931  0.053819 -0.005733   \n",
            "7  -0.152914 -0.041600  0.115807 -0.041148 -0.039541  0.071635 -0.011336   \n",
            "8   0.125563  0.033356 -0.048423  0.110720 -0.018201 -0.147717 -0.042072   \n",
            "9  -0.068213 -0.115001 -0.004743 -0.028494 -0.027653  0.079632  0.049930   \n",
            "10 -0.032338 -0.013617  0.151200 -0.075678  0.075957  0.207855  0.038870   \n",
            "11  0.051104  0.012186  0.086417  0.028352  0.156211 -0.002386  0.086014   \n",
            "12  0.082138  0.005539 -0.044524  0.132945  0.106715  0.022446  0.116672   \n",
            "13 -0.043048 -0.140184  0.012286 -0.060239 -0.119047  0.023106 -0.004832   \n",
            "14 -0.021613  0.105474  0.025686  0.030302  0.136904 -0.020385 -0.046524   \n",
            "15  0.117869  0.126556  0.051337  0.002796 -0.029835 -0.061113  0.004300   \n",
            "\n",
            "           7         8         9  ...       136       137       138       139  \\\n",
            "0   0.087136  0.136641  0.125749  ...  0.105212  0.008460  0.086248 -0.005925   \n",
            "1   0.068729 -0.004232 -0.004889  ...  0.075859  0.051621  0.013238  0.045813   \n",
            "2  -0.050265  0.004671  0.000319  ...  0.094657  0.007707  0.022765  0.004527   \n",
            "3   0.084479  0.127317 -0.104037  ...  0.000802  0.089539 -0.032844  0.050453   \n",
            "4   0.048199  0.034387 -0.074277  ...  0.045809 -0.082850  0.027167 -0.046521   \n",
            "5  -0.040837 -0.020941  0.074329  ... -0.027615 -0.036336 -0.055786  0.008235   \n",
            "6  -0.068649 -0.066126 -0.024237  ...  0.095926  0.090875 -0.025141  0.018905   \n",
            "7   0.034141  0.078109 -0.036853  ...  0.086062 -0.007312  0.005437  0.073091   \n",
            "8   0.026514 -0.127686 -0.001961  ...  0.017386 -0.016188  0.018640  0.010524   \n",
            "9  -0.042845  0.092674  0.078472  ... -0.005325  0.106815  0.014020 -0.074019   \n",
            "10 -0.053325  0.046839  0.006819  ...  0.041650  0.170038 -0.044293  0.004112   \n",
            "11  0.042763 -0.056389 -0.043120  ...  0.042914 -0.032490  0.009025  0.004950   \n",
            "12  0.132660 -0.116526 -0.137380  ... -0.008512 -0.055690  0.034679  0.085180   \n",
            "13 -0.028790  0.080668 -0.064381  ... -0.020423  0.075944 -0.070604 -0.053503   \n",
            "14  0.006731 -0.044295  0.048296  ... -0.026215  0.104971  0.072772  0.041096   \n",
            "15  0.013864 -0.005772  0.182691  ... -0.027957 -0.073938 -0.020258 -0.038572   \n",
            "\n",
            "         140       141       142       143  layer  neuron_index  \n",
            "0  -0.033853  0.113257 -0.040305  0.088088  conv2             0  \n",
            "1   0.091032  0.005281 -0.026384  0.078543  conv2             1  \n",
            "2   0.002996  0.085109  0.008654  0.067731  conv2             2  \n",
            "3  -0.007134  0.030011 -0.008236 -0.030027  conv2             3  \n",
            "4   0.067527 -0.041539  0.057837  0.032542  conv2             4  \n",
            "5  -0.075360 -0.063744  0.026530  0.010399  conv2             5  \n",
            "6   0.031970 -0.055053  0.091992 -0.061894  conv2             6  \n",
            "7   0.030808 -0.011768  0.093748  0.089395  conv2             7  \n",
            "8  -0.089284  0.094356  0.033373  0.012280  conv2             8  \n",
            "9   0.011533 -0.078723  0.016492 -0.013510  conv2             9  \n",
            "10  0.017894 -0.050105  0.031401  0.045740  conv2            10  \n",
            "11  0.000354  0.074826  0.052800  0.083592  conv2            11  \n",
            "12  0.014633  0.005773  0.066127 -0.082251  conv2            12  \n",
            "13  0.020353 -0.001231  0.079536  0.032849  conv2            13  \n",
            "14  0.005841  0.030748 -0.013547 -0.054549  conv2            14  \n",
            "15 -0.075074  0.053019 -0.002690 -0.082208  conv2            15  \n",
            "\n",
            "[16 rows x 146 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_ACTIVATIONS DataFrame:\n",
            "   layer  neuron_index  activation_value\n",
            "0    fc1             0         -3.389480\n",
            "1    fc1             1         -2.884798\n",
            "2    fc1             2         -7.907009\n",
            "3    fc1             3         -3.002098\n",
            "4    fc1             4         -3.715524\n",
            "5    fc1             5          0.351278\n",
            "6    fc1             6         -3.363002\n",
            "7    fc1             7         -0.396980\n",
            "8    fc1             8         -0.837911\n",
            "9    fc1             9          8.992842\n",
            "10   fc1            10         -3.063069\n",
            "11   fc1            11         -2.914246\n",
            "12   fc1            12         12.763997\n",
            "13   fc1            13         23.921843\n",
            "14   fc1            14         -3.676337\n",
            "15   fc1            15         -8.636127\n",
            "16   fc1            16         -4.217728\n",
            "17   fc1            17         -3.324798\n",
            "18   fc1            18          3.175265\n",
            "19   fc1            19          2.451332\n",
            "20   fc1            20         16.815941\n",
            "21   fc1            21         -6.566652\n",
            "22   fc1            22         -5.960388\n",
            "23   fc1            23         10.530318\n",
            "24   fc1            24         -3.812729\n",
            "25   fc1            25         -2.918937\n",
            "26   fc1            26         -3.617950\n",
            "27   fc1            27         -1.806318\n",
            "28   fc1            28         -4.178484\n",
            "29   fc1            29         12.370386\n",
            "30   fc1            30         -3.555715\n",
            "31   fc1            31         -6.274562\n",
            "32   fc1            32         -5.149502\n",
            "33   fc1            33         12.329739\n",
            "34   fc1            34         -3.714554\n",
            "35   fc1            35         13.013494\n",
            "36   fc1            36         28.443253\n",
            "37   fc1            37         16.083315\n",
            "38   fc1            38          7.839757\n",
            "39   fc1            39         -8.498104\n",
            "40   fc1            40         -0.164591\n",
            "41   fc1            41         -3.347553\n",
            "42   fc1            42         -2.915431\n",
            "43   fc1            43         -2.848202\n",
            "44   fc1            44         -3.375820\n",
            "45   fc1            45         -3.642740\n",
            "46   fc1            46         -3.255165\n",
            "47   fc1            47         -2.305753\n",
            "48   fc1            48         -2.349495\n",
            "49   fc1            49         25.308172\n",
            "50   fc1            50          7.081500\n",
            "51   fc1            51         -1.738613\n",
            "52   fc1            52         15.152864\n",
            "53   fc1            53         -2.498630\n",
            "54   fc1            54         -2.135576\n",
            "55   fc1            55         -4.978029\n",
            "56   fc1            56         -2.456680\n",
            "57   fc1            57          3.209952\n",
            "58   fc1            58         -5.907986\n",
            "59   fc1            59         -3.898845\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.012868  0.012967 -0.004651  0.004214  0.004450 -0.004286 -0.006892   \n",
            "1  -0.006668  0.001739  0.008233 -0.015734 -0.018150  0.006245 -0.009190   \n",
            "2  -0.004715 -0.008510 -0.027038 -0.012099 -0.017072 -0.001276 -0.004790   \n",
            "3  -0.022022  0.006158 -0.000070  0.005327 -0.010999 -0.013339 -0.000802   \n",
            "4  -0.021747  0.005062 -0.007141 -0.002509  0.002286 -0.004356 -0.013751   \n",
            "5   0.000488  0.012703  0.014533  0.016341 -0.004459  0.019696  0.002125   \n",
            "6   0.009300 -0.022666 -0.019488  0.000888 -0.000055 -0.011650 -0.015693   \n",
            "7   0.031482  0.014408  0.003062  0.002343  0.027174  0.003222  0.007536   \n",
            "8  -0.021455 -0.018184  0.010642 -0.009969 -0.002964 -0.022868  0.002633   \n",
            "9   0.008664  0.005595  0.018707  0.036951  0.012074  0.024569  0.035212   \n",
            "10 -0.015157 -0.015687 -0.020529  0.011342  0.001704 -0.014001 -0.023534   \n",
            "11  0.033904  0.009318  0.023171  0.003949  0.003715  0.000735  0.023219   \n",
            "12  0.024346  0.032305  0.026967  0.025876  0.010165  0.022697  0.011940   \n",
            "13  0.008851  0.010857  0.002167  0.024771  0.005659  0.021011  0.000578   \n",
            "14 -0.009209 -0.003457 -0.008076 -0.003801  0.010076 -0.013077 -0.018169   \n",
            "15 -0.022892 -0.007363 -0.001998 -0.025108 -0.005332 -0.008478 -0.006340   \n",
            "16 -0.011208 -0.008056 -0.020657 -0.002330  0.002901 -0.009348 -0.019556   \n",
            "17 -0.004303 -0.011543 -0.022747 -0.002148 -0.000974  0.003379  0.001518   \n",
            "18  0.014590  0.013593  0.026992  0.014942  0.002147  0.024546  0.023784   \n",
            "19  0.013653  0.015041  0.026944  0.017540  0.012362  0.005006  0.020771   \n",
            "20  0.000926 -0.002540  0.000976 -0.007426  0.008134  0.009121  0.006932   \n",
            "21 -0.008481 -0.009680 -0.015762 -0.008213  0.005569 -0.011419 -0.018695   \n",
            "22  0.002687 -0.004754  0.025822  0.022829  0.026687  0.024950  0.015143   \n",
            "23  0.012675 -0.008788  0.020707 -0.002437 -0.000562 -0.001121  0.009253   \n",
            "24 -0.021873 -0.006459  0.000207 -0.022165  0.008027 -0.006639  0.011209   \n",
            "25 -0.009614 -0.022043 -0.007360  0.004037 -0.017339 -0.001730 -0.005209   \n",
            "26 -0.003378 -0.006826 -0.019353 -0.008899  0.007771  0.007703 -0.008105   \n",
            "27 -0.009939 -0.008868  0.000785  0.008521 -0.017249  0.009543 -0.006277   \n",
            "28 -0.012323  0.002773 -0.002832  0.008380 -0.001049 -0.023035 -0.010383   \n",
            "29  0.014561  0.016938  0.017683 -0.001824  0.010139  0.014729  0.012067   \n",
            "30 -0.019001 -0.009498 -0.020953 -0.004602  0.011816  0.005326 -0.018519   \n",
            "31  0.000243  0.000025  0.021961 -0.006239  0.018972 -0.004205  0.002377   \n",
            "32 -0.003725 -0.003355 -0.020392  0.005503 -0.013929 -0.013021  0.001984   \n",
            "33  0.016569  0.002955  0.006740  0.025918  0.010518  0.006316  0.009447   \n",
            "34 -0.021455 -0.010343  0.011155  0.004942 -0.004358 -0.012782 -0.010427   \n",
            "35 -0.004436 -0.022906 -0.004501  0.005599  0.007464 -0.000397 -0.000361   \n",
            "36  0.028495  0.008049  0.033686  0.027099  0.001744  0.033556  0.005032   \n",
            "37  0.017607  0.014472  0.003794  0.007778  0.026253  0.026272  0.000011   \n",
            "38 -0.002645  0.017745  0.021209 -0.002706  0.004354  0.021500  0.015463   \n",
            "39  0.013660  0.008610  0.018474  0.003934  0.033713  0.001638  0.016512   \n",
            "40  0.030918  0.000622  0.009956  0.019433  0.000461 -0.000907  0.013701   \n",
            "41 -0.014202 -0.010007 -0.013808 -0.019121  0.001727 -0.010824  0.006965   \n",
            "42 -0.018572 -0.000763  0.011354 -0.020072 -0.000430  0.000737 -0.016534   \n",
            "43 -0.016530 -0.011671 -0.013214 -0.002635  0.008195 -0.009454 -0.014988   \n",
            "44 -0.004162  0.004954 -0.016971 -0.016766 -0.005005 -0.002056 -0.009664   \n",
            "45 -0.006126  0.000294 -0.010289 -0.013374 -0.021398 -0.016979 -0.005077   \n",
            "46 -0.003019  0.003975 -0.013475  0.004994 -0.012704 -0.002466 -0.018315   \n",
            "47  0.002745  0.005883 -0.017230  0.000825 -0.021160 -0.008062 -0.022425   \n",
            "48 -0.000349 -0.019200  0.004244  0.012784  0.002108 -0.011295 -0.014164   \n",
            "49  0.019498  0.027400  0.033523  0.031643  0.003375  0.024539  0.018732   \n",
            "50  0.002034  0.025831  0.021195  0.015002  0.011998  0.006490  0.015592   \n",
            "51  0.002681 -0.004163 -0.002470  0.002025  0.000847 -0.004836 -0.020047   \n",
            "52  0.031499  0.006817  0.030368  0.016483  0.009225  0.011744  0.023579   \n",
            "53 -0.002231 -0.016763 -0.012501  0.002132 -0.016919  0.000812  0.009893   \n",
            "54 -0.016136  0.002641  0.006465  0.005333  0.000611 -0.021958  0.005562   \n",
            "55  0.006558  0.001421 -0.006817 -0.021912 -0.021949 -0.012012  0.008985   \n",
            "56 -0.013326  0.005539 -0.001117  0.010829  0.002428 -0.019694 -0.017891   \n",
            "57  0.000068  0.003597  0.006211  0.004677 -0.000560  0.023743  0.012332   \n",
            "58  0.028027  0.022394  0.015621  0.022584  0.027930  0.013303  0.023036   \n",
            "59 -0.016706  0.001549  0.002360 -0.005833 -0.013946 -0.005872 -0.015406   \n",
            "\n",
            "           7         8         9  ...      3128      3129      3130      3131  \\\n",
            "0   0.007324 -0.009616 -0.005067  ... -0.006856  0.010652  0.010737 -0.005627   \n",
            "1  -0.018485 -0.011243 -0.014907  ...  0.003302 -0.023178 -0.002841 -0.002705   \n",
            "2   0.003348 -0.021924  0.002252  ... -0.004555 -0.022496 -0.027291  0.000060   \n",
            "3  -0.006648 -0.017124  0.011142  ... -0.008438  0.007751 -0.003895 -0.014379   \n",
            "4   0.009772 -0.011616 -0.003061  ... -0.013893 -0.023509  0.007052  0.009660   \n",
            "5   0.008894  0.007604  0.022646  ...  0.023266  0.041666  0.029488  0.033174   \n",
            "6  -0.003789  0.009797 -0.006847  ...  0.009045 -0.017039 -0.017816  0.009705   \n",
            "7   0.004008  0.015966  0.020602  ...  0.009192  0.034439  0.014094  0.028504   \n",
            "8  -0.003198 -0.022776  0.003764  ... -0.001452 -0.006950 -0.022771  0.003933   \n",
            "9   0.030930  0.004664  0.003770  ...  0.004832 -0.009257 -0.008188  0.001435   \n",
            "10 -0.013386 -0.013077 -0.018894  ... -0.001648  0.000762 -0.017888 -0.016899   \n",
            "11  0.006491  0.032536  0.000740  ...  0.003196  0.004032  0.010177  0.012247   \n",
            "12  0.025155  0.015860  0.000769  ...  0.006513  0.015600  0.012484  0.016776   \n",
            "13  0.029623  0.026839 -0.001954  ...  0.036832  0.027887  0.015517  0.010580   \n",
            "14 -0.016337  0.004727  0.010331  ... -0.012719 -0.016295  0.002682 -0.014291   \n",
            "15 -0.017671 -0.001590 -0.007524  ... -0.027042 -0.015744 -0.009734 -0.021105   \n",
            "16 -0.015554  0.001033  0.005761  ...  0.009729 -0.022332 -0.021350 -0.016759   \n",
            "17  0.010020 -0.019688 -0.004400  ...  0.007721 -0.004402 -0.016819 -0.015915   \n",
            "18  0.002784 -0.006211  0.013223  ...  0.024819  0.008977  0.035165  0.001899   \n",
            "19  0.014684  0.027412  0.018771  ...  0.027893  0.023726  0.017920  0.016699   \n",
            "20 -0.019416 -0.022032 -0.013255  ...  0.047160  0.036355  0.025181  0.022490   \n",
            "21  0.001027 -0.005196 -0.004424  ... -0.011853 -0.003937  0.003567  0.000304   \n",
            "22  0.004678  0.017792 -0.000148  ...  0.014265  0.008728 -0.017063  0.005836   \n",
            "23  0.005949 -0.008079  0.000984  ... -0.013547 -0.005859  0.002466  0.003944   \n",
            "24 -0.012757 -0.023823 -0.018663  ...  0.011678 -0.019435  0.003086 -0.019769   \n",
            "25 -0.010399  0.010380 -0.019753  ...  0.000844  0.010552 -0.019871 -0.019154   \n",
            "26  0.010319 -0.010890  0.007982  ...  0.000887  0.000956  0.001107 -0.014635   \n",
            "27 -0.004731 -0.009723 -0.003905  ...  0.005012 -0.016505 -0.011187 -0.001365   \n",
            "28 -0.015085 -0.001597 -0.002212  ... -0.009379  0.010932  0.000835  0.011187   \n",
            "29  0.009418  0.025151  0.020776  ... -0.004506 -0.015641 -0.013120 -0.010891   \n",
            "30 -0.016916  0.009760 -0.014054  ...  0.007682 -0.003761 -0.015353 -0.000961   \n",
            "31  0.012016 -0.000764 -0.007299  ... -0.012403 -0.009862 -0.011390 -0.003945   \n",
            "32 -0.024166  0.000700  0.009655  ... -0.019903 -0.006628 -0.020596  0.001379   \n",
            "33  0.010933  0.003280  0.013948  ... -0.003015  0.020623 -0.000906  0.015280   \n",
            "34 -0.017208  0.006274  0.003570  ... -0.013828 -0.008093 -0.019012 -0.001721   \n",
            "35  0.005101 -0.010963 -0.009127  ... -0.002773  0.011193  0.019547  0.002818   \n",
            "36  0.012425  0.009884  0.031366  ...  0.042560  0.039935  0.023204  0.029524   \n",
            "37  0.023419  0.006894  0.026621  ...  0.004115  0.000018  0.020470 -0.002871   \n",
            "38  0.018896  0.024307 -0.000390  ... -0.004216 -0.012399 -0.001571 -0.018753   \n",
            "39  0.010963  0.030405  0.032920  ... -0.013158  0.001995 -0.017575  0.000686   \n",
            "40  0.026295  0.013387 -0.001529  ...  0.043029  0.038009  0.020855  0.017207   \n",
            "41 -0.004680 -0.013764 -0.007863  ...  0.008035  0.010515 -0.006477  0.011474   \n",
            "42 -0.000678  0.009945  0.003108  ...  0.008813  0.006511 -0.001635 -0.017173   \n",
            "43 -0.011972 -0.016392  0.001932  ... -0.010337 -0.012674 -0.018367 -0.022919   \n",
            "44  0.003259 -0.000585  0.001807  ...  0.008413 -0.014572 -0.008983  0.005895   \n",
            "45  0.006896 -0.008086 -0.009275  ... -0.005778  0.007000  0.002406 -0.008275   \n",
            "46  0.010720 -0.002166 -0.014401  ... -0.013205 -0.022235 -0.002017 -0.000171   \n",
            "47 -0.005961 -0.003954  0.010139  ...  0.011701  0.001282 -0.018512 -0.003118   \n",
            "48 -0.013545  0.006565 -0.005001  ... -0.010218  0.010282 -0.019655  0.010408   \n",
            "49  0.018647  0.026510  0.016471  ...  0.039909  0.049060  0.028912  0.045582   \n",
            "50  0.000300  0.010269  0.008490  ...  0.023184 -0.001352  0.019045  0.003499   \n",
            "51  0.004892  0.007555  0.001219  ... -0.016053  0.001421 -0.013407 -0.006220   \n",
            "52  0.030600  0.026217  0.023260  ...  0.017529  0.006934  0.017406  0.020949   \n",
            "53 -0.010024 -0.009774 -0.018796  ... -0.015735 -0.020443  0.008829 -0.016565   \n",
            "54  0.011039 -0.000971 -0.004749  ... -0.011213  0.000786 -0.019802 -0.001971   \n",
            "55 -0.017711 -0.011881  0.008669  ...  0.003574  0.005007 -0.005511 -0.009074   \n",
            "56 -0.018805 -0.010618  0.002681  ... -0.014544 -0.013959  0.007968 -0.004889   \n",
            "57  0.013836  0.000618  0.024202  ...  0.020163  0.034249  0.023702  0.032346   \n",
            "58  0.020271 -0.000923  0.004961  ...  0.030553  0.023524  0.025896  0.012225   \n",
            "59  0.006506 -0.000810  0.000023  ... -0.021418 -0.022299 -0.008617 -0.017246   \n",
            "\n",
            "        3132      3133      3134      3135  layer  neuron_index  \n",
            "0  -0.006300  0.000761 -0.010077 -0.010894    fc1             0  \n",
            "1   0.007173  0.003588 -0.009852 -0.016465    fc1             1  \n",
            "2   0.005837 -0.015450 -0.013901  0.002791    fc1             2  \n",
            "3  -0.012918  0.007607 -0.013114 -0.016117    fc1             3  \n",
            "4   0.003922 -0.010699  0.009847 -0.021692    fc1             4  \n",
            "5   0.028678  0.014802  0.011530  0.006059    fc1             5  \n",
            "6  -0.014932  0.004915  0.002246 -0.007906    fc1             6  \n",
            "7   0.013501  0.022247  0.021532 -0.003630    fc1             7  \n",
            "8  -0.006297 -0.015515 -0.013881  0.004368    fc1             8  \n",
            "9  -0.001923  0.008458  0.021580  0.029945    fc1             9  \n",
            "10 -0.000172 -0.004697 -0.007486  0.003054    fc1            10  \n",
            "11 -0.007124 -0.002595 -0.013320 -0.004145    fc1            11  \n",
            "12 -0.001142  0.025493  0.015860  0.012417    fc1            12  \n",
            "13  0.008905  0.014851  0.010457  0.007798    fc1            13  \n",
            "14 -0.010744 -0.008016 -0.019082 -0.007001    fc1            14  \n",
            "15 -0.008899 -0.000829 -0.016586 -0.028002    fc1            15  \n",
            "16  0.011897 -0.018047 -0.002446 -0.017222    fc1            16  \n",
            "17 -0.020536  0.006682 -0.007786 -0.019112    fc1            17  \n",
            "18  0.014848  0.023633  0.009814 -0.009907    fc1            18  \n",
            "19  0.032436  0.014681  0.007802  0.015888    fc1            19  \n",
            "20  0.010047  0.026064  0.016482  0.030144    fc1            20  \n",
            "21  0.008994  0.010006 -0.018727 -0.003075    fc1            21  \n",
            "22 -0.000216 -0.005094  0.018505 -0.008893    fc1            22  \n",
            "23  0.027010  0.004446 -0.004685  0.011521    fc1            23  \n",
            "24 -0.006039  0.005686 -0.007776  0.007923    fc1            24  \n",
            "25 -0.013028 -0.005355 -0.013795  0.000189    fc1            25  \n",
            "26  0.001260  0.002817 -0.000567 -0.018858    fc1            26  \n",
            "27 -0.012066  0.005803  0.008898 -0.020256    fc1            27  \n",
            "28 -0.021011 -0.016277 -0.021642 -0.017041    fc1            28  \n",
            "29  0.015582 -0.001358  0.009005  0.020085    fc1            29  \n",
            "30  0.009142 -0.011048 -0.019065 -0.022964    fc1            30  \n",
            "31 -0.017578  0.018184 -0.014494  0.006414    fc1            31  \n",
            "32 -0.007214 -0.020134 -0.002980 -0.009061    fc1            32  \n",
            "33 -0.002171 -0.004688  0.024596  0.012860    fc1            33  \n",
            "34  0.006539  0.009605 -0.015453 -0.016520    fc1            34  \n",
            "35 -0.018720 -0.022744 -0.017856  0.009409    fc1            35  \n",
            "36  0.040159  0.015496  0.046070  0.038217    fc1            36  \n",
            "37  0.016869  0.005396  0.012609  0.013487    fc1            37  \n",
            "38 -0.000505  0.023441  0.003526  0.005332    fc1            38  \n",
            "39 -0.019023 -0.006869 -0.015280  0.005979    fc1            39  \n",
            "40  0.027607  0.004566  0.021181  0.012148    fc1            40  \n",
            "41  0.004879 -0.012685 -0.009628 -0.022963    fc1            41  \n",
            "42  0.007932 -0.012811 -0.015902 -0.008890    fc1            42  \n",
            "43 -0.016014  0.007799  0.005429 -0.008904    fc1            43  \n",
            "44  0.004329  0.007101 -0.003699 -0.012319    fc1            44  \n",
            "45  0.005780  0.009909 -0.004622 -0.021927    fc1            45  \n",
            "46 -0.013934 -0.011455 -0.007828 -0.010529    fc1            46  \n",
            "47 -0.016764 -0.023845 -0.003573  0.011422    fc1            47  \n",
            "48 -0.015247 -0.011081 -0.016139 -0.007430    fc1            48  \n",
            "49  0.037967  0.039943  0.047666  0.032830    fc1            49  \n",
            "50  0.002823 -0.002692 -0.008353  0.002865    fc1            50  \n",
            "51 -0.014761  0.009049 -0.008702 -0.023180    fc1            51  \n",
            "52  0.023632  0.022728  0.018886  0.025511    fc1            52  \n",
            "53 -0.018722  0.011494  0.004309 -0.023839    fc1            53  \n",
            "54  0.000339  0.006231  0.007703 -0.013026    fc1            54  \n",
            "55 -0.014757 -0.008799 -0.020483 -0.005706    fc1            55  \n",
            "56 -0.014795  0.005220 -0.021989 -0.015774    fc1            56  \n",
            "57 -0.000363  0.006102 -0.002093 -0.001637    fc1            57  \n",
            "58  0.026156  0.011964  0.002414  0.006485    fc1            58  \n",
            "59 -0.003636  0.008666 -0.018831 -0.015422    fc1            59  \n",
            "\n",
            "[60 rows x 3138 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_ACTIVATIONS DataFrame:\n",
            "  layer  neuron_index  activation_value\n",
            "0   fc2             0         -8.774712\n",
            "1   fc2             1         15.383517\n",
            "2   fc2             2         -2.580471\n",
            "3   fc2             3         -7.362253\n",
            "4   fc2             4         -7.561530\n",
            "5   fc2             5         -9.346140\n",
            "6   fc2             6         -9.793673\n",
            "7   fc2             7         -4.358573\n",
            "8   fc2             8        -11.279164\n",
            "9   fc2             9          7.884292\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_CONNECTIONS DataFrame:\n",
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.107950 -0.100333 -0.125853  0.091329 -0.090712 -0.024337 -0.035301   \n",
            "1  0.131501 -0.110936 -0.085158  0.066024 -0.037845 -0.071343 -0.052721   \n",
            "2  0.050709 -0.033907 -0.077746 -0.074659 -0.076966 -0.074498 -0.088701   \n",
            "3  0.049182 -0.027318 -0.033456 -0.035291 -0.043763 -0.003068  0.013614   \n",
            "4  0.042645  0.088435  0.038246  0.004014 -0.031166 -0.035241 -0.033445   \n",
            "5  0.086447 -0.126483 -0.040570  0.021679  0.066402 -0.114365  0.069705   \n",
            "6 -0.080449  0.085518 -0.090617 -0.053715 -0.023148 -0.005910  0.053310   \n",
            "7  0.092976 -0.108292 -0.003311 -0.020869  0.013422 -0.048318  0.010532   \n",
            "8 -0.123973 -0.038942  0.098014 -0.079174  0.093177  0.066027  0.115322   \n",
            "9  0.017517 -0.083782  0.037875 -0.062554 -0.064439  0.184223  0.006078   \n",
            "\n",
            "          7         8         9  ...        52        53        54        55  \\\n",
            "0  0.090453 -0.134711  0.053470  ... -0.070573  0.083195  0.095116  0.083355   \n",
            "1 -0.112283  0.045542  0.064597  ...  0.047196  0.127483  0.055454  0.038554   \n",
            "2  0.057485 -0.012508 -0.048672  ...  0.098187  0.072752 -0.079741 -0.097680   \n",
            "3 -0.134513 -0.108175  0.041074  ... -0.005931 -0.089947 -0.010503  0.077362   \n",
            "4 -0.040576  0.073950 -0.058569  ... -0.079893 -0.038126 -0.095315  0.112316   \n",
            "5 -0.126576 -0.152112 -0.069325  ... -0.028109  0.103588 -0.003243  0.043243   \n",
            "6  0.102617  0.096054  0.003210  ... -0.036933  0.010865  0.066728  0.112904   \n",
            "7  0.093271  0.076127 -0.004166  ... -0.105497  0.043385  0.102613  0.121523   \n",
            "8  0.016770 -0.150593  0.057095  ... -0.014134 -0.041150  0.043610 -0.033362   \n",
            "9  0.090238 -0.031772 -0.016623  ... -0.092037  0.067543 -0.015439  0.107121   \n",
            "\n",
            "         56        57        58        59  layer  neuron_index  \n",
            "0 -0.068317  0.057229 -0.010008 -0.046776    fc2             0  \n",
            "1  0.154287 -0.029886 -0.088726  0.017459    fc2             1  \n",
            "2 -0.002260 -0.142917  0.077585  0.014963    fc2             2  \n",
            "3 -0.088103 -0.110321  0.010353  0.046034    fc2             3  \n",
            "4  0.099081  0.096792  0.056523 -0.063165    fc2             4  \n",
            "5 -0.133819 -0.110977 -0.123005 -0.015103    fc2             5  \n",
            "6  0.097748 -0.037395  0.050451 -0.001918    fc2             6  \n",
            "7 -0.045865 -0.148744  0.003459 -0.012868    fc2             7  \n",
            "8 -0.111366 -0.025910  0.000013  0.098205    fc2             8  \n",
            "9 -0.079098  0.062496 -0.018937  0.063364    fc2             9  \n",
            "\n",
            "[10 rows x 62 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Image 6 ---\n",
            "True Label: 9\n",
            "Predicted Label: 9\n",
            "\n",
            "CONV1_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv1             0          0.337079\n",
            "1   conv1             1          0.328221\n",
            "2   conv1             2          0.145624\n",
            "3   conv1             3          0.080651\n",
            "4   conv1             4          0.141997\n",
            "5   conv1             5          0.253881\n",
            "6   conv1             6          0.027553\n",
            "7   conv1             7          0.360474\n",
            "8   conv1             8          0.163721\n",
            "9   conv1             9          0.162234\n",
            "10  conv1            10          0.142967\n",
            "11  conv1            11          0.239367\n",
            "12  conv1            12          0.012475\n",
            "13  conv1            13          0.134526\n",
            "14  conv1            14         -0.013842\n",
            "15  conv1            15          0.031238\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.400412  0.399371  0.071231 -0.067315  0.237489  0.328836 -0.150906   \n",
            "1   0.062670  0.420839  0.423627 -0.087857  0.270959 -0.074010  0.234360   \n",
            "2   0.077322  0.239514  0.362599  0.313162 -0.159267 -0.146957 -0.197186   \n",
            "3  -0.198520  0.416571  0.189112 -0.133381  0.282581  0.187163 -0.016161   \n",
            "4  -0.100635 -0.007484  0.208964  0.262549  0.069296  0.175801  0.311726   \n",
            "5  -0.273976 -0.003054  0.224042  0.069278  0.076746  0.157825  0.075656   \n",
            "6  -0.428757 -0.251183 -0.254338  0.081380  0.144253  0.244236  0.337603   \n",
            "7   0.378913  0.399986  0.071193  0.172418 -0.026089  0.130204 -0.220100   \n",
            "8   0.225943  0.192161  0.387705  0.135396 -0.048392 -0.038235 -0.115391   \n",
            "9   0.307195 -0.180329 -0.171382  0.319853  0.081811  0.181218  0.174932   \n",
            "10  0.130797  0.370466  0.380331  0.015307 -0.078076  0.016426 -0.083861   \n",
            "11  0.392175  0.247163  0.281231  0.259225 -0.270856 -0.165327  0.253949   \n",
            "12 -0.210740 -0.048877 -0.131766  0.335698  0.006075 -0.116493 -0.017947   \n",
            "13  0.119667  0.026561  0.093274  0.343750  0.204018 -0.236069  0.372014   \n",
            "14  0.198226 -0.245182  0.121569 -0.049056 -0.133100 -0.077453  0.442164   \n",
            "15  0.209107  0.173020 -0.301732  0.125146 -0.021763 -0.264287  0.322271   \n",
            "\n",
            "           7         8  layer  neuron_index  \n",
            "0   0.392861 -0.045586  conv1             0  \n",
            "1  -0.093755  0.006661  conv1             1  \n",
            "2   0.068652  0.012842  conv1             2  \n",
            "3  -0.195803  0.062113  conv1             3  \n",
            "4  -0.130634  0.326892  conv1             4  \n",
            "5   0.073296 -0.115009  conv1             5  \n",
            "6  -0.101311  0.121107  conv1             6  \n",
            "7   0.377975 -0.272754  conv1             7  \n",
            "8   0.228945  0.188063  conv1             8  \n",
            "9   0.345546  0.172957  conv1             9  \n",
            "10  0.056517  0.259929  conv1            10  \n",
            "11 -0.198929  0.022492  conv1            11  \n",
            "12  0.285077  0.004282  conv1            12  \n",
            "13  0.204587 -0.130169  conv1            13  \n",
            "14 -0.012433 -0.308704  conv1            14  \n",
            "15 -0.069897  0.135285  conv1            15  \n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv2             0          0.225892\n",
            "1   conv2             1          0.802677\n",
            "2   conv2             2          0.370411\n",
            "3   conv2             3          0.950819\n",
            "4   conv2             4         -0.267105\n",
            "5   conv2             5         -0.286952\n",
            "6   conv2             6          0.560407\n",
            "7   conv2             7          0.405050\n",
            "8   conv2             8          0.244128\n",
            "9   conv2             9          0.076333\n",
            "10  conv2            10          0.868799\n",
            "11  conv2            11          0.351540\n",
            "12  conv2            12          0.484217\n",
            "13  conv2            13         -0.004871\n",
            "14  conv2            14          0.554828\n",
            "15  conv2            15          0.337243\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.053389  0.023715 -0.082655 -0.048249 -0.076035 -0.150770  0.021789   \n",
            "1   0.047589 -0.093007  0.005160  0.069936  0.136776  0.046387  0.010860   \n",
            "2   0.058461  0.100847 -0.038368  0.022629  0.103998 -0.064163 -0.030390   \n",
            "3  -0.035365  0.023469  0.107092  0.101842  0.143467  0.217515 -0.012316   \n",
            "4   0.062249 -0.131436 -0.046978  0.021597 -0.080085 -0.078712 -0.103583   \n",
            "5  -0.019309 -0.080785  0.035847  0.020273  0.009665  0.035786 -0.006190   \n",
            "6   0.049547  0.038741 -0.036449 -0.002058  0.041931  0.053819 -0.005733   \n",
            "7  -0.152914 -0.041600  0.115807 -0.041148 -0.039541  0.071635 -0.011336   \n",
            "8   0.125563  0.033356 -0.048423  0.110720 -0.018201 -0.147717 -0.042072   \n",
            "9  -0.068213 -0.115001 -0.004743 -0.028494 -0.027653  0.079632  0.049930   \n",
            "10 -0.032338 -0.013617  0.151200 -0.075678  0.075957  0.207855  0.038870   \n",
            "11  0.051104  0.012186  0.086417  0.028352  0.156211 -0.002386  0.086014   \n",
            "12  0.082138  0.005539 -0.044524  0.132945  0.106715  0.022446  0.116672   \n",
            "13 -0.043048 -0.140184  0.012286 -0.060239 -0.119047  0.023106 -0.004832   \n",
            "14 -0.021613  0.105474  0.025686  0.030302  0.136904 -0.020385 -0.046524   \n",
            "15  0.117869  0.126556  0.051337  0.002796 -0.029835 -0.061113  0.004300   \n",
            "\n",
            "           7         8         9  ...       136       137       138       139  \\\n",
            "0   0.087136  0.136641  0.125749  ...  0.105212  0.008460  0.086248 -0.005925   \n",
            "1   0.068729 -0.004232 -0.004889  ...  0.075859  0.051621  0.013238  0.045813   \n",
            "2  -0.050265  0.004671  0.000319  ...  0.094657  0.007707  0.022765  0.004527   \n",
            "3   0.084479  0.127317 -0.104037  ...  0.000802  0.089539 -0.032844  0.050453   \n",
            "4   0.048199  0.034387 -0.074277  ...  0.045809 -0.082850  0.027167 -0.046521   \n",
            "5  -0.040837 -0.020941  0.074329  ... -0.027615 -0.036336 -0.055786  0.008235   \n",
            "6  -0.068649 -0.066126 -0.024237  ...  0.095926  0.090875 -0.025141  0.018905   \n",
            "7   0.034141  0.078109 -0.036853  ...  0.086062 -0.007312  0.005437  0.073091   \n",
            "8   0.026514 -0.127686 -0.001961  ...  0.017386 -0.016188  0.018640  0.010524   \n",
            "9  -0.042845  0.092674  0.078472  ... -0.005325  0.106815  0.014020 -0.074019   \n",
            "10 -0.053325  0.046839  0.006819  ...  0.041650  0.170038 -0.044293  0.004112   \n",
            "11  0.042763 -0.056389 -0.043120  ...  0.042914 -0.032490  0.009025  0.004950   \n",
            "12  0.132660 -0.116526 -0.137380  ... -0.008512 -0.055690  0.034679  0.085180   \n",
            "13 -0.028790  0.080668 -0.064381  ... -0.020423  0.075944 -0.070604 -0.053503   \n",
            "14  0.006731 -0.044295  0.048296  ... -0.026215  0.104971  0.072772  0.041096   \n",
            "15  0.013864 -0.005772  0.182691  ... -0.027957 -0.073938 -0.020258 -0.038572   \n",
            "\n",
            "         140       141       142       143  layer  neuron_index  \n",
            "0  -0.033853  0.113257 -0.040305  0.088088  conv2             0  \n",
            "1   0.091032  0.005281 -0.026384  0.078543  conv2             1  \n",
            "2   0.002996  0.085109  0.008654  0.067731  conv2             2  \n",
            "3  -0.007134  0.030011 -0.008236 -0.030027  conv2             3  \n",
            "4   0.067527 -0.041539  0.057837  0.032542  conv2             4  \n",
            "5  -0.075360 -0.063744  0.026530  0.010399  conv2             5  \n",
            "6   0.031970 -0.055053  0.091992 -0.061894  conv2             6  \n",
            "7   0.030808 -0.011768  0.093748  0.089395  conv2             7  \n",
            "8  -0.089284  0.094356  0.033373  0.012280  conv2             8  \n",
            "9   0.011533 -0.078723  0.016492 -0.013510  conv2             9  \n",
            "10  0.017894 -0.050105  0.031401  0.045740  conv2            10  \n",
            "11  0.000354  0.074826  0.052800  0.083592  conv2            11  \n",
            "12  0.014633  0.005773  0.066127 -0.082251  conv2            12  \n",
            "13  0.020353 -0.001231  0.079536  0.032849  conv2            13  \n",
            "14  0.005841  0.030748 -0.013547 -0.054549  conv2            14  \n",
            "15 -0.075074  0.053019 -0.002690 -0.082208  conv2            15  \n",
            "\n",
            "[16 rows x 146 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_ACTIVATIONS DataFrame:\n",
            "   layer  neuron_index  activation_value\n",
            "0    fc1             0         -3.308735\n",
            "1    fc1             1         -3.860808\n",
            "2    fc1             2        -10.484216\n",
            "3    fc1             3         -2.858599\n",
            "4    fc1             4         -5.364908\n",
            "5    fc1             5         19.151150\n",
            "6    fc1             6         -4.886180\n",
            "7    fc1             7         12.738941\n",
            "8    fc1             8         -0.909420\n",
            "9    fc1             9          8.706751\n",
            "10   fc1            10         -4.537073\n",
            "11   fc1            11         -9.005508\n",
            "12   fc1            12         11.449482\n",
            "13   fc1            13         23.018011\n",
            "14   fc1            14         -4.327274\n",
            "15   fc1            15        -12.344006\n",
            "16   fc1            16         -5.788860\n",
            "17   fc1            17         -4.578452\n",
            "18   fc1            18          3.419636\n",
            "19   fc1            19         18.545233\n",
            "20   fc1            20         19.362814\n",
            "21   fc1            21         -8.998622\n",
            "22   fc1            22        -19.739464\n",
            "23   fc1            23          5.109560\n",
            "24   fc1            24         -5.733384\n",
            "25   fc1            25         -3.420836\n",
            "26   fc1            26         -5.183217\n",
            "27   fc1            27         -3.311488\n",
            "28   fc1            28         -5.317863\n",
            "29   fc1            29         -2.061681\n",
            "30   fc1            30         -5.997660\n",
            "31   fc1            31        -20.271120\n",
            "32   fc1            32         -7.469486\n",
            "33   fc1            33          2.616146\n",
            "34   fc1            34         -5.088727\n",
            "35   fc1            35         16.788540\n",
            "36   fc1            36         32.586590\n",
            "37   fc1            37          5.667165\n",
            "38   fc1            38        -10.130164\n",
            "39   fc1            39        -21.218195\n",
            "40   fc1            40         19.099396\n",
            "41   fc1            41         -3.990417\n",
            "42   fc1            42         -4.406953\n",
            "43   fc1            43         -4.216154\n",
            "44   fc1            44         -5.076197\n",
            "45   fc1            45         -5.573945\n",
            "46   fc1            46         -3.802148\n",
            "47   fc1            47         -3.290465\n",
            "48   fc1            48         -3.661507\n",
            "49   fc1            49         40.469494\n",
            "50   fc1            50          8.451682\n",
            "51   fc1            51         -3.060675\n",
            "52   fc1            52         19.801813\n",
            "53   fc1            53         -3.888230\n",
            "54   fc1            54         -3.619132\n",
            "55   fc1            55         -6.421734\n",
            "56   fc1            56         -2.795418\n",
            "57   fc1            57          4.513072\n",
            "58   fc1            58          6.075258\n",
            "59   fc1            59         -5.306196\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.012868  0.012967 -0.004651  0.004214  0.004450 -0.004286 -0.006892   \n",
            "1  -0.006668  0.001739  0.008233 -0.015734 -0.018150  0.006245 -0.009190   \n",
            "2  -0.004715 -0.008510 -0.027038 -0.012099 -0.017072 -0.001276 -0.004790   \n",
            "3  -0.022022  0.006158 -0.000070  0.005327 -0.010999 -0.013339 -0.000802   \n",
            "4  -0.021747  0.005062 -0.007141 -0.002509  0.002286 -0.004356 -0.013751   \n",
            "5   0.000488  0.012703  0.014533  0.016341 -0.004459  0.019696  0.002125   \n",
            "6   0.009300 -0.022666 -0.019488  0.000888 -0.000055 -0.011650 -0.015693   \n",
            "7   0.031482  0.014408  0.003062  0.002343  0.027174  0.003222  0.007536   \n",
            "8  -0.021455 -0.018184  0.010642 -0.009969 -0.002964 -0.022868  0.002633   \n",
            "9   0.008664  0.005595  0.018707  0.036951  0.012074  0.024569  0.035212   \n",
            "10 -0.015157 -0.015687 -0.020529  0.011342  0.001704 -0.014001 -0.023534   \n",
            "11  0.033904  0.009318  0.023171  0.003949  0.003715  0.000735  0.023219   \n",
            "12  0.024346  0.032305  0.026967  0.025876  0.010165  0.022697  0.011940   \n",
            "13  0.008851  0.010857  0.002167  0.024771  0.005659  0.021011  0.000578   \n",
            "14 -0.009209 -0.003457 -0.008076 -0.003801  0.010076 -0.013077 -0.018169   \n",
            "15 -0.022892 -0.007363 -0.001998 -0.025108 -0.005332 -0.008478 -0.006340   \n",
            "16 -0.011208 -0.008056 -0.020657 -0.002330  0.002901 -0.009348 -0.019556   \n",
            "17 -0.004303 -0.011543 -0.022747 -0.002148 -0.000974  0.003379  0.001518   \n",
            "18  0.014590  0.013593  0.026992  0.014942  0.002147  0.024546  0.023784   \n",
            "19  0.013653  0.015041  0.026944  0.017540  0.012362  0.005006  0.020771   \n",
            "20  0.000926 -0.002540  0.000976 -0.007426  0.008134  0.009121  0.006932   \n",
            "21 -0.008481 -0.009680 -0.015762 -0.008213  0.005569 -0.011419 -0.018695   \n",
            "22  0.002687 -0.004754  0.025822  0.022829  0.026687  0.024950  0.015143   \n",
            "23  0.012675 -0.008788  0.020707 -0.002437 -0.000562 -0.001121  0.009253   \n",
            "24 -0.021873 -0.006459  0.000207 -0.022165  0.008027 -0.006639  0.011209   \n",
            "25 -0.009614 -0.022043 -0.007360  0.004037 -0.017339 -0.001730 -0.005209   \n",
            "26 -0.003378 -0.006826 -0.019353 -0.008899  0.007771  0.007703 -0.008105   \n",
            "27 -0.009939 -0.008868  0.000785  0.008521 -0.017249  0.009543 -0.006277   \n",
            "28 -0.012323  0.002773 -0.002832  0.008380 -0.001049 -0.023035 -0.010383   \n",
            "29  0.014561  0.016938  0.017683 -0.001824  0.010139  0.014729  0.012067   \n",
            "30 -0.019001 -0.009498 -0.020953 -0.004602  0.011816  0.005326 -0.018519   \n",
            "31  0.000243  0.000025  0.021961 -0.006239  0.018972 -0.004205  0.002377   \n",
            "32 -0.003725 -0.003355 -0.020392  0.005503 -0.013929 -0.013021  0.001984   \n",
            "33  0.016569  0.002955  0.006740  0.025918  0.010518  0.006316  0.009447   \n",
            "34 -0.021455 -0.010343  0.011155  0.004942 -0.004358 -0.012782 -0.010427   \n",
            "35 -0.004436 -0.022906 -0.004501  0.005599  0.007464 -0.000397 -0.000361   \n",
            "36  0.028495  0.008049  0.033686  0.027099  0.001744  0.033556  0.005032   \n",
            "37  0.017607  0.014472  0.003794  0.007778  0.026253  0.026272  0.000011   \n",
            "38 -0.002645  0.017745  0.021209 -0.002706  0.004354  0.021500  0.015463   \n",
            "39  0.013660  0.008610  0.018474  0.003934  0.033713  0.001638  0.016512   \n",
            "40  0.030918  0.000622  0.009956  0.019433  0.000461 -0.000907  0.013701   \n",
            "41 -0.014202 -0.010007 -0.013808 -0.019121  0.001727 -0.010824  0.006965   \n",
            "42 -0.018572 -0.000763  0.011354 -0.020072 -0.000430  0.000737 -0.016534   \n",
            "43 -0.016530 -0.011671 -0.013214 -0.002635  0.008195 -0.009454 -0.014988   \n",
            "44 -0.004162  0.004954 -0.016971 -0.016766 -0.005005 -0.002056 -0.009664   \n",
            "45 -0.006126  0.000294 -0.010289 -0.013374 -0.021398 -0.016979 -0.005077   \n",
            "46 -0.003019  0.003975 -0.013475  0.004994 -0.012704 -0.002466 -0.018315   \n",
            "47  0.002745  0.005883 -0.017230  0.000825 -0.021160 -0.008062 -0.022425   \n",
            "48 -0.000349 -0.019200  0.004244  0.012784  0.002108 -0.011295 -0.014164   \n",
            "49  0.019498  0.027400  0.033523  0.031643  0.003375  0.024539  0.018732   \n",
            "50  0.002034  0.025831  0.021195  0.015002  0.011998  0.006490  0.015592   \n",
            "51  0.002681 -0.004163 -0.002470  0.002025  0.000847 -0.004836 -0.020047   \n",
            "52  0.031499  0.006817  0.030368  0.016483  0.009225  0.011744  0.023579   \n",
            "53 -0.002231 -0.016763 -0.012501  0.002132 -0.016919  0.000812  0.009893   \n",
            "54 -0.016136  0.002641  0.006465  0.005333  0.000611 -0.021958  0.005562   \n",
            "55  0.006558  0.001421 -0.006817 -0.021912 -0.021949 -0.012012  0.008985   \n",
            "56 -0.013326  0.005539 -0.001117  0.010829  0.002428 -0.019694 -0.017891   \n",
            "57  0.000068  0.003597  0.006211  0.004677 -0.000560  0.023743  0.012332   \n",
            "58  0.028027  0.022394  0.015621  0.022584  0.027930  0.013303  0.023036   \n",
            "59 -0.016706  0.001549  0.002360 -0.005833 -0.013946 -0.005872 -0.015406   \n",
            "\n",
            "           7         8         9  ...      3128      3129      3130      3131  \\\n",
            "0   0.007324 -0.009616 -0.005067  ... -0.006856  0.010652  0.010737 -0.005627   \n",
            "1  -0.018485 -0.011243 -0.014907  ...  0.003302 -0.023178 -0.002841 -0.002705   \n",
            "2   0.003348 -0.021924  0.002252  ... -0.004555 -0.022496 -0.027291  0.000060   \n",
            "3  -0.006648 -0.017124  0.011142  ... -0.008438  0.007751 -0.003895 -0.014379   \n",
            "4   0.009772 -0.011616 -0.003061  ... -0.013893 -0.023509  0.007052  0.009660   \n",
            "5   0.008894  0.007604  0.022646  ...  0.023266  0.041666  0.029488  0.033174   \n",
            "6  -0.003789  0.009797 -0.006847  ...  0.009045 -0.017039 -0.017816  0.009705   \n",
            "7   0.004008  0.015966  0.020602  ...  0.009192  0.034439  0.014094  0.028504   \n",
            "8  -0.003198 -0.022776  0.003764  ... -0.001452 -0.006950 -0.022771  0.003933   \n",
            "9   0.030930  0.004664  0.003770  ...  0.004832 -0.009257 -0.008188  0.001435   \n",
            "10 -0.013386 -0.013077 -0.018894  ... -0.001648  0.000762 -0.017888 -0.016899   \n",
            "11  0.006491  0.032536  0.000740  ...  0.003196  0.004032  0.010177  0.012247   \n",
            "12  0.025155  0.015860  0.000769  ...  0.006513  0.015600  0.012484  0.016776   \n",
            "13  0.029623  0.026839 -0.001954  ...  0.036832  0.027887  0.015517  0.010580   \n",
            "14 -0.016337  0.004727  0.010331  ... -0.012719 -0.016295  0.002682 -0.014291   \n",
            "15 -0.017671 -0.001590 -0.007524  ... -0.027042 -0.015744 -0.009734 -0.021105   \n",
            "16 -0.015554  0.001033  0.005761  ...  0.009729 -0.022332 -0.021350 -0.016759   \n",
            "17  0.010020 -0.019688 -0.004400  ...  0.007721 -0.004402 -0.016819 -0.015915   \n",
            "18  0.002784 -0.006211  0.013223  ...  0.024819  0.008977  0.035165  0.001899   \n",
            "19  0.014684  0.027412  0.018771  ...  0.027893  0.023726  0.017920  0.016699   \n",
            "20 -0.019416 -0.022032 -0.013255  ...  0.047160  0.036355  0.025181  0.022490   \n",
            "21  0.001027 -0.005196 -0.004424  ... -0.011853 -0.003937  0.003567  0.000304   \n",
            "22  0.004678  0.017792 -0.000148  ...  0.014265  0.008728 -0.017063  0.005836   \n",
            "23  0.005949 -0.008079  0.000984  ... -0.013547 -0.005859  0.002466  0.003944   \n",
            "24 -0.012757 -0.023823 -0.018663  ...  0.011678 -0.019435  0.003086 -0.019769   \n",
            "25 -0.010399  0.010380 -0.019753  ...  0.000844  0.010552 -0.019871 -0.019154   \n",
            "26  0.010319 -0.010890  0.007982  ...  0.000887  0.000956  0.001107 -0.014635   \n",
            "27 -0.004731 -0.009723 -0.003905  ...  0.005012 -0.016505 -0.011187 -0.001365   \n",
            "28 -0.015085 -0.001597 -0.002212  ... -0.009379  0.010932  0.000835  0.011187   \n",
            "29  0.009418  0.025151  0.020776  ... -0.004506 -0.015641 -0.013120 -0.010891   \n",
            "30 -0.016916  0.009760 -0.014054  ...  0.007682 -0.003761 -0.015353 -0.000961   \n",
            "31  0.012016 -0.000764 -0.007299  ... -0.012403 -0.009862 -0.011390 -0.003945   \n",
            "32 -0.024166  0.000700  0.009655  ... -0.019903 -0.006628 -0.020596  0.001379   \n",
            "33  0.010933  0.003280  0.013948  ... -0.003015  0.020623 -0.000906  0.015280   \n",
            "34 -0.017208  0.006274  0.003570  ... -0.013828 -0.008093 -0.019012 -0.001721   \n",
            "35  0.005101 -0.010963 -0.009127  ... -0.002773  0.011193  0.019547  0.002818   \n",
            "36  0.012425  0.009884  0.031366  ...  0.042560  0.039935  0.023204  0.029524   \n",
            "37  0.023419  0.006894  0.026621  ...  0.004115  0.000018  0.020470 -0.002871   \n",
            "38  0.018896  0.024307 -0.000390  ... -0.004216 -0.012399 -0.001571 -0.018753   \n",
            "39  0.010963  0.030405  0.032920  ... -0.013158  0.001995 -0.017575  0.000686   \n",
            "40  0.026295  0.013387 -0.001529  ...  0.043029  0.038009  0.020855  0.017207   \n",
            "41 -0.004680 -0.013764 -0.007863  ...  0.008035  0.010515 -0.006477  0.011474   \n",
            "42 -0.000678  0.009945  0.003108  ...  0.008813  0.006511 -0.001635 -0.017173   \n",
            "43 -0.011972 -0.016392  0.001932  ... -0.010337 -0.012674 -0.018367 -0.022919   \n",
            "44  0.003259 -0.000585  0.001807  ...  0.008413 -0.014572 -0.008983  0.005895   \n",
            "45  0.006896 -0.008086 -0.009275  ... -0.005778  0.007000  0.002406 -0.008275   \n",
            "46  0.010720 -0.002166 -0.014401  ... -0.013205 -0.022235 -0.002017 -0.000171   \n",
            "47 -0.005961 -0.003954  0.010139  ...  0.011701  0.001282 -0.018512 -0.003118   \n",
            "48 -0.013545  0.006565 -0.005001  ... -0.010218  0.010282 -0.019655  0.010408   \n",
            "49  0.018647  0.026510  0.016471  ...  0.039909  0.049060  0.028912  0.045582   \n",
            "50  0.000300  0.010269  0.008490  ...  0.023184 -0.001352  0.019045  0.003499   \n",
            "51  0.004892  0.007555  0.001219  ... -0.016053  0.001421 -0.013407 -0.006220   \n",
            "52  0.030600  0.026217  0.023260  ...  0.017529  0.006934  0.017406  0.020949   \n",
            "53 -0.010024 -0.009774 -0.018796  ... -0.015735 -0.020443  0.008829 -0.016565   \n",
            "54  0.011039 -0.000971 -0.004749  ... -0.011213  0.000786 -0.019802 -0.001971   \n",
            "55 -0.017711 -0.011881  0.008669  ...  0.003574  0.005007 -0.005511 -0.009074   \n",
            "56 -0.018805 -0.010618  0.002681  ... -0.014544 -0.013959  0.007968 -0.004889   \n",
            "57  0.013836  0.000618  0.024202  ...  0.020163  0.034249  0.023702  0.032346   \n",
            "58  0.020271 -0.000923  0.004961  ...  0.030553  0.023524  0.025896  0.012225   \n",
            "59  0.006506 -0.000810  0.000023  ... -0.021418 -0.022299 -0.008617 -0.017246   \n",
            "\n",
            "        3132      3133      3134      3135  layer  neuron_index  \n",
            "0  -0.006300  0.000761 -0.010077 -0.010894    fc1             0  \n",
            "1   0.007173  0.003588 -0.009852 -0.016465    fc1             1  \n",
            "2   0.005837 -0.015450 -0.013901  0.002791    fc1             2  \n",
            "3  -0.012918  0.007607 -0.013114 -0.016117    fc1             3  \n",
            "4   0.003922 -0.010699  0.009847 -0.021692    fc1             4  \n",
            "5   0.028678  0.014802  0.011530  0.006059    fc1             5  \n",
            "6  -0.014932  0.004915  0.002246 -0.007906    fc1             6  \n",
            "7   0.013501  0.022247  0.021532 -0.003630    fc1             7  \n",
            "8  -0.006297 -0.015515 -0.013881  0.004368    fc1             8  \n",
            "9  -0.001923  0.008458  0.021580  0.029945    fc1             9  \n",
            "10 -0.000172 -0.004697 -0.007486  0.003054    fc1            10  \n",
            "11 -0.007124 -0.002595 -0.013320 -0.004145    fc1            11  \n",
            "12 -0.001142  0.025493  0.015860  0.012417    fc1            12  \n",
            "13  0.008905  0.014851  0.010457  0.007798    fc1            13  \n",
            "14 -0.010744 -0.008016 -0.019082 -0.007001    fc1            14  \n",
            "15 -0.008899 -0.000829 -0.016586 -0.028002    fc1            15  \n",
            "16  0.011897 -0.018047 -0.002446 -0.017222    fc1            16  \n",
            "17 -0.020536  0.006682 -0.007786 -0.019112    fc1            17  \n",
            "18  0.014848  0.023633  0.009814 -0.009907    fc1            18  \n",
            "19  0.032436  0.014681  0.007802  0.015888    fc1            19  \n",
            "20  0.010047  0.026064  0.016482  0.030144    fc1            20  \n",
            "21  0.008994  0.010006 -0.018727 -0.003075    fc1            21  \n",
            "22 -0.000216 -0.005094  0.018505 -0.008893    fc1            22  \n",
            "23  0.027010  0.004446 -0.004685  0.011521    fc1            23  \n",
            "24 -0.006039  0.005686 -0.007776  0.007923    fc1            24  \n",
            "25 -0.013028 -0.005355 -0.013795  0.000189    fc1            25  \n",
            "26  0.001260  0.002817 -0.000567 -0.018858    fc1            26  \n",
            "27 -0.012066  0.005803  0.008898 -0.020256    fc1            27  \n",
            "28 -0.021011 -0.016277 -0.021642 -0.017041    fc1            28  \n",
            "29  0.015582 -0.001358  0.009005  0.020085    fc1            29  \n",
            "30  0.009142 -0.011048 -0.019065 -0.022964    fc1            30  \n",
            "31 -0.017578  0.018184 -0.014494  0.006414    fc1            31  \n",
            "32 -0.007214 -0.020134 -0.002980 -0.009061    fc1            32  \n",
            "33 -0.002171 -0.004688  0.024596  0.012860    fc1            33  \n",
            "34  0.006539  0.009605 -0.015453 -0.016520    fc1            34  \n",
            "35 -0.018720 -0.022744 -0.017856  0.009409    fc1            35  \n",
            "36  0.040159  0.015496  0.046070  0.038217    fc1            36  \n",
            "37  0.016869  0.005396  0.012609  0.013487    fc1            37  \n",
            "38 -0.000505  0.023441  0.003526  0.005332    fc1            38  \n",
            "39 -0.019023 -0.006869 -0.015280  0.005979    fc1            39  \n",
            "40  0.027607  0.004566  0.021181  0.012148    fc1            40  \n",
            "41  0.004879 -0.012685 -0.009628 -0.022963    fc1            41  \n",
            "42  0.007932 -0.012811 -0.015902 -0.008890    fc1            42  \n",
            "43 -0.016014  0.007799  0.005429 -0.008904    fc1            43  \n",
            "44  0.004329  0.007101 -0.003699 -0.012319    fc1            44  \n",
            "45  0.005780  0.009909 -0.004622 -0.021927    fc1            45  \n",
            "46 -0.013934 -0.011455 -0.007828 -0.010529    fc1            46  \n",
            "47 -0.016764 -0.023845 -0.003573  0.011422    fc1            47  \n",
            "48 -0.015247 -0.011081 -0.016139 -0.007430    fc1            48  \n",
            "49  0.037967  0.039943  0.047666  0.032830    fc1            49  \n",
            "50  0.002823 -0.002692 -0.008353  0.002865    fc1            50  \n",
            "51 -0.014761  0.009049 -0.008702 -0.023180    fc1            51  \n",
            "52  0.023632  0.022728  0.018886  0.025511    fc1            52  \n",
            "53 -0.018722  0.011494  0.004309 -0.023839    fc1            53  \n",
            "54  0.000339  0.006231  0.007703 -0.013026    fc1            54  \n",
            "55 -0.014757 -0.008799 -0.020483 -0.005706    fc1            55  \n",
            "56 -0.014795  0.005220 -0.021989 -0.015774    fc1            56  \n",
            "57 -0.000363  0.006102 -0.002093 -0.001637    fc1            57  \n",
            "58  0.026156  0.011964  0.002414  0.006485    fc1            58  \n",
            "59 -0.003636  0.008666 -0.018831 -0.015422    fc1            59  \n",
            "\n",
            "[60 rows x 3138 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_ACTIVATIONS DataFrame:\n",
            "  layer  neuron_index  activation_value\n",
            "0   fc2             0        -12.406099\n",
            "1   fc2             1          4.819819\n",
            "2   fc2             2         -3.512989\n",
            "3   fc2             3         -7.431727\n",
            "4   fc2             4        -12.657157\n",
            "5   fc2             5        -12.563261\n",
            "6   fc2             6        -11.646470\n",
            "7   fc2             7         -5.007533\n",
            "8   fc2             8         -9.513083\n",
            "9   fc2             9         21.274040\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_CONNECTIONS DataFrame:\n",
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.107950 -0.100333 -0.125853  0.091329 -0.090712 -0.024337 -0.035301   \n",
            "1  0.131501 -0.110936 -0.085158  0.066024 -0.037845 -0.071343 -0.052721   \n",
            "2  0.050709 -0.033907 -0.077746 -0.074659 -0.076966 -0.074498 -0.088701   \n",
            "3  0.049182 -0.027318 -0.033456 -0.035291 -0.043763 -0.003068  0.013614   \n",
            "4  0.042645  0.088435  0.038246  0.004014 -0.031166 -0.035241 -0.033445   \n",
            "5  0.086447 -0.126483 -0.040570  0.021679  0.066402 -0.114365  0.069705   \n",
            "6 -0.080449  0.085518 -0.090617 -0.053715 -0.023148 -0.005910  0.053310   \n",
            "7  0.092976 -0.108292 -0.003311 -0.020869  0.013422 -0.048318  0.010532   \n",
            "8 -0.123973 -0.038942  0.098014 -0.079174  0.093177  0.066027  0.115322   \n",
            "9  0.017517 -0.083782  0.037875 -0.062554 -0.064439  0.184223  0.006078   \n",
            "\n",
            "          7         8         9  ...        52        53        54        55  \\\n",
            "0  0.090453 -0.134711  0.053470  ... -0.070573  0.083195  0.095116  0.083355   \n",
            "1 -0.112283  0.045542  0.064597  ...  0.047196  0.127483  0.055454  0.038554   \n",
            "2  0.057485 -0.012508 -0.048672  ...  0.098187  0.072752 -0.079741 -0.097680   \n",
            "3 -0.134513 -0.108175  0.041074  ... -0.005931 -0.089947 -0.010503  0.077362   \n",
            "4 -0.040576  0.073950 -0.058569  ... -0.079893 -0.038126 -0.095315  0.112316   \n",
            "5 -0.126576 -0.152112 -0.069325  ... -0.028109  0.103588 -0.003243  0.043243   \n",
            "6  0.102617  0.096054  0.003210  ... -0.036933  0.010865  0.066728  0.112904   \n",
            "7  0.093271  0.076127 -0.004166  ... -0.105497  0.043385  0.102613  0.121523   \n",
            "8  0.016770 -0.150593  0.057095  ... -0.014134 -0.041150  0.043610 -0.033362   \n",
            "9  0.090238 -0.031772 -0.016623  ... -0.092037  0.067543 -0.015439  0.107121   \n",
            "\n",
            "         56        57        58        59  layer  neuron_index  \n",
            "0 -0.068317  0.057229 -0.010008 -0.046776    fc2             0  \n",
            "1  0.154287 -0.029886 -0.088726  0.017459    fc2             1  \n",
            "2 -0.002260 -0.142917  0.077585  0.014963    fc2             2  \n",
            "3 -0.088103 -0.110321  0.010353  0.046034    fc2             3  \n",
            "4  0.099081  0.096792  0.056523 -0.063165    fc2             4  \n",
            "5 -0.133819 -0.110977 -0.123005 -0.015103    fc2             5  \n",
            "6  0.097748 -0.037395  0.050451 -0.001918    fc2             6  \n",
            "7 -0.045865 -0.148744  0.003459 -0.012868    fc2             7  \n",
            "8 -0.111366 -0.025910  0.000013  0.098205    fc2             8  \n",
            "9 -0.079098  0.062496 -0.018937  0.063364    fc2             9  \n",
            "\n",
            "[10 rows x 62 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Image 7 ---\n",
            "True Label: 9\n",
            "Predicted Label: 9\n",
            "\n",
            "CONV1_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv1             0          0.325491\n",
            "1   conv1             1          0.319614\n",
            "2   conv1             2          0.141402\n",
            "3   conv1             3          0.076259\n",
            "4   conv1             4          0.133737\n",
            "5   conv1             5          0.251774\n",
            "6   conv1             6          0.028345\n",
            "7   conv1             7          0.352989\n",
            "8   conv1             8          0.155167\n",
            "9   conv1             9          0.153122\n",
            "10  conv1            10          0.135067\n",
            "11  conv1            11          0.233292\n",
            "12  conv1            12          0.011696\n",
            "13  conv1            13          0.127145\n",
            "14  conv1            14         -0.013369\n",
            "15  conv1            15          0.028966\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.400412  0.399371  0.071231 -0.067315  0.237489  0.328836 -0.150906   \n",
            "1   0.062670  0.420839  0.423627 -0.087857  0.270959 -0.074010  0.234360   \n",
            "2   0.077322  0.239514  0.362599  0.313162 -0.159267 -0.146957 -0.197186   \n",
            "3  -0.198520  0.416571  0.189112 -0.133381  0.282581  0.187163 -0.016161   \n",
            "4  -0.100635 -0.007484  0.208964  0.262549  0.069296  0.175801  0.311726   \n",
            "5  -0.273976 -0.003054  0.224042  0.069278  0.076746  0.157825  0.075656   \n",
            "6  -0.428757 -0.251183 -0.254338  0.081380  0.144253  0.244236  0.337603   \n",
            "7   0.378913  0.399986  0.071193  0.172418 -0.026089  0.130204 -0.220100   \n",
            "8   0.225943  0.192161  0.387705  0.135396 -0.048392 -0.038235 -0.115391   \n",
            "9   0.307195 -0.180329 -0.171382  0.319853  0.081811  0.181218  0.174932   \n",
            "10  0.130797  0.370466  0.380331  0.015307 -0.078076  0.016426 -0.083861   \n",
            "11  0.392175  0.247163  0.281231  0.259225 -0.270856 -0.165327  0.253949   \n",
            "12 -0.210740 -0.048877 -0.131766  0.335698  0.006075 -0.116493 -0.017947   \n",
            "13  0.119667  0.026561  0.093274  0.343750  0.204018 -0.236069  0.372014   \n",
            "14  0.198226 -0.245182  0.121569 -0.049056 -0.133100 -0.077453  0.442164   \n",
            "15  0.209107  0.173020 -0.301732  0.125146 -0.021763 -0.264287  0.322271   \n",
            "\n",
            "           7         8  layer  neuron_index  \n",
            "0   0.392861 -0.045586  conv1             0  \n",
            "1  -0.093755  0.006661  conv1             1  \n",
            "2   0.068652  0.012842  conv1             2  \n",
            "3  -0.195803  0.062113  conv1             3  \n",
            "4  -0.130634  0.326892  conv1             4  \n",
            "5   0.073296 -0.115009  conv1             5  \n",
            "6  -0.101311  0.121107  conv1             6  \n",
            "7   0.377975 -0.272754  conv1             7  \n",
            "8   0.228945  0.188063  conv1             8  \n",
            "9   0.345546  0.172957  conv1             9  \n",
            "10  0.056517  0.259929  conv1            10  \n",
            "11 -0.198929  0.022492  conv1            11  \n",
            "12  0.285077  0.004282  conv1            12  \n",
            "13  0.204587 -0.130169  conv1            13  \n",
            "14 -0.012433 -0.308704  conv1            14  \n",
            "15 -0.069897  0.135285  conv1            15  \n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv2             0          0.201510\n",
            "1   conv2             1          0.774342\n",
            "2   conv2             2          0.349977\n",
            "3   conv2             3          0.923219\n",
            "4   conv2             4         -0.258462\n",
            "5   conv2             5         -0.276746\n",
            "6   conv2             6          0.540662\n",
            "7   conv2             7          0.391004\n",
            "8   conv2             8          0.233132\n",
            "9   conv2             9          0.072630\n",
            "10  conv2            10          0.838952\n",
            "11  conv2            11          0.329838\n",
            "12  conv2            12          0.474305\n",
            "13  conv2            13         -0.001122\n",
            "14  conv2            14          0.537225\n",
            "15  conv2            15          0.318739\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.053389  0.023715 -0.082655 -0.048249 -0.076035 -0.150770  0.021789   \n",
            "1   0.047589 -0.093007  0.005160  0.069936  0.136776  0.046387  0.010860   \n",
            "2   0.058461  0.100847 -0.038368  0.022629  0.103998 -0.064163 -0.030390   \n",
            "3  -0.035365  0.023469  0.107092  0.101842  0.143467  0.217515 -0.012316   \n",
            "4   0.062249 -0.131436 -0.046978  0.021597 -0.080085 -0.078712 -0.103583   \n",
            "5  -0.019309 -0.080785  0.035847  0.020273  0.009665  0.035786 -0.006190   \n",
            "6   0.049547  0.038741 -0.036449 -0.002058  0.041931  0.053819 -0.005733   \n",
            "7  -0.152914 -0.041600  0.115807 -0.041148 -0.039541  0.071635 -0.011336   \n",
            "8   0.125563  0.033356 -0.048423  0.110720 -0.018201 -0.147717 -0.042072   \n",
            "9  -0.068213 -0.115001 -0.004743 -0.028494 -0.027653  0.079632  0.049930   \n",
            "10 -0.032338 -0.013617  0.151200 -0.075678  0.075957  0.207855  0.038870   \n",
            "11  0.051104  0.012186  0.086417  0.028352  0.156211 -0.002386  0.086014   \n",
            "12  0.082138  0.005539 -0.044524  0.132945  0.106715  0.022446  0.116672   \n",
            "13 -0.043048 -0.140184  0.012286 -0.060239 -0.119047  0.023106 -0.004832   \n",
            "14 -0.021613  0.105474  0.025686  0.030302  0.136904 -0.020385 -0.046524   \n",
            "15  0.117869  0.126556  0.051337  0.002796 -0.029835 -0.061113  0.004300   \n",
            "\n",
            "           7         8         9  ...       136       137       138       139  \\\n",
            "0   0.087136  0.136641  0.125749  ...  0.105212  0.008460  0.086248 -0.005925   \n",
            "1   0.068729 -0.004232 -0.004889  ...  0.075859  0.051621  0.013238  0.045813   \n",
            "2  -0.050265  0.004671  0.000319  ...  0.094657  0.007707  0.022765  0.004527   \n",
            "3   0.084479  0.127317 -0.104037  ...  0.000802  0.089539 -0.032844  0.050453   \n",
            "4   0.048199  0.034387 -0.074277  ...  0.045809 -0.082850  0.027167 -0.046521   \n",
            "5  -0.040837 -0.020941  0.074329  ... -0.027615 -0.036336 -0.055786  0.008235   \n",
            "6  -0.068649 -0.066126 -0.024237  ...  0.095926  0.090875 -0.025141  0.018905   \n",
            "7   0.034141  0.078109 -0.036853  ...  0.086062 -0.007312  0.005437  0.073091   \n",
            "8   0.026514 -0.127686 -0.001961  ...  0.017386 -0.016188  0.018640  0.010524   \n",
            "9  -0.042845  0.092674  0.078472  ... -0.005325  0.106815  0.014020 -0.074019   \n",
            "10 -0.053325  0.046839  0.006819  ...  0.041650  0.170038 -0.044293  0.004112   \n",
            "11  0.042763 -0.056389 -0.043120  ...  0.042914 -0.032490  0.009025  0.004950   \n",
            "12  0.132660 -0.116526 -0.137380  ... -0.008512 -0.055690  0.034679  0.085180   \n",
            "13 -0.028790  0.080668 -0.064381  ... -0.020423  0.075944 -0.070604 -0.053503   \n",
            "14  0.006731 -0.044295  0.048296  ... -0.026215  0.104971  0.072772  0.041096   \n",
            "15  0.013864 -0.005772  0.182691  ... -0.027957 -0.073938 -0.020258 -0.038572   \n",
            "\n",
            "         140       141       142       143  layer  neuron_index  \n",
            "0  -0.033853  0.113257 -0.040305  0.088088  conv2             0  \n",
            "1   0.091032  0.005281 -0.026384  0.078543  conv2             1  \n",
            "2   0.002996  0.085109  0.008654  0.067731  conv2             2  \n",
            "3  -0.007134  0.030011 -0.008236 -0.030027  conv2             3  \n",
            "4   0.067527 -0.041539  0.057837  0.032542  conv2             4  \n",
            "5  -0.075360 -0.063744  0.026530  0.010399  conv2             5  \n",
            "6   0.031970 -0.055053  0.091992 -0.061894  conv2             6  \n",
            "7   0.030808 -0.011768  0.093748  0.089395  conv2             7  \n",
            "8  -0.089284  0.094356  0.033373  0.012280  conv2             8  \n",
            "9   0.011533 -0.078723  0.016492 -0.013510  conv2             9  \n",
            "10  0.017894 -0.050105  0.031401  0.045740  conv2            10  \n",
            "11  0.000354  0.074826  0.052800  0.083592  conv2            11  \n",
            "12  0.014633  0.005773  0.066127 -0.082251  conv2            12  \n",
            "13  0.020353 -0.001231  0.079536  0.032849  conv2            13  \n",
            "14  0.005841  0.030748 -0.013547 -0.054549  conv2            14  \n",
            "15 -0.075074  0.053019 -0.002690 -0.082208  conv2            15  \n",
            "\n",
            "[16 rows x 146 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_ACTIVATIONS DataFrame:\n",
            "   layer  neuron_index  activation_value\n",
            "0    fc1             0         -2.475060\n",
            "1    fc1             1         -3.634043\n",
            "2    fc1             2         -9.312841\n",
            "3    fc1             3         -2.697258\n",
            "4    fc1             4         -4.859474\n",
            "5    fc1             5         20.600525\n",
            "6    fc1             6         -4.904605\n",
            "7    fc1             7         12.853840\n",
            "8    fc1             8         -1.280344\n",
            "9    fc1             9          7.451765\n",
            "10   fc1            10         -4.296096\n",
            "11   fc1            11         -8.160020\n",
            "12   fc1            12         10.104733\n",
            "13   fc1            13         21.285238\n",
            "14   fc1            14         -4.129700\n",
            "15   fc1            15        -11.269062\n",
            "16   fc1            16         -5.272766\n",
            "17   fc1            17         -4.056108\n",
            "18   fc1            18          3.829073\n",
            "19   fc1            19         19.430485\n",
            "20   fc1            20         12.079043\n",
            "21   fc1            21         -8.967997\n",
            "22   fc1            22        -20.178028\n",
            "23   fc1            23          5.212232\n",
            "24   fc1            24         -5.320604\n",
            "25   fc1            25         -2.911291\n",
            "26   fc1            26         -5.404966\n",
            "27   fc1            27         -3.352185\n",
            "28   fc1            28         -5.550096\n",
            "29   fc1            29         -3.848064\n",
            "30   fc1            30         -5.233668\n",
            "31   fc1            31        -20.019167\n",
            "32   fc1            32         -6.785275\n",
            "33   fc1            33          1.589492\n",
            "34   fc1            34         -4.845773\n",
            "35   fc1            35         12.740920\n",
            "36   fc1            36         29.984478\n",
            "37   fc1            37          3.951158\n",
            "38   fc1            38        -11.413001\n",
            "39   fc1            39        -21.468103\n",
            "40   fc1            40         19.956331\n",
            "41   fc1            41         -3.905056\n",
            "42   fc1            42         -3.972392\n",
            "43   fc1            43         -4.385683\n",
            "44   fc1            44         -5.305297\n",
            "45   fc1            45         -5.003367\n",
            "46   fc1            46         -2.912185\n",
            "47   fc1            47         -2.669034\n",
            "48   fc1            48         -3.916558\n",
            "49   fc1            49         39.004929\n",
            "50   fc1            50          8.410062\n",
            "51   fc1            51         -2.662572\n",
            "52   fc1            52         18.043404\n",
            "53   fc1            53         -3.454799\n",
            "54   fc1            54         -3.405384\n",
            "55   fc1            55         -5.473640\n",
            "56   fc1            56         -2.394076\n",
            "57   fc1            57          6.101996\n",
            "58   fc1            58          7.493845\n",
            "59   fc1            59         -5.659228\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.012868  0.012967 -0.004651  0.004214  0.004450 -0.004286 -0.006892   \n",
            "1  -0.006668  0.001739  0.008233 -0.015734 -0.018150  0.006245 -0.009190   \n",
            "2  -0.004715 -0.008510 -0.027038 -0.012099 -0.017072 -0.001276 -0.004790   \n",
            "3  -0.022022  0.006158 -0.000070  0.005327 -0.010999 -0.013339 -0.000802   \n",
            "4  -0.021747  0.005062 -0.007141 -0.002509  0.002286 -0.004356 -0.013751   \n",
            "5   0.000488  0.012703  0.014533  0.016341 -0.004459  0.019696  0.002125   \n",
            "6   0.009300 -0.022666 -0.019488  0.000888 -0.000055 -0.011650 -0.015693   \n",
            "7   0.031482  0.014408  0.003062  0.002343  0.027174  0.003222  0.007536   \n",
            "8  -0.021455 -0.018184  0.010642 -0.009969 -0.002964 -0.022868  0.002633   \n",
            "9   0.008664  0.005595  0.018707  0.036951  0.012074  0.024569  0.035212   \n",
            "10 -0.015157 -0.015687 -0.020529  0.011342  0.001704 -0.014001 -0.023534   \n",
            "11  0.033904  0.009318  0.023171  0.003949  0.003715  0.000735  0.023219   \n",
            "12  0.024346  0.032305  0.026967  0.025876  0.010165  0.022697  0.011940   \n",
            "13  0.008851  0.010857  0.002167  0.024771  0.005659  0.021011  0.000578   \n",
            "14 -0.009209 -0.003457 -0.008076 -0.003801  0.010076 -0.013077 -0.018169   \n",
            "15 -0.022892 -0.007363 -0.001998 -0.025108 -0.005332 -0.008478 -0.006340   \n",
            "16 -0.011208 -0.008056 -0.020657 -0.002330  0.002901 -0.009348 -0.019556   \n",
            "17 -0.004303 -0.011543 -0.022747 -0.002148 -0.000974  0.003379  0.001518   \n",
            "18  0.014590  0.013593  0.026992  0.014942  0.002147  0.024546  0.023784   \n",
            "19  0.013653  0.015041  0.026944  0.017540  0.012362  0.005006  0.020771   \n",
            "20  0.000926 -0.002540  0.000976 -0.007426  0.008134  0.009121  0.006932   \n",
            "21 -0.008481 -0.009680 -0.015762 -0.008213  0.005569 -0.011419 -0.018695   \n",
            "22  0.002687 -0.004754  0.025822  0.022829  0.026687  0.024950  0.015143   \n",
            "23  0.012675 -0.008788  0.020707 -0.002437 -0.000562 -0.001121  0.009253   \n",
            "24 -0.021873 -0.006459  0.000207 -0.022165  0.008027 -0.006639  0.011209   \n",
            "25 -0.009614 -0.022043 -0.007360  0.004037 -0.017339 -0.001730 -0.005209   \n",
            "26 -0.003378 -0.006826 -0.019353 -0.008899  0.007771  0.007703 -0.008105   \n",
            "27 -0.009939 -0.008868  0.000785  0.008521 -0.017249  0.009543 -0.006277   \n",
            "28 -0.012323  0.002773 -0.002832  0.008380 -0.001049 -0.023035 -0.010383   \n",
            "29  0.014561  0.016938  0.017683 -0.001824  0.010139  0.014729  0.012067   \n",
            "30 -0.019001 -0.009498 -0.020953 -0.004602  0.011816  0.005326 -0.018519   \n",
            "31  0.000243  0.000025  0.021961 -0.006239  0.018972 -0.004205  0.002377   \n",
            "32 -0.003725 -0.003355 -0.020392  0.005503 -0.013929 -0.013021  0.001984   \n",
            "33  0.016569  0.002955  0.006740  0.025918  0.010518  0.006316  0.009447   \n",
            "34 -0.021455 -0.010343  0.011155  0.004942 -0.004358 -0.012782 -0.010427   \n",
            "35 -0.004436 -0.022906 -0.004501  0.005599  0.007464 -0.000397 -0.000361   \n",
            "36  0.028495  0.008049  0.033686  0.027099  0.001744  0.033556  0.005032   \n",
            "37  0.017607  0.014472  0.003794  0.007778  0.026253  0.026272  0.000011   \n",
            "38 -0.002645  0.017745  0.021209 -0.002706  0.004354  0.021500  0.015463   \n",
            "39  0.013660  0.008610  0.018474  0.003934  0.033713  0.001638  0.016512   \n",
            "40  0.030918  0.000622  0.009956  0.019433  0.000461 -0.000907  0.013701   \n",
            "41 -0.014202 -0.010007 -0.013808 -0.019121  0.001727 -0.010824  0.006965   \n",
            "42 -0.018572 -0.000763  0.011354 -0.020072 -0.000430  0.000737 -0.016534   \n",
            "43 -0.016530 -0.011671 -0.013214 -0.002635  0.008195 -0.009454 -0.014988   \n",
            "44 -0.004162  0.004954 -0.016971 -0.016766 -0.005005 -0.002056 -0.009664   \n",
            "45 -0.006126  0.000294 -0.010289 -0.013374 -0.021398 -0.016979 -0.005077   \n",
            "46 -0.003019  0.003975 -0.013475  0.004994 -0.012704 -0.002466 -0.018315   \n",
            "47  0.002745  0.005883 -0.017230  0.000825 -0.021160 -0.008062 -0.022425   \n",
            "48 -0.000349 -0.019200  0.004244  0.012784  0.002108 -0.011295 -0.014164   \n",
            "49  0.019498  0.027400  0.033523  0.031643  0.003375  0.024539  0.018732   \n",
            "50  0.002034  0.025831  0.021195  0.015002  0.011998  0.006490  0.015592   \n",
            "51  0.002681 -0.004163 -0.002470  0.002025  0.000847 -0.004836 -0.020047   \n",
            "52  0.031499  0.006817  0.030368  0.016483  0.009225  0.011744  0.023579   \n",
            "53 -0.002231 -0.016763 -0.012501  0.002132 -0.016919  0.000812  0.009893   \n",
            "54 -0.016136  0.002641  0.006465  0.005333  0.000611 -0.021958  0.005562   \n",
            "55  0.006558  0.001421 -0.006817 -0.021912 -0.021949 -0.012012  0.008985   \n",
            "56 -0.013326  0.005539 -0.001117  0.010829  0.002428 -0.019694 -0.017891   \n",
            "57  0.000068  0.003597  0.006211  0.004677 -0.000560  0.023743  0.012332   \n",
            "58  0.028027  0.022394  0.015621  0.022584  0.027930  0.013303  0.023036   \n",
            "59 -0.016706  0.001549  0.002360 -0.005833 -0.013946 -0.005872 -0.015406   \n",
            "\n",
            "           7         8         9  ...      3128      3129      3130      3131  \\\n",
            "0   0.007324 -0.009616 -0.005067  ... -0.006856  0.010652  0.010737 -0.005627   \n",
            "1  -0.018485 -0.011243 -0.014907  ...  0.003302 -0.023178 -0.002841 -0.002705   \n",
            "2   0.003348 -0.021924  0.002252  ... -0.004555 -0.022496 -0.027291  0.000060   \n",
            "3  -0.006648 -0.017124  0.011142  ... -0.008438  0.007751 -0.003895 -0.014379   \n",
            "4   0.009772 -0.011616 -0.003061  ... -0.013893 -0.023509  0.007052  0.009660   \n",
            "5   0.008894  0.007604  0.022646  ...  0.023266  0.041666  0.029488  0.033174   \n",
            "6  -0.003789  0.009797 -0.006847  ...  0.009045 -0.017039 -0.017816  0.009705   \n",
            "7   0.004008  0.015966  0.020602  ...  0.009192  0.034439  0.014094  0.028504   \n",
            "8  -0.003198 -0.022776  0.003764  ... -0.001452 -0.006950 -0.022771  0.003933   \n",
            "9   0.030930  0.004664  0.003770  ...  0.004832 -0.009257 -0.008188  0.001435   \n",
            "10 -0.013386 -0.013077 -0.018894  ... -0.001648  0.000762 -0.017888 -0.016899   \n",
            "11  0.006491  0.032536  0.000740  ...  0.003196  0.004032  0.010177  0.012247   \n",
            "12  0.025155  0.015860  0.000769  ...  0.006513  0.015600  0.012484  0.016776   \n",
            "13  0.029623  0.026839 -0.001954  ...  0.036832  0.027887  0.015517  0.010580   \n",
            "14 -0.016337  0.004727  0.010331  ... -0.012719 -0.016295  0.002682 -0.014291   \n",
            "15 -0.017671 -0.001590 -0.007524  ... -0.027042 -0.015744 -0.009734 -0.021105   \n",
            "16 -0.015554  0.001033  0.005761  ...  0.009729 -0.022332 -0.021350 -0.016759   \n",
            "17  0.010020 -0.019688 -0.004400  ...  0.007721 -0.004402 -0.016819 -0.015915   \n",
            "18  0.002784 -0.006211  0.013223  ...  0.024819  0.008977  0.035165  0.001899   \n",
            "19  0.014684  0.027412  0.018771  ...  0.027893  0.023726  0.017920  0.016699   \n",
            "20 -0.019416 -0.022032 -0.013255  ...  0.047160  0.036355  0.025181  0.022490   \n",
            "21  0.001027 -0.005196 -0.004424  ... -0.011853 -0.003937  0.003567  0.000304   \n",
            "22  0.004678  0.017792 -0.000148  ...  0.014265  0.008728 -0.017063  0.005836   \n",
            "23  0.005949 -0.008079  0.000984  ... -0.013547 -0.005859  0.002466  0.003944   \n",
            "24 -0.012757 -0.023823 -0.018663  ...  0.011678 -0.019435  0.003086 -0.019769   \n",
            "25 -0.010399  0.010380 -0.019753  ...  0.000844  0.010552 -0.019871 -0.019154   \n",
            "26  0.010319 -0.010890  0.007982  ...  0.000887  0.000956  0.001107 -0.014635   \n",
            "27 -0.004731 -0.009723 -0.003905  ...  0.005012 -0.016505 -0.011187 -0.001365   \n",
            "28 -0.015085 -0.001597 -0.002212  ... -0.009379  0.010932  0.000835  0.011187   \n",
            "29  0.009418  0.025151  0.020776  ... -0.004506 -0.015641 -0.013120 -0.010891   \n",
            "30 -0.016916  0.009760 -0.014054  ...  0.007682 -0.003761 -0.015353 -0.000961   \n",
            "31  0.012016 -0.000764 -0.007299  ... -0.012403 -0.009862 -0.011390 -0.003945   \n",
            "32 -0.024166  0.000700  0.009655  ... -0.019903 -0.006628 -0.020596  0.001379   \n",
            "33  0.010933  0.003280  0.013948  ... -0.003015  0.020623 -0.000906  0.015280   \n",
            "34 -0.017208  0.006274  0.003570  ... -0.013828 -0.008093 -0.019012 -0.001721   \n",
            "35  0.005101 -0.010963 -0.009127  ... -0.002773  0.011193  0.019547  0.002818   \n",
            "36  0.012425  0.009884  0.031366  ...  0.042560  0.039935  0.023204  0.029524   \n",
            "37  0.023419  0.006894  0.026621  ...  0.004115  0.000018  0.020470 -0.002871   \n",
            "38  0.018896  0.024307 -0.000390  ... -0.004216 -0.012399 -0.001571 -0.018753   \n",
            "39  0.010963  0.030405  0.032920  ... -0.013158  0.001995 -0.017575  0.000686   \n",
            "40  0.026295  0.013387 -0.001529  ...  0.043029  0.038009  0.020855  0.017207   \n",
            "41 -0.004680 -0.013764 -0.007863  ...  0.008035  0.010515 -0.006477  0.011474   \n",
            "42 -0.000678  0.009945  0.003108  ...  0.008813  0.006511 -0.001635 -0.017173   \n",
            "43 -0.011972 -0.016392  0.001932  ... -0.010337 -0.012674 -0.018367 -0.022919   \n",
            "44  0.003259 -0.000585  0.001807  ...  0.008413 -0.014572 -0.008983  0.005895   \n",
            "45  0.006896 -0.008086 -0.009275  ... -0.005778  0.007000  0.002406 -0.008275   \n",
            "46  0.010720 -0.002166 -0.014401  ... -0.013205 -0.022235 -0.002017 -0.000171   \n",
            "47 -0.005961 -0.003954  0.010139  ...  0.011701  0.001282 -0.018512 -0.003118   \n",
            "48 -0.013545  0.006565 -0.005001  ... -0.010218  0.010282 -0.019655  0.010408   \n",
            "49  0.018647  0.026510  0.016471  ...  0.039909  0.049060  0.028912  0.045582   \n",
            "50  0.000300  0.010269  0.008490  ...  0.023184 -0.001352  0.019045  0.003499   \n",
            "51  0.004892  0.007555  0.001219  ... -0.016053  0.001421 -0.013407 -0.006220   \n",
            "52  0.030600  0.026217  0.023260  ...  0.017529  0.006934  0.017406  0.020949   \n",
            "53 -0.010024 -0.009774 -0.018796  ... -0.015735 -0.020443  0.008829 -0.016565   \n",
            "54  0.011039 -0.000971 -0.004749  ... -0.011213  0.000786 -0.019802 -0.001971   \n",
            "55 -0.017711 -0.011881  0.008669  ...  0.003574  0.005007 -0.005511 -0.009074   \n",
            "56 -0.018805 -0.010618  0.002681  ... -0.014544 -0.013959  0.007968 -0.004889   \n",
            "57  0.013836  0.000618  0.024202  ...  0.020163  0.034249  0.023702  0.032346   \n",
            "58  0.020271 -0.000923  0.004961  ...  0.030553  0.023524  0.025896  0.012225   \n",
            "59  0.006506 -0.000810  0.000023  ... -0.021418 -0.022299 -0.008617 -0.017246   \n",
            "\n",
            "        3132      3133      3134      3135  layer  neuron_index  \n",
            "0  -0.006300  0.000761 -0.010077 -0.010894    fc1             0  \n",
            "1   0.007173  0.003588 -0.009852 -0.016465    fc1             1  \n",
            "2   0.005837 -0.015450 -0.013901  0.002791    fc1             2  \n",
            "3  -0.012918  0.007607 -0.013114 -0.016117    fc1             3  \n",
            "4   0.003922 -0.010699  0.009847 -0.021692    fc1             4  \n",
            "5   0.028678  0.014802  0.011530  0.006059    fc1             5  \n",
            "6  -0.014932  0.004915  0.002246 -0.007906    fc1             6  \n",
            "7   0.013501  0.022247  0.021532 -0.003630    fc1             7  \n",
            "8  -0.006297 -0.015515 -0.013881  0.004368    fc1             8  \n",
            "9  -0.001923  0.008458  0.021580  0.029945    fc1             9  \n",
            "10 -0.000172 -0.004697 -0.007486  0.003054    fc1            10  \n",
            "11 -0.007124 -0.002595 -0.013320 -0.004145    fc1            11  \n",
            "12 -0.001142  0.025493  0.015860  0.012417    fc1            12  \n",
            "13  0.008905  0.014851  0.010457  0.007798    fc1            13  \n",
            "14 -0.010744 -0.008016 -0.019082 -0.007001    fc1            14  \n",
            "15 -0.008899 -0.000829 -0.016586 -0.028002    fc1            15  \n",
            "16  0.011897 -0.018047 -0.002446 -0.017222    fc1            16  \n",
            "17 -0.020536  0.006682 -0.007786 -0.019112    fc1            17  \n",
            "18  0.014848  0.023633  0.009814 -0.009907    fc1            18  \n",
            "19  0.032436  0.014681  0.007802  0.015888    fc1            19  \n",
            "20  0.010047  0.026064  0.016482  0.030144    fc1            20  \n",
            "21  0.008994  0.010006 -0.018727 -0.003075    fc1            21  \n",
            "22 -0.000216 -0.005094  0.018505 -0.008893    fc1            22  \n",
            "23  0.027010  0.004446 -0.004685  0.011521    fc1            23  \n",
            "24 -0.006039  0.005686 -0.007776  0.007923    fc1            24  \n",
            "25 -0.013028 -0.005355 -0.013795  0.000189    fc1            25  \n",
            "26  0.001260  0.002817 -0.000567 -0.018858    fc1            26  \n",
            "27 -0.012066  0.005803  0.008898 -0.020256    fc1            27  \n",
            "28 -0.021011 -0.016277 -0.021642 -0.017041    fc1            28  \n",
            "29  0.015582 -0.001358  0.009005  0.020085    fc1            29  \n",
            "30  0.009142 -0.011048 -0.019065 -0.022964    fc1            30  \n",
            "31 -0.017578  0.018184 -0.014494  0.006414    fc1            31  \n",
            "32 -0.007214 -0.020134 -0.002980 -0.009061    fc1            32  \n",
            "33 -0.002171 -0.004688  0.024596  0.012860    fc1            33  \n",
            "34  0.006539  0.009605 -0.015453 -0.016520    fc1            34  \n",
            "35 -0.018720 -0.022744 -0.017856  0.009409    fc1            35  \n",
            "36  0.040159  0.015496  0.046070  0.038217    fc1            36  \n",
            "37  0.016869  0.005396  0.012609  0.013487    fc1            37  \n",
            "38 -0.000505  0.023441  0.003526  0.005332    fc1            38  \n",
            "39 -0.019023 -0.006869 -0.015280  0.005979    fc1            39  \n",
            "40  0.027607  0.004566  0.021181  0.012148    fc1            40  \n",
            "41  0.004879 -0.012685 -0.009628 -0.022963    fc1            41  \n",
            "42  0.007932 -0.012811 -0.015902 -0.008890    fc1            42  \n",
            "43 -0.016014  0.007799  0.005429 -0.008904    fc1            43  \n",
            "44  0.004329  0.007101 -0.003699 -0.012319    fc1            44  \n",
            "45  0.005780  0.009909 -0.004622 -0.021927    fc1            45  \n",
            "46 -0.013934 -0.011455 -0.007828 -0.010529    fc1            46  \n",
            "47 -0.016764 -0.023845 -0.003573  0.011422    fc1            47  \n",
            "48 -0.015247 -0.011081 -0.016139 -0.007430    fc1            48  \n",
            "49  0.037967  0.039943  0.047666  0.032830    fc1            49  \n",
            "50  0.002823 -0.002692 -0.008353  0.002865    fc1            50  \n",
            "51 -0.014761  0.009049 -0.008702 -0.023180    fc1            51  \n",
            "52  0.023632  0.022728  0.018886  0.025511    fc1            52  \n",
            "53 -0.018722  0.011494  0.004309 -0.023839    fc1            53  \n",
            "54  0.000339  0.006231  0.007703 -0.013026    fc1            54  \n",
            "55 -0.014757 -0.008799 -0.020483 -0.005706    fc1            55  \n",
            "56 -0.014795  0.005220 -0.021989 -0.015774    fc1            56  \n",
            "57 -0.000363  0.006102 -0.002093 -0.001637    fc1            57  \n",
            "58  0.026156  0.011964  0.002414  0.006485    fc1            58  \n",
            "59 -0.003636  0.008666 -0.018831 -0.015422    fc1            59  \n",
            "\n",
            "[60 rows x 3138 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_ACTIVATIONS DataFrame:\n",
            "  layer  neuron_index  activation_value\n",
            "0   fc2             0        -11.022217\n",
            "1   fc2             1          3.770842\n",
            "2   fc2             2         -3.518831\n",
            "3   fc2             3         -6.687406\n",
            "4   fc2             4        -11.344072\n",
            "5   fc2             5        -11.472692\n",
            "6   fc2             6        -10.909607\n",
            "7   fc2             7         -5.440419\n",
            "8   fc2             8         -7.470374\n",
            "9   fc2             9         20.566538\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_CONNECTIONS DataFrame:\n",
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.107950 -0.100333 -0.125853  0.091329 -0.090712 -0.024337 -0.035301   \n",
            "1  0.131501 -0.110936 -0.085158  0.066024 -0.037845 -0.071343 -0.052721   \n",
            "2  0.050709 -0.033907 -0.077746 -0.074659 -0.076966 -0.074498 -0.088701   \n",
            "3  0.049182 -0.027318 -0.033456 -0.035291 -0.043763 -0.003068  0.013614   \n",
            "4  0.042645  0.088435  0.038246  0.004014 -0.031166 -0.035241 -0.033445   \n",
            "5  0.086447 -0.126483 -0.040570  0.021679  0.066402 -0.114365  0.069705   \n",
            "6 -0.080449  0.085518 -0.090617 -0.053715 -0.023148 -0.005910  0.053310   \n",
            "7  0.092976 -0.108292 -0.003311 -0.020869  0.013422 -0.048318  0.010532   \n",
            "8 -0.123973 -0.038942  0.098014 -0.079174  0.093177  0.066027  0.115322   \n",
            "9  0.017517 -0.083782  0.037875 -0.062554 -0.064439  0.184223  0.006078   \n",
            "\n",
            "          7         8         9  ...        52        53        54        55  \\\n",
            "0  0.090453 -0.134711  0.053470  ... -0.070573  0.083195  0.095116  0.083355   \n",
            "1 -0.112283  0.045542  0.064597  ...  0.047196  0.127483  0.055454  0.038554   \n",
            "2  0.057485 -0.012508 -0.048672  ...  0.098187  0.072752 -0.079741 -0.097680   \n",
            "3 -0.134513 -0.108175  0.041074  ... -0.005931 -0.089947 -0.010503  0.077362   \n",
            "4 -0.040576  0.073950 -0.058569  ... -0.079893 -0.038126 -0.095315  0.112316   \n",
            "5 -0.126576 -0.152112 -0.069325  ... -0.028109  0.103588 -0.003243  0.043243   \n",
            "6  0.102617  0.096054  0.003210  ... -0.036933  0.010865  0.066728  0.112904   \n",
            "7  0.093271  0.076127 -0.004166  ... -0.105497  0.043385  0.102613  0.121523   \n",
            "8  0.016770 -0.150593  0.057095  ... -0.014134 -0.041150  0.043610 -0.033362   \n",
            "9  0.090238 -0.031772 -0.016623  ... -0.092037  0.067543 -0.015439  0.107121   \n",
            "\n",
            "         56        57        58        59  layer  neuron_index  \n",
            "0 -0.068317  0.057229 -0.010008 -0.046776    fc2             0  \n",
            "1  0.154287 -0.029886 -0.088726  0.017459    fc2             1  \n",
            "2 -0.002260 -0.142917  0.077585  0.014963    fc2             2  \n",
            "3 -0.088103 -0.110321  0.010353  0.046034    fc2             3  \n",
            "4  0.099081  0.096792  0.056523 -0.063165    fc2             4  \n",
            "5 -0.133819 -0.110977 -0.123005 -0.015103    fc2             5  \n",
            "6  0.097748 -0.037395  0.050451 -0.001918    fc2             6  \n",
            "7 -0.045865 -0.148744  0.003459 -0.012868    fc2             7  \n",
            "8 -0.111366 -0.025910  0.000013  0.098205    fc2             8  \n",
            "9 -0.079098  0.062496 -0.018937  0.063364    fc2             9  \n",
            "\n",
            "[10 rows x 62 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Image 8 ---\n",
            "True Label: 1\n",
            "Predicted Label: 1\n",
            "\n",
            "CONV1_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv1             0          0.319215\n",
            "1   conv1             1          0.314952\n",
            "2   conv1             2          0.139115\n",
            "3   conv1             3          0.073881\n",
            "4   conv1             4          0.129264\n",
            "5   conv1             5          0.250633\n",
            "6   conv1             6          0.028774\n",
            "7   conv1             7          0.348935\n",
            "8   conv1             8          0.150535\n",
            "9   conv1             9          0.148186\n",
            "10  conv1            10          0.130789\n",
            "11  conv1            11          0.230002\n",
            "12  conv1            12          0.011274\n",
            "13  conv1            13          0.123148\n",
            "14  conv1            14         -0.013112\n",
            "15  conv1            15          0.027735\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.400412  0.399371  0.071231 -0.067315  0.237489  0.328836 -0.150906   \n",
            "1   0.062670  0.420839  0.423627 -0.087857  0.270959 -0.074010  0.234360   \n",
            "2   0.077322  0.239514  0.362599  0.313162 -0.159267 -0.146957 -0.197186   \n",
            "3  -0.198520  0.416571  0.189112 -0.133381  0.282581  0.187163 -0.016161   \n",
            "4  -0.100635 -0.007484  0.208964  0.262549  0.069296  0.175801  0.311726   \n",
            "5  -0.273976 -0.003054  0.224042  0.069278  0.076746  0.157825  0.075656   \n",
            "6  -0.428757 -0.251183 -0.254338  0.081380  0.144253  0.244236  0.337603   \n",
            "7   0.378913  0.399986  0.071193  0.172418 -0.026089  0.130204 -0.220100   \n",
            "8   0.225943  0.192161  0.387705  0.135396 -0.048392 -0.038235 -0.115391   \n",
            "9   0.307195 -0.180329 -0.171382  0.319853  0.081811  0.181218  0.174932   \n",
            "10  0.130797  0.370466  0.380331  0.015307 -0.078076  0.016426 -0.083861   \n",
            "11  0.392175  0.247163  0.281231  0.259225 -0.270856 -0.165327  0.253949   \n",
            "12 -0.210740 -0.048877 -0.131766  0.335698  0.006075 -0.116493 -0.017947   \n",
            "13  0.119667  0.026561  0.093274  0.343750  0.204018 -0.236069  0.372014   \n",
            "14  0.198226 -0.245182  0.121569 -0.049056 -0.133100 -0.077453  0.442164   \n",
            "15  0.209107  0.173020 -0.301732  0.125146 -0.021763 -0.264287  0.322271   \n",
            "\n",
            "           7         8  layer  neuron_index  \n",
            "0   0.392861 -0.045586  conv1             0  \n",
            "1  -0.093755  0.006661  conv1             1  \n",
            "2   0.068652  0.012842  conv1             2  \n",
            "3  -0.195803  0.062113  conv1             3  \n",
            "4  -0.130634  0.326892  conv1             4  \n",
            "5   0.073296 -0.115009  conv1             5  \n",
            "6  -0.101311  0.121107  conv1             6  \n",
            "7   0.377975 -0.272754  conv1             7  \n",
            "8   0.228945  0.188063  conv1             8  \n",
            "9   0.345546  0.172957  conv1             9  \n",
            "10  0.056517  0.259929  conv1            10  \n",
            "11 -0.198929  0.022492  conv1            11  \n",
            "12  0.285077  0.004282  conv1            12  \n",
            "13  0.204587 -0.130169  conv1            13  \n",
            "14 -0.012433 -0.308704  conv1            14  \n",
            "15 -0.069897  0.135285  conv1            15  \n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv2             0          0.202917\n",
            "1   conv2             1          0.748655\n",
            "2   conv2             2          0.340799\n",
            "3   conv2             3          0.896695\n",
            "4   conv2             4         -0.251508\n",
            "5   conv2             5         -0.279391\n",
            "6   conv2             6          0.528534\n",
            "7   conv2             7          0.384783\n",
            "8   conv2             8          0.231009\n",
            "9   conv2             9          0.075927\n",
            "10  conv2            10          0.811425\n",
            "11  conv2            11          0.324231\n",
            "12  conv2            12          0.454995\n",
            "13  conv2            13          0.001478\n",
            "14  conv2            14          0.514112\n",
            "15  conv2            15          0.316795\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.053389  0.023715 -0.082655 -0.048249 -0.076035 -0.150770  0.021789   \n",
            "1   0.047589 -0.093007  0.005160  0.069936  0.136776  0.046387  0.010860   \n",
            "2   0.058461  0.100847 -0.038368  0.022629  0.103998 -0.064163 -0.030390   \n",
            "3  -0.035365  0.023469  0.107092  0.101842  0.143467  0.217515 -0.012316   \n",
            "4   0.062249 -0.131436 -0.046978  0.021597 -0.080085 -0.078712 -0.103583   \n",
            "5  -0.019309 -0.080785  0.035847  0.020273  0.009665  0.035786 -0.006190   \n",
            "6   0.049547  0.038741 -0.036449 -0.002058  0.041931  0.053819 -0.005733   \n",
            "7  -0.152914 -0.041600  0.115807 -0.041148 -0.039541  0.071635 -0.011336   \n",
            "8   0.125563  0.033356 -0.048423  0.110720 -0.018201 -0.147717 -0.042072   \n",
            "9  -0.068213 -0.115001 -0.004743 -0.028494 -0.027653  0.079632  0.049930   \n",
            "10 -0.032338 -0.013617  0.151200 -0.075678  0.075957  0.207855  0.038870   \n",
            "11  0.051104  0.012186  0.086417  0.028352  0.156211 -0.002386  0.086014   \n",
            "12  0.082138  0.005539 -0.044524  0.132945  0.106715  0.022446  0.116672   \n",
            "13 -0.043048 -0.140184  0.012286 -0.060239 -0.119047  0.023106 -0.004832   \n",
            "14 -0.021613  0.105474  0.025686  0.030302  0.136904 -0.020385 -0.046524   \n",
            "15  0.117869  0.126556  0.051337  0.002796 -0.029835 -0.061113  0.004300   \n",
            "\n",
            "           7         8         9  ...       136       137       138       139  \\\n",
            "0   0.087136  0.136641  0.125749  ...  0.105212  0.008460  0.086248 -0.005925   \n",
            "1   0.068729 -0.004232 -0.004889  ...  0.075859  0.051621  0.013238  0.045813   \n",
            "2  -0.050265  0.004671  0.000319  ...  0.094657  0.007707  0.022765  0.004527   \n",
            "3   0.084479  0.127317 -0.104037  ...  0.000802  0.089539 -0.032844  0.050453   \n",
            "4   0.048199  0.034387 -0.074277  ...  0.045809 -0.082850  0.027167 -0.046521   \n",
            "5  -0.040837 -0.020941  0.074329  ... -0.027615 -0.036336 -0.055786  0.008235   \n",
            "6  -0.068649 -0.066126 -0.024237  ...  0.095926  0.090875 -0.025141  0.018905   \n",
            "7   0.034141  0.078109 -0.036853  ...  0.086062 -0.007312  0.005437  0.073091   \n",
            "8   0.026514 -0.127686 -0.001961  ...  0.017386 -0.016188  0.018640  0.010524   \n",
            "9  -0.042845  0.092674  0.078472  ... -0.005325  0.106815  0.014020 -0.074019   \n",
            "10 -0.053325  0.046839  0.006819  ...  0.041650  0.170038 -0.044293  0.004112   \n",
            "11  0.042763 -0.056389 -0.043120  ...  0.042914 -0.032490  0.009025  0.004950   \n",
            "12  0.132660 -0.116526 -0.137380  ... -0.008512 -0.055690  0.034679  0.085180   \n",
            "13 -0.028790  0.080668 -0.064381  ... -0.020423  0.075944 -0.070604 -0.053503   \n",
            "14  0.006731 -0.044295  0.048296  ... -0.026215  0.104971  0.072772  0.041096   \n",
            "15  0.013864 -0.005772  0.182691  ... -0.027957 -0.073938 -0.020258 -0.038572   \n",
            "\n",
            "         140       141       142       143  layer  neuron_index  \n",
            "0  -0.033853  0.113257 -0.040305  0.088088  conv2             0  \n",
            "1   0.091032  0.005281 -0.026384  0.078543  conv2             1  \n",
            "2   0.002996  0.085109  0.008654  0.067731  conv2             2  \n",
            "3  -0.007134  0.030011 -0.008236 -0.030027  conv2             3  \n",
            "4   0.067527 -0.041539  0.057837  0.032542  conv2             4  \n",
            "5  -0.075360 -0.063744  0.026530  0.010399  conv2             5  \n",
            "6   0.031970 -0.055053  0.091992 -0.061894  conv2             6  \n",
            "7   0.030808 -0.011768  0.093748  0.089395  conv2             7  \n",
            "8  -0.089284  0.094356  0.033373  0.012280  conv2             8  \n",
            "9   0.011533 -0.078723  0.016492 -0.013510  conv2             9  \n",
            "10  0.017894 -0.050105  0.031401  0.045740  conv2            10  \n",
            "11  0.000354  0.074826  0.052800  0.083592  conv2            11  \n",
            "12  0.014633  0.005773  0.066127 -0.082251  conv2            12  \n",
            "13  0.020353 -0.001231  0.079536  0.032849  conv2            13  \n",
            "14  0.005841  0.030748 -0.013547 -0.054549  conv2            14  \n",
            "15 -0.075074  0.053019 -0.002690 -0.082208  conv2            15  \n",
            "\n",
            "[16 rows x 146 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_ACTIVATIONS DataFrame:\n",
            "   layer  neuron_index  activation_value\n",
            "0    fc1             0         -3.842015\n",
            "1    fc1             1         -3.728878\n",
            "2    fc1             2        -10.085854\n",
            "3    fc1             3         -3.894689\n",
            "4    fc1             4         -5.000023\n",
            "5    fc1             5          1.602005\n",
            "6    fc1             6         -5.057931\n",
            "7    fc1             7         -0.265975\n",
            "8    fc1             8         -0.927601\n",
            "9    fc1             9          9.991329\n",
            "10   fc1            10         -3.984593\n",
            "11   fc1            11         -4.872675\n",
            "12   fc1            12         15.775801\n",
            "13   fc1            13         30.071909\n",
            "14   fc1            14         -4.123498\n",
            "15   fc1            15        -11.733695\n",
            "16   fc1            16         -5.904447\n",
            "17   fc1            17         -3.754991\n",
            "18   fc1            18          3.720698\n",
            "19   fc1            19          3.731127\n",
            "20   fc1            20         16.371969\n",
            "21   fc1            21         -8.744480\n",
            "22   fc1            22         -8.300736\n",
            "23   fc1            23         14.536884\n",
            "24   fc1            24         -5.033106\n",
            "25   fc1            25         -3.999710\n",
            "26   fc1            26         -4.502080\n",
            "27   fc1            27         -2.712178\n",
            "28   fc1            28         -5.234144\n",
            "29   fc1            29         15.705704\n",
            "30   fc1            30         -4.558568\n",
            "31   fc1            31         -7.400479\n",
            "32   fc1            32         -6.852383\n",
            "33   fc1            33         14.223872\n",
            "34   fc1            34         -5.527795\n",
            "35   fc1            35         16.476215\n",
            "36   fc1            36         36.934155\n",
            "37   fc1            37         19.772142\n",
            "38   fc1            38          9.642697\n",
            "39   fc1            39        -11.184076\n",
            "40   fc1            40          0.406085\n",
            "41   fc1            41         -4.082735\n",
            "42   fc1            42         -3.993624\n",
            "43   fc1            43         -4.160530\n",
            "44   fc1            44         -4.300208\n",
            "45   fc1            45         -5.817835\n",
            "46   fc1            46         -3.689883\n",
            "47   fc1            47         -3.049734\n",
            "48   fc1            48         -3.067275\n",
            "49   fc1            49         31.939972\n",
            "50   fc1            50          9.626537\n",
            "51   fc1            51         -2.452935\n",
            "52   fc1            52         19.442810\n",
            "53   fc1            53         -3.091329\n",
            "54   fc1            54         -3.297050\n",
            "55   fc1            55         -7.199781\n",
            "56   fc1            56         -3.129602\n",
            "57   fc1            57          4.308709\n",
            "58   fc1            58         -7.322227\n",
            "59   fc1            59         -4.377963\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.012868  0.012967 -0.004651  0.004214  0.004450 -0.004286 -0.006892   \n",
            "1  -0.006668  0.001739  0.008233 -0.015734 -0.018150  0.006245 -0.009190   \n",
            "2  -0.004715 -0.008510 -0.027038 -0.012099 -0.017072 -0.001276 -0.004790   \n",
            "3  -0.022022  0.006158 -0.000070  0.005327 -0.010999 -0.013339 -0.000802   \n",
            "4  -0.021747  0.005062 -0.007141 -0.002509  0.002286 -0.004356 -0.013751   \n",
            "5   0.000488  0.012703  0.014533  0.016341 -0.004459  0.019696  0.002125   \n",
            "6   0.009300 -0.022666 -0.019488  0.000888 -0.000055 -0.011650 -0.015693   \n",
            "7   0.031482  0.014408  0.003062  0.002343  0.027174  0.003222  0.007536   \n",
            "8  -0.021455 -0.018184  0.010642 -0.009969 -0.002964 -0.022868  0.002633   \n",
            "9   0.008664  0.005595  0.018707  0.036951  0.012074  0.024569  0.035212   \n",
            "10 -0.015157 -0.015687 -0.020529  0.011342  0.001704 -0.014001 -0.023534   \n",
            "11  0.033904  0.009318  0.023171  0.003949  0.003715  0.000735  0.023219   \n",
            "12  0.024346  0.032305  0.026967  0.025876  0.010165  0.022697  0.011940   \n",
            "13  0.008851  0.010857  0.002167  0.024771  0.005659  0.021011  0.000578   \n",
            "14 -0.009209 -0.003457 -0.008076 -0.003801  0.010076 -0.013077 -0.018169   \n",
            "15 -0.022892 -0.007363 -0.001998 -0.025108 -0.005332 -0.008478 -0.006340   \n",
            "16 -0.011208 -0.008056 -0.020657 -0.002330  0.002901 -0.009348 -0.019556   \n",
            "17 -0.004303 -0.011543 -0.022747 -0.002148 -0.000974  0.003379  0.001518   \n",
            "18  0.014590  0.013593  0.026992  0.014942  0.002147  0.024546  0.023784   \n",
            "19  0.013653  0.015041  0.026944  0.017540  0.012362  0.005006  0.020771   \n",
            "20  0.000926 -0.002540  0.000976 -0.007426  0.008134  0.009121  0.006932   \n",
            "21 -0.008481 -0.009680 -0.015762 -0.008213  0.005569 -0.011419 -0.018695   \n",
            "22  0.002687 -0.004754  0.025822  0.022829  0.026687  0.024950  0.015143   \n",
            "23  0.012675 -0.008788  0.020707 -0.002437 -0.000562 -0.001121  0.009253   \n",
            "24 -0.021873 -0.006459  0.000207 -0.022165  0.008027 -0.006639  0.011209   \n",
            "25 -0.009614 -0.022043 -0.007360  0.004037 -0.017339 -0.001730 -0.005209   \n",
            "26 -0.003378 -0.006826 -0.019353 -0.008899  0.007771  0.007703 -0.008105   \n",
            "27 -0.009939 -0.008868  0.000785  0.008521 -0.017249  0.009543 -0.006277   \n",
            "28 -0.012323  0.002773 -0.002832  0.008380 -0.001049 -0.023035 -0.010383   \n",
            "29  0.014561  0.016938  0.017683 -0.001824  0.010139  0.014729  0.012067   \n",
            "30 -0.019001 -0.009498 -0.020953 -0.004602  0.011816  0.005326 -0.018519   \n",
            "31  0.000243  0.000025  0.021961 -0.006239  0.018972 -0.004205  0.002377   \n",
            "32 -0.003725 -0.003355 -0.020392  0.005503 -0.013929 -0.013021  0.001984   \n",
            "33  0.016569  0.002955  0.006740  0.025918  0.010518  0.006316  0.009447   \n",
            "34 -0.021455 -0.010343  0.011155  0.004942 -0.004358 -0.012782 -0.010427   \n",
            "35 -0.004436 -0.022906 -0.004501  0.005599  0.007464 -0.000397 -0.000361   \n",
            "36  0.028495  0.008049  0.033686  0.027099  0.001744  0.033556  0.005032   \n",
            "37  0.017607  0.014472  0.003794  0.007778  0.026253  0.026272  0.000011   \n",
            "38 -0.002645  0.017745  0.021209 -0.002706  0.004354  0.021500  0.015463   \n",
            "39  0.013660  0.008610  0.018474  0.003934  0.033713  0.001638  0.016512   \n",
            "40  0.030918  0.000622  0.009956  0.019433  0.000461 -0.000907  0.013701   \n",
            "41 -0.014202 -0.010007 -0.013808 -0.019121  0.001727 -0.010824  0.006965   \n",
            "42 -0.018572 -0.000763  0.011354 -0.020072 -0.000430  0.000737 -0.016534   \n",
            "43 -0.016530 -0.011671 -0.013214 -0.002635  0.008195 -0.009454 -0.014988   \n",
            "44 -0.004162  0.004954 -0.016971 -0.016766 -0.005005 -0.002056 -0.009664   \n",
            "45 -0.006126  0.000294 -0.010289 -0.013374 -0.021398 -0.016979 -0.005077   \n",
            "46 -0.003019  0.003975 -0.013475  0.004994 -0.012704 -0.002466 -0.018315   \n",
            "47  0.002745  0.005883 -0.017230  0.000825 -0.021160 -0.008062 -0.022425   \n",
            "48 -0.000349 -0.019200  0.004244  0.012784  0.002108 -0.011295 -0.014164   \n",
            "49  0.019498  0.027400  0.033523  0.031643  0.003375  0.024539  0.018732   \n",
            "50  0.002034  0.025831  0.021195  0.015002  0.011998  0.006490  0.015592   \n",
            "51  0.002681 -0.004163 -0.002470  0.002025  0.000847 -0.004836 -0.020047   \n",
            "52  0.031499  0.006817  0.030368  0.016483  0.009225  0.011744  0.023579   \n",
            "53 -0.002231 -0.016763 -0.012501  0.002132 -0.016919  0.000812  0.009893   \n",
            "54 -0.016136  0.002641  0.006465  0.005333  0.000611 -0.021958  0.005562   \n",
            "55  0.006558  0.001421 -0.006817 -0.021912 -0.021949 -0.012012  0.008985   \n",
            "56 -0.013326  0.005539 -0.001117  0.010829  0.002428 -0.019694 -0.017891   \n",
            "57  0.000068  0.003597  0.006211  0.004677 -0.000560  0.023743  0.012332   \n",
            "58  0.028027  0.022394  0.015621  0.022584  0.027930  0.013303  0.023036   \n",
            "59 -0.016706  0.001549  0.002360 -0.005833 -0.013946 -0.005872 -0.015406   \n",
            "\n",
            "           7         8         9  ...      3128      3129      3130      3131  \\\n",
            "0   0.007324 -0.009616 -0.005067  ... -0.006856  0.010652  0.010737 -0.005627   \n",
            "1  -0.018485 -0.011243 -0.014907  ...  0.003302 -0.023178 -0.002841 -0.002705   \n",
            "2   0.003348 -0.021924  0.002252  ... -0.004555 -0.022496 -0.027291  0.000060   \n",
            "3  -0.006648 -0.017124  0.011142  ... -0.008438  0.007751 -0.003895 -0.014379   \n",
            "4   0.009772 -0.011616 -0.003061  ... -0.013893 -0.023509  0.007052  0.009660   \n",
            "5   0.008894  0.007604  0.022646  ...  0.023266  0.041666  0.029488  0.033174   \n",
            "6  -0.003789  0.009797 -0.006847  ...  0.009045 -0.017039 -0.017816  0.009705   \n",
            "7   0.004008  0.015966  0.020602  ...  0.009192  0.034439  0.014094  0.028504   \n",
            "8  -0.003198 -0.022776  0.003764  ... -0.001452 -0.006950 -0.022771  0.003933   \n",
            "9   0.030930  0.004664  0.003770  ...  0.004832 -0.009257 -0.008188  0.001435   \n",
            "10 -0.013386 -0.013077 -0.018894  ... -0.001648  0.000762 -0.017888 -0.016899   \n",
            "11  0.006491  0.032536  0.000740  ...  0.003196  0.004032  0.010177  0.012247   \n",
            "12  0.025155  0.015860  0.000769  ...  0.006513  0.015600  0.012484  0.016776   \n",
            "13  0.029623  0.026839 -0.001954  ...  0.036832  0.027887  0.015517  0.010580   \n",
            "14 -0.016337  0.004727  0.010331  ... -0.012719 -0.016295  0.002682 -0.014291   \n",
            "15 -0.017671 -0.001590 -0.007524  ... -0.027042 -0.015744 -0.009734 -0.021105   \n",
            "16 -0.015554  0.001033  0.005761  ...  0.009729 -0.022332 -0.021350 -0.016759   \n",
            "17  0.010020 -0.019688 -0.004400  ...  0.007721 -0.004402 -0.016819 -0.015915   \n",
            "18  0.002784 -0.006211  0.013223  ...  0.024819  0.008977  0.035165  0.001899   \n",
            "19  0.014684  0.027412  0.018771  ...  0.027893  0.023726  0.017920  0.016699   \n",
            "20 -0.019416 -0.022032 -0.013255  ...  0.047160  0.036355  0.025181  0.022490   \n",
            "21  0.001027 -0.005196 -0.004424  ... -0.011853 -0.003937  0.003567  0.000304   \n",
            "22  0.004678  0.017792 -0.000148  ...  0.014265  0.008728 -0.017063  0.005836   \n",
            "23  0.005949 -0.008079  0.000984  ... -0.013547 -0.005859  0.002466  0.003944   \n",
            "24 -0.012757 -0.023823 -0.018663  ...  0.011678 -0.019435  0.003086 -0.019769   \n",
            "25 -0.010399  0.010380 -0.019753  ...  0.000844  0.010552 -0.019871 -0.019154   \n",
            "26  0.010319 -0.010890  0.007982  ...  0.000887  0.000956  0.001107 -0.014635   \n",
            "27 -0.004731 -0.009723 -0.003905  ...  0.005012 -0.016505 -0.011187 -0.001365   \n",
            "28 -0.015085 -0.001597 -0.002212  ... -0.009379  0.010932  0.000835  0.011187   \n",
            "29  0.009418  0.025151  0.020776  ... -0.004506 -0.015641 -0.013120 -0.010891   \n",
            "30 -0.016916  0.009760 -0.014054  ...  0.007682 -0.003761 -0.015353 -0.000961   \n",
            "31  0.012016 -0.000764 -0.007299  ... -0.012403 -0.009862 -0.011390 -0.003945   \n",
            "32 -0.024166  0.000700  0.009655  ... -0.019903 -0.006628 -0.020596  0.001379   \n",
            "33  0.010933  0.003280  0.013948  ... -0.003015  0.020623 -0.000906  0.015280   \n",
            "34 -0.017208  0.006274  0.003570  ... -0.013828 -0.008093 -0.019012 -0.001721   \n",
            "35  0.005101 -0.010963 -0.009127  ... -0.002773  0.011193  0.019547  0.002818   \n",
            "36  0.012425  0.009884  0.031366  ...  0.042560  0.039935  0.023204  0.029524   \n",
            "37  0.023419  0.006894  0.026621  ...  0.004115  0.000018  0.020470 -0.002871   \n",
            "38  0.018896  0.024307 -0.000390  ... -0.004216 -0.012399 -0.001571 -0.018753   \n",
            "39  0.010963  0.030405  0.032920  ... -0.013158  0.001995 -0.017575  0.000686   \n",
            "40  0.026295  0.013387 -0.001529  ...  0.043029  0.038009  0.020855  0.017207   \n",
            "41 -0.004680 -0.013764 -0.007863  ...  0.008035  0.010515 -0.006477  0.011474   \n",
            "42 -0.000678  0.009945  0.003108  ...  0.008813  0.006511 -0.001635 -0.017173   \n",
            "43 -0.011972 -0.016392  0.001932  ... -0.010337 -0.012674 -0.018367 -0.022919   \n",
            "44  0.003259 -0.000585  0.001807  ...  0.008413 -0.014572 -0.008983  0.005895   \n",
            "45  0.006896 -0.008086 -0.009275  ... -0.005778  0.007000  0.002406 -0.008275   \n",
            "46  0.010720 -0.002166 -0.014401  ... -0.013205 -0.022235 -0.002017 -0.000171   \n",
            "47 -0.005961 -0.003954  0.010139  ...  0.011701  0.001282 -0.018512 -0.003118   \n",
            "48 -0.013545  0.006565 -0.005001  ... -0.010218  0.010282 -0.019655  0.010408   \n",
            "49  0.018647  0.026510  0.016471  ...  0.039909  0.049060  0.028912  0.045582   \n",
            "50  0.000300  0.010269  0.008490  ...  0.023184 -0.001352  0.019045  0.003499   \n",
            "51  0.004892  0.007555  0.001219  ... -0.016053  0.001421 -0.013407 -0.006220   \n",
            "52  0.030600  0.026217  0.023260  ...  0.017529  0.006934  0.017406  0.020949   \n",
            "53 -0.010024 -0.009774 -0.018796  ... -0.015735 -0.020443  0.008829 -0.016565   \n",
            "54  0.011039 -0.000971 -0.004749  ... -0.011213  0.000786 -0.019802 -0.001971   \n",
            "55 -0.017711 -0.011881  0.008669  ...  0.003574  0.005007 -0.005511 -0.009074   \n",
            "56 -0.018805 -0.010618  0.002681  ... -0.014544 -0.013959  0.007968 -0.004889   \n",
            "57  0.013836  0.000618  0.024202  ...  0.020163  0.034249  0.023702  0.032346   \n",
            "58  0.020271 -0.000923  0.004961  ...  0.030553  0.023524  0.025896  0.012225   \n",
            "59  0.006506 -0.000810  0.000023  ... -0.021418 -0.022299 -0.008617 -0.017246   \n",
            "\n",
            "        3132      3133      3134      3135  layer  neuron_index  \n",
            "0  -0.006300  0.000761 -0.010077 -0.010894    fc1             0  \n",
            "1   0.007173  0.003588 -0.009852 -0.016465    fc1             1  \n",
            "2   0.005837 -0.015450 -0.013901  0.002791    fc1             2  \n",
            "3  -0.012918  0.007607 -0.013114 -0.016117    fc1             3  \n",
            "4   0.003922 -0.010699  0.009847 -0.021692    fc1             4  \n",
            "5   0.028678  0.014802  0.011530  0.006059    fc1             5  \n",
            "6  -0.014932  0.004915  0.002246 -0.007906    fc1             6  \n",
            "7   0.013501  0.022247  0.021532 -0.003630    fc1             7  \n",
            "8  -0.006297 -0.015515 -0.013881  0.004368    fc1             8  \n",
            "9  -0.001923  0.008458  0.021580  0.029945    fc1             9  \n",
            "10 -0.000172 -0.004697 -0.007486  0.003054    fc1            10  \n",
            "11 -0.007124 -0.002595 -0.013320 -0.004145    fc1            11  \n",
            "12 -0.001142  0.025493  0.015860  0.012417    fc1            12  \n",
            "13  0.008905  0.014851  0.010457  0.007798    fc1            13  \n",
            "14 -0.010744 -0.008016 -0.019082 -0.007001    fc1            14  \n",
            "15 -0.008899 -0.000829 -0.016586 -0.028002    fc1            15  \n",
            "16  0.011897 -0.018047 -0.002446 -0.017222    fc1            16  \n",
            "17 -0.020536  0.006682 -0.007786 -0.019112    fc1            17  \n",
            "18  0.014848  0.023633  0.009814 -0.009907    fc1            18  \n",
            "19  0.032436  0.014681  0.007802  0.015888    fc1            19  \n",
            "20  0.010047  0.026064  0.016482  0.030144    fc1            20  \n",
            "21  0.008994  0.010006 -0.018727 -0.003075    fc1            21  \n",
            "22 -0.000216 -0.005094  0.018505 -0.008893    fc1            22  \n",
            "23  0.027010  0.004446 -0.004685  0.011521    fc1            23  \n",
            "24 -0.006039  0.005686 -0.007776  0.007923    fc1            24  \n",
            "25 -0.013028 -0.005355 -0.013795  0.000189    fc1            25  \n",
            "26  0.001260  0.002817 -0.000567 -0.018858    fc1            26  \n",
            "27 -0.012066  0.005803  0.008898 -0.020256    fc1            27  \n",
            "28 -0.021011 -0.016277 -0.021642 -0.017041    fc1            28  \n",
            "29  0.015582 -0.001358  0.009005  0.020085    fc1            29  \n",
            "30  0.009142 -0.011048 -0.019065 -0.022964    fc1            30  \n",
            "31 -0.017578  0.018184 -0.014494  0.006414    fc1            31  \n",
            "32 -0.007214 -0.020134 -0.002980 -0.009061    fc1            32  \n",
            "33 -0.002171 -0.004688  0.024596  0.012860    fc1            33  \n",
            "34  0.006539  0.009605 -0.015453 -0.016520    fc1            34  \n",
            "35 -0.018720 -0.022744 -0.017856  0.009409    fc1            35  \n",
            "36  0.040159  0.015496  0.046070  0.038217    fc1            36  \n",
            "37  0.016869  0.005396  0.012609  0.013487    fc1            37  \n",
            "38 -0.000505  0.023441  0.003526  0.005332    fc1            38  \n",
            "39 -0.019023 -0.006869 -0.015280  0.005979    fc1            39  \n",
            "40  0.027607  0.004566  0.021181  0.012148    fc1            40  \n",
            "41  0.004879 -0.012685 -0.009628 -0.022963    fc1            41  \n",
            "42  0.007932 -0.012811 -0.015902 -0.008890    fc1            42  \n",
            "43 -0.016014  0.007799  0.005429 -0.008904    fc1            43  \n",
            "44  0.004329  0.007101 -0.003699 -0.012319    fc1            44  \n",
            "45  0.005780  0.009909 -0.004622 -0.021927    fc1            45  \n",
            "46 -0.013934 -0.011455 -0.007828 -0.010529    fc1            46  \n",
            "47 -0.016764 -0.023845 -0.003573  0.011422    fc1            47  \n",
            "48 -0.015247 -0.011081 -0.016139 -0.007430    fc1            48  \n",
            "49  0.037967  0.039943  0.047666  0.032830    fc1            49  \n",
            "50  0.002823 -0.002692 -0.008353  0.002865    fc1            50  \n",
            "51 -0.014761  0.009049 -0.008702 -0.023180    fc1            51  \n",
            "52  0.023632  0.022728  0.018886  0.025511    fc1            52  \n",
            "53 -0.018722  0.011494  0.004309 -0.023839    fc1            53  \n",
            "54  0.000339  0.006231  0.007703 -0.013026    fc1            54  \n",
            "55 -0.014757 -0.008799 -0.020483 -0.005706    fc1            55  \n",
            "56 -0.014795  0.005220 -0.021989 -0.015774    fc1            56  \n",
            "57 -0.000363  0.006102 -0.002093 -0.001637    fc1            57  \n",
            "58  0.026156  0.011964  0.002414  0.006485    fc1            58  \n",
            "59 -0.003636  0.008666 -0.018831 -0.015422    fc1            59  \n",
            "\n",
            "[60 rows x 3138 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_ACTIVATIONS DataFrame:\n",
            "  layer  neuron_index  activation_value\n",
            "0   fc2             0        -10.673137\n",
            "1   fc2             1         19.558620\n",
            "2   fc2             2         -3.645514\n",
            "3   fc2             3         -9.020881\n",
            "4   fc2             4         -9.562969\n",
            "5   fc2             5        -11.331858\n",
            "6   fc2             6        -12.159142\n",
            "7   fc2             7         -5.926852\n",
            "8   fc2             8        -13.619586\n",
            "9   fc2             9          9.835047\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_CONNECTIONS DataFrame:\n",
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.107950 -0.100333 -0.125853  0.091329 -0.090712 -0.024337 -0.035301   \n",
            "1  0.131501 -0.110936 -0.085158  0.066024 -0.037845 -0.071343 -0.052721   \n",
            "2  0.050709 -0.033907 -0.077746 -0.074659 -0.076966 -0.074498 -0.088701   \n",
            "3  0.049182 -0.027318 -0.033456 -0.035291 -0.043763 -0.003068  0.013614   \n",
            "4  0.042645  0.088435  0.038246  0.004014 -0.031166 -0.035241 -0.033445   \n",
            "5  0.086447 -0.126483 -0.040570  0.021679  0.066402 -0.114365  0.069705   \n",
            "6 -0.080449  0.085518 -0.090617 -0.053715 -0.023148 -0.005910  0.053310   \n",
            "7  0.092976 -0.108292 -0.003311 -0.020869  0.013422 -0.048318  0.010532   \n",
            "8 -0.123973 -0.038942  0.098014 -0.079174  0.093177  0.066027  0.115322   \n",
            "9  0.017517 -0.083782  0.037875 -0.062554 -0.064439  0.184223  0.006078   \n",
            "\n",
            "          7         8         9  ...        52        53        54        55  \\\n",
            "0  0.090453 -0.134711  0.053470  ... -0.070573  0.083195  0.095116  0.083355   \n",
            "1 -0.112283  0.045542  0.064597  ...  0.047196  0.127483  0.055454  0.038554   \n",
            "2  0.057485 -0.012508 -0.048672  ...  0.098187  0.072752 -0.079741 -0.097680   \n",
            "3 -0.134513 -0.108175  0.041074  ... -0.005931 -0.089947 -0.010503  0.077362   \n",
            "4 -0.040576  0.073950 -0.058569  ... -0.079893 -0.038126 -0.095315  0.112316   \n",
            "5 -0.126576 -0.152112 -0.069325  ... -0.028109  0.103588 -0.003243  0.043243   \n",
            "6  0.102617  0.096054  0.003210  ... -0.036933  0.010865  0.066728  0.112904   \n",
            "7  0.093271  0.076127 -0.004166  ... -0.105497  0.043385  0.102613  0.121523   \n",
            "8  0.016770 -0.150593  0.057095  ... -0.014134 -0.041150  0.043610 -0.033362   \n",
            "9  0.090238 -0.031772 -0.016623  ... -0.092037  0.067543 -0.015439  0.107121   \n",
            "\n",
            "         56        57        58        59  layer  neuron_index  \n",
            "0 -0.068317  0.057229 -0.010008 -0.046776    fc2             0  \n",
            "1  0.154287 -0.029886 -0.088726  0.017459    fc2             1  \n",
            "2 -0.002260 -0.142917  0.077585  0.014963    fc2             2  \n",
            "3 -0.088103 -0.110321  0.010353  0.046034    fc2             3  \n",
            "4  0.099081  0.096792  0.056523 -0.063165    fc2             4  \n",
            "5 -0.133819 -0.110977 -0.123005 -0.015103    fc2             5  \n",
            "6  0.097748 -0.037395  0.050451 -0.001918    fc2             6  \n",
            "7 -0.045865 -0.148744  0.003459 -0.012868    fc2             7  \n",
            "8 -0.111366 -0.025910  0.000013  0.098205    fc2             8  \n",
            "9 -0.079098  0.062496 -0.018937  0.063364    fc2             9  \n",
            "\n",
            "[10 rows x 62 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Image 9 ---\n",
            "True Label: 1\n",
            "Predicted Label: 1\n",
            "\n",
            "CONV1_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv1             0          0.242353\n",
            "1   conv1             1          0.257860\n",
            "2   conv1             2          0.111112\n",
            "3   conv1             3          0.044749\n",
            "4   conv1             4          0.074479\n",
            "5   conv1             5          0.236658\n",
            "6   conv1             6          0.034025\n",
            "7   conv1             7          0.299289\n",
            "8   conv1             8          0.093801\n",
            "9   conv1             9          0.087742\n",
            "10  conv1            10          0.078390\n",
            "11  conv1            11          0.189710\n",
            "12  conv1            12          0.006106\n",
            "13  conv1            13          0.074195\n",
            "14  conv1            14         -0.009973\n",
            "15  conv1            15          0.012663\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.400412  0.399371  0.071231 -0.067315  0.237489  0.328836 -0.150906   \n",
            "1   0.062670  0.420839  0.423627 -0.087857  0.270959 -0.074010  0.234360   \n",
            "2   0.077322  0.239514  0.362599  0.313162 -0.159267 -0.146957 -0.197186   \n",
            "3  -0.198520  0.416571  0.189112 -0.133381  0.282581  0.187163 -0.016161   \n",
            "4  -0.100635 -0.007484  0.208964  0.262549  0.069296  0.175801  0.311726   \n",
            "5  -0.273976 -0.003054  0.224042  0.069278  0.076746  0.157825  0.075656   \n",
            "6  -0.428757 -0.251183 -0.254338  0.081380  0.144253  0.244236  0.337603   \n",
            "7   0.378913  0.399986  0.071193  0.172418 -0.026089  0.130204 -0.220100   \n",
            "8   0.225943  0.192161  0.387705  0.135396 -0.048392 -0.038235 -0.115391   \n",
            "9   0.307195 -0.180329 -0.171382  0.319853  0.081811  0.181218  0.174932   \n",
            "10  0.130797  0.370466  0.380331  0.015307 -0.078076  0.016426 -0.083861   \n",
            "11  0.392175  0.247163  0.281231  0.259225 -0.270856 -0.165327  0.253949   \n",
            "12 -0.210740 -0.048877 -0.131766  0.335698  0.006075 -0.116493 -0.017947   \n",
            "13  0.119667  0.026561  0.093274  0.343750  0.204018 -0.236069  0.372014   \n",
            "14  0.198226 -0.245182  0.121569 -0.049056 -0.133100 -0.077453  0.442164   \n",
            "15  0.209107  0.173020 -0.301732  0.125146 -0.021763 -0.264287  0.322271   \n",
            "\n",
            "           7         8  layer  neuron_index  \n",
            "0   0.392861 -0.045586  conv1             0  \n",
            "1  -0.093755  0.006661  conv1             1  \n",
            "2   0.068652  0.012842  conv1             2  \n",
            "3  -0.195803  0.062113  conv1             3  \n",
            "4  -0.130634  0.326892  conv1             4  \n",
            "5   0.073296 -0.115009  conv1             5  \n",
            "6  -0.101311  0.121107  conv1             6  \n",
            "7   0.377975 -0.272754  conv1             7  \n",
            "8   0.228945  0.188063  conv1             8  \n",
            "9   0.345546  0.172957  conv1             9  \n",
            "10  0.056517  0.259929  conv1            10  \n",
            "11 -0.198929  0.022492  conv1            11  \n",
            "12  0.285077  0.004282  conv1            12  \n",
            "13  0.204587 -0.130169  conv1            13  \n",
            "14 -0.012433 -0.308704  conv1            14  \n",
            "15 -0.069897  0.135285  conv1            15  \n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_ACTIVATIONS DataFrame:\n",
            "    layer  neuron_index  activation_value\n",
            "0   conv2             0          0.116234\n",
            "1   conv2             1          0.543552\n",
            "2   conv2             2          0.222372\n",
            "3   conv2             3          0.669037\n",
            "4   conv2             4         -0.193032\n",
            "5   conv2             5         -0.252856\n",
            "6   conv2             6          0.400091\n",
            "7   conv2             7          0.312833\n",
            "8   conv2             8          0.180553\n",
            "9   conv2             9          0.071155\n",
            "10  conv2            10          0.586583\n",
            "11  conv2            11          0.213461\n",
            "12  conv2            12          0.341269\n",
            "13  conv2            13          0.021500\n",
            "14  conv2            14          0.355413\n",
            "15  conv2            15          0.232849\n",
            "\n",
            "==================================================\n",
            "\n",
            "CONV2_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.053389  0.023715 -0.082655 -0.048249 -0.076035 -0.150770  0.021789   \n",
            "1   0.047589 -0.093007  0.005160  0.069936  0.136776  0.046387  0.010860   \n",
            "2   0.058461  0.100847 -0.038368  0.022629  0.103998 -0.064163 -0.030390   \n",
            "3  -0.035365  0.023469  0.107092  0.101842  0.143467  0.217515 -0.012316   \n",
            "4   0.062249 -0.131436 -0.046978  0.021597 -0.080085 -0.078712 -0.103583   \n",
            "5  -0.019309 -0.080785  0.035847  0.020273  0.009665  0.035786 -0.006190   \n",
            "6   0.049547  0.038741 -0.036449 -0.002058  0.041931  0.053819 -0.005733   \n",
            "7  -0.152914 -0.041600  0.115807 -0.041148 -0.039541  0.071635 -0.011336   \n",
            "8   0.125563  0.033356 -0.048423  0.110720 -0.018201 -0.147717 -0.042072   \n",
            "9  -0.068213 -0.115001 -0.004743 -0.028494 -0.027653  0.079632  0.049930   \n",
            "10 -0.032338 -0.013617  0.151200 -0.075678  0.075957  0.207855  0.038870   \n",
            "11  0.051104  0.012186  0.086417  0.028352  0.156211 -0.002386  0.086014   \n",
            "12  0.082138  0.005539 -0.044524  0.132945  0.106715  0.022446  0.116672   \n",
            "13 -0.043048 -0.140184  0.012286 -0.060239 -0.119047  0.023106 -0.004832   \n",
            "14 -0.021613  0.105474  0.025686  0.030302  0.136904 -0.020385 -0.046524   \n",
            "15  0.117869  0.126556  0.051337  0.002796 -0.029835 -0.061113  0.004300   \n",
            "\n",
            "           7         8         9  ...       136       137       138       139  \\\n",
            "0   0.087136  0.136641  0.125749  ...  0.105212  0.008460  0.086248 -0.005925   \n",
            "1   0.068729 -0.004232 -0.004889  ...  0.075859  0.051621  0.013238  0.045813   \n",
            "2  -0.050265  0.004671  0.000319  ...  0.094657  0.007707  0.022765  0.004527   \n",
            "3   0.084479  0.127317 -0.104037  ...  0.000802  0.089539 -0.032844  0.050453   \n",
            "4   0.048199  0.034387 -0.074277  ...  0.045809 -0.082850  0.027167 -0.046521   \n",
            "5  -0.040837 -0.020941  0.074329  ... -0.027615 -0.036336 -0.055786  0.008235   \n",
            "6  -0.068649 -0.066126 -0.024237  ...  0.095926  0.090875 -0.025141  0.018905   \n",
            "7   0.034141  0.078109 -0.036853  ...  0.086062 -0.007312  0.005437  0.073091   \n",
            "8   0.026514 -0.127686 -0.001961  ...  0.017386 -0.016188  0.018640  0.010524   \n",
            "9  -0.042845  0.092674  0.078472  ... -0.005325  0.106815  0.014020 -0.074019   \n",
            "10 -0.053325  0.046839  0.006819  ...  0.041650  0.170038 -0.044293  0.004112   \n",
            "11  0.042763 -0.056389 -0.043120  ...  0.042914 -0.032490  0.009025  0.004950   \n",
            "12  0.132660 -0.116526 -0.137380  ... -0.008512 -0.055690  0.034679  0.085180   \n",
            "13 -0.028790  0.080668 -0.064381  ... -0.020423  0.075944 -0.070604 -0.053503   \n",
            "14  0.006731 -0.044295  0.048296  ... -0.026215  0.104971  0.072772  0.041096   \n",
            "15  0.013864 -0.005772  0.182691  ... -0.027957 -0.073938 -0.020258 -0.038572   \n",
            "\n",
            "         140       141       142       143  layer  neuron_index  \n",
            "0  -0.033853  0.113257 -0.040305  0.088088  conv2             0  \n",
            "1   0.091032  0.005281 -0.026384  0.078543  conv2             1  \n",
            "2   0.002996  0.085109  0.008654  0.067731  conv2             2  \n",
            "3  -0.007134  0.030011 -0.008236 -0.030027  conv2             3  \n",
            "4   0.067527 -0.041539  0.057837  0.032542  conv2             4  \n",
            "5  -0.075360 -0.063744  0.026530  0.010399  conv2             5  \n",
            "6   0.031970 -0.055053  0.091992 -0.061894  conv2             6  \n",
            "7   0.030808 -0.011768  0.093748  0.089395  conv2             7  \n",
            "8  -0.089284  0.094356  0.033373  0.012280  conv2             8  \n",
            "9   0.011533 -0.078723  0.016492 -0.013510  conv2             9  \n",
            "10  0.017894 -0.050105  0.031401  0.045740  conv2            10  \n",
            "11  0.000354  0.074826  0.052800  0.083592  conv2            11  \n",
            "12  0.014633  0.005773  0.066127 -0.082251  conv2            12  \n",
            "13  0.020353 -0.001231  0.079536  0.032849  conv2            13  \n",
            "14  0.005841  0.030748 -0.013547 -0.054549  conv2            14  \n",
            "15 -0.075074  0.053019 -0.002690 -0.082208  conv2            15  \n",
            "\n",
            "[16 rows x 146 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_ACTIVATIONS DataFrame:\n",
            "   layer  neuron_index  activation_value\n",
            "0    fc1             0         -3.182595\n",
            "1    fc1             1         -2.509684\n",
            "2    fc1             2         -7.511284\n",
            "3    fc1             3         -2.924870\n",
            "4    fc1             4         -3.367198\n",
            "5    fc1             5         -0.540793\n",
            "6    fc1             6         -3.408646\n",
            "7    fc1             7         -0.264224\n",
            "8    fc1             8         -0.821266\n",
            "9    fc1             9          8.802488\n",
            "10   fc1            10         -2.881606\n",
            "11   fc1            11         -3.125971\n",
            "12   fc1            12         12.712928\n",
            "13   fc1            13         22.991764\n",
            "14   fc1            14         -3.590734\n",
            "15   fc1            15         -8.205577\n",
            "16   fc1            16         -3.857293\n",
            "17   fc1            17         -2.890127\n",
            "18   fc1            18          2.834705\n",
            "19   fc1            19          1.864277\n",
            "20   fc1            20         16.561907\n",
            "21   fc1            21         -6.338756\n",
            "22   fc1            22         -5.163738\n",
            "23   fc1            23         10.575979\n",
            "24   fc1            24         -3.650862\n",
            "25   fc1            25         -2.776977\n",
            "26   fc1            26         -3.043094\n",
            "27   fc1            27         -1.770322\n",
            "28   fc1            28         -3.770077\n",
            "29   fc1            29         12.339962\n",
            "30   fc1            30         -3.675580\n",
            "31   fc1            31         -5.561275\n",
            "32   fc1            32         -4.829978\n",
            "33   fc1            33         12.020888\n",
            "34   fc1            34         -3.542069\n",
            "35   fc1            35         11.802756\n",
            "36   fc1            36         27.175959\n",
            "37   fc1            37         15.711160\n",
            "38   fc1            38          8.263906\n",
            "39   fc1            39         -7.857522\n",
            "40   fc1            40         -0.581159\n",
            "41   fc1            41         -3.313338\n",
            "42   fc1            42         -2.851951\n",
            "43   fc1            43         -2.781403\n",
            "44   fc1            44         -3.282030\n",
            "45   fc1            45         -3.633728\n",
            "46   fc1            46         -2.732232\n",
            "47   fc1            47         -2.242489\n",
            "48   fc1            48         -2.378112\n",
            "49   fc1            49         23.830463\n",
            "50   fc1            50          6.878926\n",
            "51   fc1            51         -1.703607\n",
            "52   fc1            52         14.652704\n",
            "53   fc1            53         -2.584326\n",
            "54   fc1            54         -2.220841\n",
            "55   fc1            55         -4.857000\n",
            "56   fc1            56         -2.216338\n",
            "57   fc1            57          2.577182\n",
            "58   fc1            58         -5.947222\n",
            "59   fc1            59         -3.776191\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC1_CONNECTIONS DataFrame:\n",
            "           0         1         2         3         4         5         6  \\\n",
            "0   0.012868  0.012967 -0.004651  0.004214  0.004450 -0.004286 -0.006892   \n",
            "1  -0.006668  0.001739  0.008233 -0.015734 -0.018150  0.006245 -0.009190   \n",
            "2  -0.004715 -0.008510 -0.027038 -0.012099 -0.017072 -0.001276 -0.004790   \n",
            "3  -0.022022  0.006158 -0.000070  0.005327 -0.010999 -0.013339 -0.000802   \n",
            "4  -0.021747  0.005062 -0.007141 -0.002509  0.002286 -0.004356 -0.013751   \n",
            "5   0.000488  0.012703  0.014533  0.016341 -0.004459  0.019696  0.002125   \n",
            "6   0.009300 -0.022666 -0.019488  0.000888 -0.000055 -0.011650 -0.015693   \n",
            "7   0.031482  0.014408  0.003062  0.002343  0.027174  0.003222  0.007536   \n",
            "8  -0.021455 -0.018184  0.010642 -0.009969 -0.002964 -0.022868  0.002633   \n",
            "9   0.008664  0.005595  0.018707  0.036951  0.012074  0.024569  0.035212   \n",
            "10 -0.015157 -0.015687 -0.020529  0.011342  0.001704 -0.014001 -0.023534   \n",
            "11  0.033904  0.009318  0.023171  0.003949  0.003715  0.000735  0.023219   \n",
            "12  0.024346  0.032305  0.026967  0.025876  0.010165  0.022697  0.011940   \n",
            "13  0.008851  0.010857  0.002167  0.024771  0.005659  0.021011  0.000578   \n",
            "14 -0.009209 -0.003457 -0.008076 -0.003801  0.010076 -0.013077 -0.018169   \n",
            "15 -0.022892 -0.007363 -0.001998 -0.025108 -0.005332 -0.008478 -0.006340   \n",
            "16 -0.011208 -0.008056 -0.020657 -0.002330  0.002901 -0.009348 -0.019556   \n",
            "17 -0.004303 -0.011543 -0.022747 -0.002148 -0.000974  0.003379  0.001518   \n",
            "18  0.014590  0.013593  0.026992  0.014942  0.002147  0.024546  0.023784   \n",
            "19  0.013653  0.015041  0.026944  0.017540  0.012362  0.005006  0.020771   \n",
            "20  0.000926 -0.002540  0.000976 -0.007426  0.008134  0.009121  0.006932   \n",
            "21 -0.008481 -0.009680 -0.015762 -0.008213  0.005569 -0.011419 -0.018695   \n",
            "22  0.002687 -0.004754  0.025822  0.022829  0.026687  0.024950  0.015143   \n",
            "23  0.012675 -0.008788  0.020707 -0.002437 -0.000562 -0.001121  0.009253   \n",
            "24 -0.021873 -0.006459  0.000207 -0.022165  0.008027 -0.006639  0.011209   \n",
            "25 -0.009614 -0.022043 -0.007360  0.004037 -0.017339 -0.001730 -0.005209   \n",
            "26 -0.003378 -0.006826 -0.019353 -0.008899  0.007771  0.007703 -0.008105   \n",
            "27 -0.009939 -0.008868  0.000785  0.008521 -0.017249  0.009543 -0.006277   \n",
            "28 -0.012323  0.002773 -0.002832  0.008380 -0.001049 -0.023035 -0.010383   \n",
            "29  0.014561  0.016938  0.017683 -0.001824  0.010139  0.014729  0.012067   \n",
            "30 -0.019001 -0.009498 -0.020953 -0.004602  0.011816  0.005326 -0.018519   \n",
            "31  0.000243  0.000025  0.021961 -0.006239  0.018972 -0.004205  0.002377   \n",
            "32 -0.003725 -0.003355 -0.020392  0.005503 -0.013929 -0.013021  0.001984   \n",
            "33  0.016569  0.002955  0.006740  0.025918  0.010518  0.006316  0.009447   \n",
            "34 -0.021455 -0.010343  0.011155  0.004942 -0.004358 -0.012782 -0.010427   \n",
            "35 -0.004436 -0.022906 -0.004501  0.005599  0.007464 -0.000397 -0.000361   \n",
            "36  0.028495  0.008049  0.033686  0.027099  0.001744  0.033556  0.005032   \n",
            "37  0.017607  0.014472  0.003794  0.007778  0.026253  0.026272  0.000011   \n",
            "38 -0.002645  0.017745  0.021209 -0.002706  0.004354  0.021500  0.015463   \n",
            "39  0.013660  0.008610  0.018474  0.003934  0.033713  0.001638  0.016512   \n",
            "40  0.030918  0.000622  0.009956  0.019433  0.000461 -0.000907  0.013701   \n",
            "41 -0.014202 -0.010007 -0.013808 -0.019121  0.001727 -0.010824  0.006965   \n",
            "42 -0.018572 -0.000763  0.011354 -0.020072 -0.000430  0.000737 -0.016534   \n",
            "43 -0.016530 -0.011671 -0.013214 -0.002635  0.008195 -0.009454 -0.014988   \n",
            "44 -0.004162  0.004954 -0.016971 -0.016766 -0.005005 -0.002056 -0.009664   \n",
            "45 -0.006126  0.000294 -0.010289 -0.013374 -0.021398 -0.016979 -0.005077   \n",
            "46 -0.003019  0.003975 -0.013475  0.004994 -0.012704 -0.002466 -0.018315   \n",
            "47  0.002745  0.005883 -0.017230  0.000825 -0.021160 -0.008062 -0.022425   \n",
            "48 -0.000349 -0.019200  0.004244  0.012784  0.002108 -0.011295 -0.014164   \n",
            "49  0.019498  0.027400  0.033523  0.031643  0.003375  0.024539  0.018732   \n",
            "50  0.002034  0.025831  0.021195  0.015002  0.011998  0.006490  0.015592   \n",
            "51  0.002681 -0.004163 -0.002470  0.002025  0.000847 -0.004836 -0.020047   \n",
            "52  0.031499  0.006817  0.030368  0.016483  0.009225  0.011744  0.023579   \n",
            "53 -0.002231 -0.016763 -0.012501  0.002132 -0.016919  0.000812  0.009893   \n",
            "54 -0.016136  0.002641  0.006465  0.005333  0.000611 -0.021958  0.005562   \n",
            "55  0.006558  0.001421 -0.006817 -0.021912 -0.021949 -0.012012  0.008985   \n",
            "56 -0.013326  0.005539 -0.001117  0.010829  0.002428 -0.019694 -0.017891   \n",
            "57  0.000068  0.003597  0.006211  0.004677 -0.000560  0.023743  0.012332   \n",
            "58  0.028027  0.022394  0.015621  0.022584  0.027930  0.013303  0.023036   \n",
            "59 -0.016706  0.001549  0.002360 -0.005833 -0.013946 -0.005872 -0.015406   \n",
            "\n",
            "           7         8         9  ...      3128      3129      3130      3131  \\\n",
            "0   0.007324 -0.009616 -0.005067  ... -0.006856  0.010652  0.010737 -0.005627   \n",
            "1  -0.018485 -0.011243 -0.014907  ...  0.003302 -0.023178 -0.002841 -0.002705   \n",
            "2   0.003348 -0.021924  0.002252  ... -0.004555 -0.022496 -0.027291  0.000060   \n",
            "3  -0.006648 -0.017124  0.011142  ... -0.008438  0.007751 -0.003895 -0.014379   \n",
            "4   0.009772 -0.011616 -0.003061  ... -0.013893 -0.023509  0.007052  0.009660   \n",
            "5   0.008894  0.007604  0.022646  ...  0.023266  0.041666  0.029488  0.033174   \n",
            "6  -0.003789  0.009797 -0.006847  ...  0.009045 -0.017039 -0.017816  0.009705   \n",
            "7   0.004008  0.015966  0.020602  ...  0.009192  0.034439  0.014094  0.028504   \n",
            "8  -0.003198 -0.022776  0.003764  ... -0.001452 -0.006950 -0.022771  0.003933   \n",
            "9   0.030930  0.004664  0.003770  ...  0.004832 -0.009257 -0.008188  0.001435   \n",
            "10 -0.013386 -0.013077 -0.018894  ... -0.001648  0.000762 -0.017888 -0.016899   \n",
            "11  0.006491  0.032536  0.000740  ...  0.003196  0.004032  0.010177  0.012247   \n",
            "12  0.025155  0.015860  0.000769  ...  0.006513  0.015600  0.012484  0.016776   \n",
            "13  0.029623  0.026839 -0.001954  ...  0.036832  0.027887  0.015517  0.010580   \n",
            "14 -0.016337  0.004727  0.010331  ... -0.012719 -0.016295  0.002682 -0.014291   \n",
            "15 -0.017671 -0.001590 -0.007524  ... -0.027042 -0.015744 -0.009734 -0.021105   \n",
            "16 -0.015554  0.001033  0.005761  ...  0.009729 -0.022332 -0.021350 -0.016759   \n",
            "17  0.010020 -0.019688 -0.004400  ...  0.007721 -0.004402 -0.016819 -0.015915   \n",
            "18  0.002784 -0.006211  0.013223  ...  0.024819  0.008977  0.035165  0.001899   \n",
            "19  0.014684  0.027412  0.018771  ...  0.027893  0.023726  0.017920  0.016699   \n",
            "20 -0.019416 -0.022032 -0.013255  ...  0.047160  0.036355  0.025181  0.022490   \n",
            "21  0.001027 -0.005196 -0.004424  ... -0.011853 -0.003937  0.003567  0.000304   \n",
            "22  0.004678  0.017792 -0.000148  ...  0.014265  0.008728 -0.017063  0.005836   \n",
            "23  0.005949 -0.008079  0.000984  ... -0.013547 -0.005859  0.002466  0.003944   \n",
            "24 -0.012757 -0.023823 -0.018663  ...  0.011678 -0.019435  0.003086 -0.019769   \n",
            "25 -0.010399  0.010380 -0.019753  ...  0.000844  0.010552 -0.019871 -0.019154   \n",
            "26  0.010319 -0.010890  0.007982  ...  0.000887  0.000956  0.001107 -0.014635   \n",
            "27 -0.004731 -0.009723 -0.003905  ...  0.005012 -0.016505 -0.011187 -0.001365   \n",
            "28 -0.015085 -0.001597 -0.002212  ... -0.009379  0.010932  0.000835  0.011187   \n",
            "29  0.009418  0.025151  0.020776  ... -0.004506 -0.015641 -0.013120 -0.010891   \n",
            "30 -0.016916  0.009760 -0.014054  ...  0.007682 -0.003761 -0.015353 -0.000961   \n",
            "31  0.012016 -0.000764 -0.007299  ... -0.012403 -0.009862 -0.011390 -0.003945   \n",
            "32 -0.024166  0.000700  0.009655  ... -0.019903 -0.006628 -0.020596  0.001379   \n",
            "33  0.010933  0.003280  0.013948  ... -0.003015  0.020623 -0.000906  0.015280   \n",
            "34 -0.017208  0.006274  0.003570  ... -0.013828 -0.008093 -0.019012 -0.001721   \n",
            "35  0.005101 -0.010963 -0.009127  ... -0.002773  0.011193  0.019547  0.002818   \n",
            "36  0.012425  0.009884  0.031366  ...  0.042560  0.039935  0.023204  0.029524   \n",
            "37  0.023419  0.006894  0.026621  ...  0.004115  0.000018  0.020470 -0.002871   \n",
            "38  0.018896  0.024307 -0.000390  ... -0.004216 -0.012399 -0.001571 -0.018753   \n",
            "39  0.010963  0.030405  0.032920  ... -0.013158  0.001995 -0.017575  0.000686   \n",
            "40  0.026295  0.013387 -0.001529  ...  0.043029  0.038009  0.020855  0.017207   \n",
            "41 -0.004680 -0.013764 -0.007863  ...  0.008035  0.010515 -0.006477  0.011474   \n",
            "42 -0.000678  0.009945  0.003108  ...  0.008813  0.006511 -0.001635 -0.017173   \n",
            "43 -0.011972 -0.016392  0.001932  ... -0.010337 -0.012674 -0.018367 -0.022919   \n",
            "44  0.003259 -0.000585  0.001807  ...  0.008413 -0.014572 -0.008983  0.005895   \n",
            "45  0.006896 -0.008086 -0.009275  ... -0.005778  0.007000  0.002406 -0.008275   \n",
            "46  0.010720 -0.002166 -0.014401  ... -0.013205 -0.022235 -0.002017 -0.000171   \n",
            "47 -0.005961 -0.003954  0.010139  ...  0.011701  0.001282 -0.018512 -0.003118   \n",
            "48 -0.013545  0.006565 -0.005001  ... -0.010218  0.010282 -0.019655  0.010408   \n",
            "49  0.018647  0.026510  0.016471  ...  0.039909  0.049060  0.028912  0.045582   \n",
            "50  0.000300  0.010269  0.008490  ...  0.023184 -0.001352  0.019045  0.003499   \n",
            "51  0.004892  0.007555  0.001219  ... -0.016053  0.001421 -0.013407 -0.006220   \n",
            "52  0.030600  0.026217  0.023260  ...  0.017529  0.006934  0.017406  0.020949   \n",
            "53 -0.010024 -0.009774 -0.018796  ... -0.015735 -0.020443  0.008829 -0.016565   \n",
            "54  0.011039 -0.000971 -0.004749  ... -0.011213  0.000786 -0.019802 -0.001971   \n",
            "55 -0.017711 -0.011881  0.008669  ...  0.003574  0.005007 -0.005511 -0.009074   \n",
            "56 -0.018805 -0.010618  0.002681  ... -0.014544 -0.013959  0.007968 -0.004889   \n",
            "57  0.013836  0.000618  0.024202  ...  0.020163  0.034249  0.023702  0.032346   \n",
            "58  0.020271 -0.000923  0.004961  ...  0.030553  0.023524  0.025896  0.012225   \n",
            "59  0.006506 -0.000810  0.000023  ... -0.021418 -0.022299 -0.008617 -0.017246   \n",
            "\n",
            "        3132      3133      3134      3135  layer  neuron_index  \n",
            "0  -0.006300  0.000761 -0.010077 -0.010894    fc1             0  \n",
            "1   0.007173  0.003588 -0.009852 -0.016465    fc1             1  \n",
            "2   0.005837 -0.015450 -0.013901  0.002791    fc1             2  \n",
            "3  -0.012918  0.007607 -0.013114 -0.016117    fc1             3  \n",
            "4   0.003922 -0.010699  0.009847 -0.021692    fc1             4  \n",
            "5   0.028678  0.014802  0.011530  0.006059    fc1             5  \n",
            "6  -0.014932  0.004915  0.002246 -0.007906    fc1             6  \n",
            "7   0.013501  0.022247  0.021532 -0.003630    fc1             7  \n",
            "8  -0.006297 -0.015515 -0.013881  0.004368    fc1             8  \n",
            "9  -0.001923  0.008458  0.021580  0.029945    fc1             9  \n",
            "10 -0.000172 -0.004697 -0.007486  0.003054    fc1            10  \n",
            "11 -0.007124 -0.002595 -0.013320 -0.004145    fc1            11  \n",
            "12 -0.001142  0.025493  0.015860  0.012417    fc1            12  \n",
            "13  0.008905  0.014851  0.010457  0.007798    fc1            13  \n",
            "14 -0.010744 -0.008016 -0.019082 -0.007001    fc1            14  \n",
            "15 -0.008899 -0.000829 -0.016586 -0.028002    fc1            15  \n",
            "16  0.011897 -0.018047 -0.002446 -0.017222    fc1            16  \n",
            "17 -0.020536  0.006682 -0.007786 -0.019112    fc1            17  \n",
            "18  0.014848  0.023633  0.009814 -0.009907    fc1            18  \n",
            "19  0.032436  0.014681  0.007802  0.015888    fc1            19  \n",
            "20  0.010047  0.026064  0.016482  0.030144    fc1            20  \n",
            "21  0.008994  0.010006 -0.018727 -0.003075    fc1            21  \n",
            "22 -0.000216 -0.005094  0.018505 -0.008893    fc1            22  \n",
            "23  0.027010  0.004446 -0.004685  0.011521    fc1            23  \n",
            "24 -0.006039  0.005686 -0.007776  0.007923    fc1            24  \n",
            "25 -0.013028 -0.005355 -0.013795  0.000189    fc1            25  \n",
            "26  0.001260  0.002817 -0.000567 -0.018858    fc1            26  \n",
            "27 -0.012066  0.005803  0.008898 -0.020256    fc1            27  \n",
            "28 -0.021011 -0.016277 -0.021642 -0.017041    fc1            28  \n",
            "29  0.015582 -0.001358  0.009005  0.020085    fc1            29  \n",
            "30  0.009142 -0.011048 -0.019065 -0.022964    fc1            30  \n",
            "31 -0.017578  0.018184 -0.014494  0.006414    fc1            31  \n",
            "32 -0.007214 -0.020134 -0.002980 -0.009061    fc1            32  \n",
            "33 -0.002171 -0.004688  0.024596  0.012860    fc1            33  \n",
            "34  0.006539  0.009605 -0.015453 -0.016520    fc1            34  \n",
            "35 -0.018720 -0.022744 -0.017856  0.009409    fc1            35  \n",
            "36  0.040159  0.015496  0.046070  0.038217    fc1            36  \n",
            "37  0.016869  0.005396  0.012609  0.013487    fc1            37  \n",
            "38 -0.000505  0.023441  0.003526  0.005332    fc1            38  \n",
            "39 -0.019023 -0.006869 -0.015280  0.005979    fc1            39  \n",
            "40  0.027607  0.004566  0.021181  0.012148    fc1            40  \n",
            "41  0.004879 -0.012685 -0.009628 -0.022963    fc1            41  \n",
            "42  0.007932 -0.012811 -0.015902 -0.008890    fc1            42  \n",
            "43 -0.016014  0.007799  0.005429 -0.008904    fc1            43  \n",
            "44  0.004329  0.007101 -0.003699 -0.012319    fc1            44  \n",
            "45  0.005780  0.009909 -0.004622 -0.021927    fc1            45  \n",
            "46 -0.013934 -0.011455 -0.007828 -0.010529    fc1            46  \n",
            "47 -0.016764 -0.023845 -0.003573  0.011422    fc1            47  \n",
            "48 -0.015247 -0.011081 -0.016139 -0.007430    fc1            48  \n",
            "49  0.037967  0.039943  0.047666  0.032830    fc1            49  \n",
            "50  0.002823 -0.002692 -0.008353  0.002865    fc1            50  \n",
            "51 -0.014761  0.009049 -0.008702 -0.023180    fc1            51  \n",
            "52  0.023632  0.022728  0.018886  0.025511    fc1            52  \n",
            "53 -0.018722  0.011494  0.004309 -0.023839    fc1            53  \n",
            "54  0.000339  0.006231  0.007703 -0.013026    fc1            54  \n",
            "55 -0.014757 -0.008799 -0.020483 -0.005706    fc1            55  \n",
            "56 -0.014795  0.005220 -0.021989 -0.015774    fc1            56  \n",
            "57 -0.000363  0.006102 -0.002093 -0.001637    fc1            57  \n",
            "58  0.026156  0.011964  0.002414  0.006485    fc1            58  \n",
            "59 -0.003636  0.008666 -0.018831 -0.015422    fc1            59  \n",
            "\n",
            "[60 rows x 3138 columns]\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_ACTIVATIONS DataFrame:\n",
            "  layer  neuron_index  activation_value\n",
            "0   fc2             0         -8.335674\n",
            "1   fc2             1         15.028265\n",
            "2   fc2             2         -2.172051\n",
            "3   fc2             3         -7.107998\n",
            "4   fc2             4         -7.123154\n",
            "5   fc2             5         -8.994482\n",
            "6   fc2             6         -9.348644\n",
            "7   fc2             7         -4.188916\n",
            "8   fc2             8        -10.835687\n",
            "9   fc2             9          7.258918\n",
            "\n",
            "==================================================\n",
            "\n",
            "FC2_CONNECTIONS DataFrame:\n",
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.107950 -0.100333 -0.125853  0.091329 -0.090712 -0.024337 -0.035301   \n",
            "1  0.131501 -0.110936 -0.085158  0.066024 -0.037845 -0.071343 -0.052721   \n",
            "2  0.050709 -0.033907 -0.077746 -0.074659 -0.076966 -0.074498 -0.088701   \n",
            "3  0.049182 -0.027318 -0.033456 -0.035291 -0.043763 -0.003068  0.013614   \n",
            "4  0.042645  0.088435  0.038246  0.004014 -0.031166 -0.035241 -0.033445   \n",
            "5  0.086447 -0.126483 -0.040570  0.021679  0.066402 -0.114365  0.069705   \n",
            "6 -0.080449  0.085518 -0.090617 -0.053715 -0.023148 -0.005910  0.053310   \n",
            "7  0.092976 -0.108292 -0.003311 -0.020869  0.013422 -0.048318  0.010532   \n",
            "8 -0.123973 -0.038942  0.098014 -0.079174  0.093177  0.066027  0.115322   \n",
            "9  0.017517 -0.083782  0.037875 -0.062554 -0.064439  0.184223  0.006078   \n",
            "\n",
            "          7         8         9  ...        52        53        54        55  \\\n",
            "0  0.090453 -0.134711  0.053470  ... -0.070573  0.083195  0.095116  0.083355   \n",
            "1 -0.112283  0.045542  0.064597  ...  0.047196  0.127483  0.055454  0.038554   \n",
            "2  0.057485 -0.012508 -0.048672  ...  0.098187  0.072752 -0.079741 -0.097680   \n",
            "3 -0.134513 -0.108175  0.041074  ... -0.005931 -0.089947 -0.010503  0.077362   \n",
            "4 -0.040576  0.073950 -0.058569  ... -0.079893 -0.038126 -0.095315  0.112316   \n",
            "5 -0.126576 -0.152112 -0.069325  ... -0.028109  0.103588 -0.003243  0.043243   \n",
            "6  0.102617  0.096054  0.003210  ... -0.036933  0.010865  0.066728  0.112904   \n",
            "7  0.093271  0.076127 -0.004166  ... -0.105497  0.043385  0.102613  0.121523   \n",
            "8  0.016770 -0.150593  0.057095  ... -0.014134 -0.041150  0.043610 -0.033362   \n",
            "9  0.090238 -0.031772 -0.016623  ... -0.092037  0.067543 -0.015439  0.107121   \n",
            "\n",
            "         56        57        58        59  layer  neuron_index  \n",
            "0 -0.068317  0.057229 -0.010008 -0.046776    fc2             0  \n",
            "1  0.154287 -0.029886 -0.088726  0.017459    fc2             1  \n",
            "2 -0.002260 -0.142917  0.077585  0.014963    fc2             2  \n",
            "3 -0.088103 -0.110321  0.010353  0.046034    fc2             3  \n",
            "4  0.099081  0.096792  0.056523 -0.063165    fc2             4  \n",
            "5 -0.133819 -0.110977 -0.123005 -0.015103    fc2             5  \n",
            "6  0.097748 -0.037395  0.050451 -0.001918    fc2             6  \n",
            "7 -0.045865 -0.148744  0.003459 -0.012868    fc2             7  \n",
            "8 -0.111366 -0.025910  0.000013  0.098205    fc2             8  \n",
            "9 -0.079098  0.062496 -0.018937  0.063364    fc2             9  \n",
            "\n",
            "[10 rows x 62 columns]\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2U9usCXsfLDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network graphs"
      ],
      "metadata": {
        "id": "puM39SfNdpzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualizar_red_neuronal_interactiva(per_image_neuron_data, imagen_index=0):\n",
        "    \"\"\"\n",
        "    Crea una visualización interactiva de la red neuronal con Plotly\n",
        "\n",
        "    Args:\n",
        "        per_image_neuron_data: Datos de activaciones y conexiones\n",
        "        imagen_index: Índice de la imagen a visualizar\n",
        "\n",
        "    Returns:\n",
        "        figura de plotly interactiva\n",
        "    \"\"\"\n",
        "    # Obtener datos de la imagen\n",
        "    imagen_data = per_image_neuron_data[imagen_index]\n",
        "    dfs = imagen_data['dataframes']\n",
        "\n",
        "    # Crear grafo dirigido\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Estructura de capas\n",
        "    capas = ['input', 'conv1', 'conv2', 'fc1', 'fc2', 'output']\n",
        "    layer_positions = {capa: i for i, capa in enumerate(capas)}\n",
        "\n",
        "    # Número de neuronas por capa (completo)\n",
        "    neuron_counts = {\n",
        "        'input': 1,\n",
        "        'conv1': len(dfs['conv1_activations']),\n",
        "        'conv2': len(dfs['conv2_activations']),\n",
        "        'fc1': len(dfs['fc1_activations']),\n",
        "        'fc2': len(dfs['fc2_activations']),\n",
        "        'output': 1\n",
        "    }\n",
        "\n",
        "    # Posiciones de los nodos\n",
        "    pos = {}\n",
        "    node_colors = {}\n",
        "    node_sizes = {}\n",
        "    node_texts = {}\n",
        "\n",
        "    # Añadir nodos para cada capa\n",
        "    # Nodo de entrada\n",
        "    G.add_node('input')\n",
        "    pos['input'] = (0, 0)\n",
        "    node_colors['input'] = 'rgba(200, 200, 200, 0.8)'\n",
        "    node_sizes['input'] = 25\n",
        "    node_texts['input'] = f\"Imagen de entrada\"\n",
        "\n",
        "    # Nodos para capas ocultas\n",
        "    for capa in ['conv1', 'conv2', 'fc1', 'fc2']:\n",
        "        # Obtener número de neuronas\n",
        "        num_neuronas = neuron_counts[capa]\n",
        "\n",
        "        # Añadir cada neurona\n",
        "        for i in range(num_neuronas):\n",
        "            nodo_id = f\"{capa}_{i}\"\n",
        "            G.add_node(nodo_id)\n",
        "\n",
        "            # Calcular posición vertical (distribuir uniformemente)\n",
        "            y_pos = (i - (num_neuronas - 1) / 2) * (10 / max(1, num_neuronas))\n",
        "            pos[nodo_id] = (layer_positions[capa], y_pos)\n",
        "\n",
        "            # Obtener valor de activación\n",
        "            activacion = dfs[f'{capa}_activations'].iloc[i]['activation_value']\n",
        "\n",
        "            # Color basado en activación (positiva: azul, negativa: rojo, cercana a cero: gris)\n",
        "            if activacion > 0.1:\n",
        "                intensidad = min(255, int(100 + 155 * activacion))\n",
        "                node_colors[nodo_id] = f'rgba(0, 0, {intensidad}, 0.8)'\n",
        "            elif activacion < -0.1:\n",
        "                intensidad = min(255, int(100 + 155 * abs(activacion)))\n",
        "                node_colors[nodo_id] = f'rgba({intensidad}, 0, 0, 0.8)'\n",
        "            else:\n",
        "                node_colors[nodo_id] = f'rgba(150, 150, 150, 0.8)'\n",
        "\n",
        "            # Tamaño basado en la magnitud de la activación\n",
        "            node_sizes[nodo_id] = 10 + 15 * abs(activacion)\n",
        "\n",
        "            # Texto al pasar el ratón\n",
        "            node_texts[nodo_id] = f\"{capa} neurona {i}<br>Activación: {activacion:.4f}\"\n",
        "\n",
        "    # Nodo de salida\n",
        "    G.add_node('output')\n",
        "    pos['output'] = (layer_positions['output'], 0)\n",
        "    node_colors['output'] = 'rgba(0, 200, 0, 0.8)'\n",
        "    node_sizes['output'] = 25\n",
        "    node_texts['output'] = f\"Clase predicha: {imagen_data['predicted_label']}\"\n",
        "\n",
        "    # Añadir conexiones entre capas\n",
        "    edge_x = []\n",
        "    edge_y = []\n",
        "    edge_colors = []\n",
        "    edge_widths = []\n",
        "    edge_texts = []\n",
        "\n",
        "    # Conectar input a conv1\n",
        "    for i in range(neuron_counts['conv1']):\n",
        "        G.add_edge('input', f'conv1_{i}')\n",
        "        x0, y0 = pos['input']\n",
        "        x1, y1 = pos[f'conv1_{i}']\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "        edge_colors.append('rgba(100, 100, 100, 0.3)')\n",
        "        edge_widths.append(1)\n",
        "        edge_texts.append(\"Conexión entrada\")\n",
        "\n",
        "    # Conectar capas ocultas entre sí\n",
        "    layer_pairs = [('conv1', 'conv2'), ('conv2', 'fc1'), ('fc1', 'fc2')]\n",
        "    for capa1, capa2 in layer_pairs:\n",
        "        # Limitar conexiones para visualización\n",
        "        max_display = min(3000, neuron_counts[capa1] * neuron_counts[capa2])\n",
        "        connections_shown = 0\n",
        "\n",
        "        for i in range(min(neuron_counts[capa1], 20)):\n",
        "            for j in range(min(neuron_counts[capa2], 20)):\n",
        "                if connections_shown >= max_display:\n",
        "                    break\n",
        "\n",
        "                # Intentar obtener peso de la conexión\n",
        "                try:\n",
        "                    peso = dfs[f'{capa1}_connections'].iloc[i][j]\n",
        "\n",
        "                    # Solo mostrar conexiones significativas\n",
        "                    if abs(peso) > 0.001:\n",
        "                        G.add_edge(f\"{capa1}_{i}\", f\"{capa2}_{j}\")\n",
        "\n",
        "                        x0, y0 = pos[f\"{capa1}_{i}\"]\n",
        "                        x1, y1 = pos[f\"{capa2}_{j}\"]\n",
        "\n",
        "                        edge_x.extend([x0, x1, None])\n",
        "                        edge_y.extend([y0, y1, None])\n",
        "\n",
        "                        # Color basado en el peso (positivo: azul, negativo: rojo)\n",
        "                        if peso > 0:\n",
        "                            edge_colors.append(f'rgba(0, 0, 255, {min(0.8, 0.1 + abs(peso))})')\n",
        "                        else:\n",
        "                            edge_colors.append(f'rgba(255, 0, 0, {min(0.8, 0.1 + abs(peso))})')\n",
        "\n",
        "                        # Grosor basado en la magnitud del peso\n",
        "                        edge_widths.append(max(0.5, min(3, abs(peso) * 3)))\n",
        "\n",
        "                        # Texto al pasar el ratón\n",
        "                        edge_texts.append(f\"Peso: {peso:.4f}\")\n",
        "\n",
        "                        connections_shown += 1\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            if connections_shown >= max_display:\n",
        "                break\n",
        "\n",
        "    # Conectar fc2 a output\n",
        "    for i in range(neuron_counts['fc2']):\n",
        "        G.add_edge(f'fc2_{i}', 'output')\n",
        "        x0, y0 = pos[f'fc2_{i}']\n",
        "        x1, y1 = pos['output']\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "\n",
        "        # Intentar obtener peso para la clase predicha\n",
        "        try:\n",
        "            peso = dfs['fc2_connections'].iloc[i][imagen_data['predicted_label']]\n",
        "            if peso > 0:\n",
        "                edge_colors.append(f'rgba(0, 0, 255, {min(0.8, 0.3 + abs(peso))})')\n",
        "            else:\n",
        "                edge_colors.append(f'rgba(255, 0, 0, {min(0.8, 0.3 + abs(peso))})')\n",
        "            edge_widths.append(max(1, min(4, abs(peso) * 3)))\n",
        "            edge_texts.append(f\"Peso: {peso:.4f}\")\n",
        "        except:\n",
        "            edge_colors.append('rgba(100, 100, 100, 0.3)')\n",
        "            edge_widths.append(1)\n",
        "            edge_texts.append(\"Conexión a salida\")\n",
        "\n",
        "    # Create trazas para los bordes\n",
        "    edge_trace = go.Scatter(\n",
        "        x=edge_x, y=edge_y,\n",
        "        hoverinfo='text',\n",
        "        mode='lines',\n",
        "        line=dict(width=0.5),  # Remove color here\n",
        "        marker=dict(color=edge_colors, size=edge_widths),  # Add colors and widths here\n",
        "        text=edge_texts\n",
        "    )\n",
        "\n",
        "    # Crear trazas para los nodos\n",
        "    node_x = []\n",
        "    node_y = []\n",
        "    node_color = []\n",
        "    node_size = []\n",
        "    node_text = []\n",
        "\n",
        "    for node in G.nodes():\n",
        "        x, y = pos[node]\n",
        "        node_x.append(x)\n",
        "        node_y.append(y)\n",
        "        node_color.append(node_colors[node])\n",
        "        node_size.append(node_sizes[node])\n",
        "        node_text.append(node_texts[node])\n",
        "\n",
        "    node_trace = go.Scatter(\n",
        "        x=node_x, y=node_y,\n",
        "        mode='markers',\n",
        "        hoverinfo='text',\n",
        "        marker=dict(\n",
        "            showscale=False,\n",
        "            color=node_color,\n",
        "            size=node_size,\n",
        "            line_width=2,\n",
        "            line=dict(color='white')\n",
        "        ),\n",
        "        text=node_text\n",
        "    )\n",
        "\n",
        "    # Añadir etiquetas de capas\n",
        "    layer_annotations = []\n",
        "    for capa, x_pos in layer_positions.items():\n",
        "        layer_annotations.append(\n",
        "            dict(\n",
        "                x=x_pos,\n",
        "                y=5,  # Posición encima de la red\n",
        "                xref='x',\n",
        "                yref='y',\n",
        "                text=f\"{capa.upper()}<br>({neuron_counts[capa]} neuronas)\",\n",
        "                showarrow=False,\n",
        "                font=dict(size=14, color='black')\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Crear figura\n",
        "    fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                    layout=go.Layout(\n",
        "                        title=f\"Red Neuronal - Imagen {imagen_index}<br>Etiqueta real: {imagen_data['true_label']}, Predicción: {imagen_data['predicted_label']}\",\n",
        "                        titlefont_size=16,\n",
        "                        showlegend=False,\n",
        "                        hovermode='closest',\n",
        "                        margin=dict(b=20, l=5, r=5, t=40),\n",
        "                        annotations=layer_annotations,\n",
        "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                        height=1000,\n",
        "                        width=1600\n",
        "                    ))\n",
        "\n",
        "    # Añadir leyenda explicativa\n",
        "    fig.add_annotation(\n",
        "        x=1.02, y=0.5,\n",
        "        xref='paper', yref='paper',\n",
        "        text=\"<b>Leyenda:</b><br><br>\" +\n",
        "             \"<b>Color de nodo:</b><br>\" +\n",
        "             \"• <span style='color:blue'>Azul</span>: Activación positiva<br>\" +\n",
        "             \"• <span style='color:red'>Rojo</span>: Activación negativa<br>\" +\n",
        "             \"• <span style='color:gray'>Gris</span>: Activación cercana a cero<br><br>\" +\n",
        "             \"<b>Tamaño de nodo:</b><br>\" +\n",
        "             \"• Mayor tamaño = Mayor magnitud de activación<br><br>\" +\n",
        "             \"<b>Color de conexión:</b><br>\" +\n",
        "             \"• <span style='color:blue'>Azul</span>: Peso positivo<br>\" +\n",
        "             \"• <span style='color:red'>Rojo</span>: Peso negativo<br><br>\" +\n",
        "             \"<b>Grosor de conexión:</b><br>\" +\n",
        "             \"• Mayor grosor = Mayor magnitud del peso\",\n",
        "        showarrow=False,\n",
        "        align='left'\n",
        "    )\n",
        "\n",
        "    # Añadir explicación de activaciones negativas\n",
        "    fig.add_annotation(\n",
        "        x=1.02, y=0.05,\n",
        "        xref='paper', yref='paper',\n",
        "        text=\"<b>¿Por qué hay activaciones negativas?</b><br><br>\" +\n",
        "             \"En redes neuronales, las activaciones negativas<br>\" +\n",
        "             \"ocurren cuando el input a una neurona produce<br>\" +\n",
        "             \"un valor negativo. Esto es común en capas con<br>\" +\n",
        "             \"funciones de activación como ReLU, tanh o<br>\" +\n",
        "             \"funciones lineales. Las activaciones negativas<br>\" +\n",
        "             \"pueden indicar inhibición o respuesta contraria<br>\" +\n",
        "             \"a ciertas características de entrada.\",\n",
        "        showarrow=False,\n",
        "        align='left'\n",
        "    )\n",
        "\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "5ow77Q_051ia"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = visualizar_red_neuronal_interactiva(per_image_neuron_data, imagen_index=5)\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fotx-3UlU2_T",
        "outputId": "6a638b79-158d-476d-a343-e33db5402a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f7d0e9c6-25de-45df-8723-98c0d574a9b5\" class=\"plotly-graph-div\" style=\"height:1000px; width:1600px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f7d0e9c6-25de-45df-8723-98c0d574a9b5\")) {                    Plotly.newPlot(                        \"f7d0e9c6-25de-45df-8723-98c0d574a9b5\",                        [{\"hoverinfo\":\"text\",\"line\":{\"width\":0.5},\"marker\":{\"color\":[\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(0, 0, 255, 0.49548879861831663)\",\"rgba(0, 0, 255, 0.478199428319931)\",\"rgba(0, 0, 255, 0.14466232284903527)\",\"rgba(255, 0, 0, 0.177683687210083)\",\"rgba(0, 0, 255, 0.3098765522241592)\",\"rgba(0, 0, 255, 0.3973027229309082)\",\"rgba(255, 0, 0, 0.2523659825325012)\",\"rgba(0, 0, 255, 0.47543243169784544)\",\"rgba(255, 0, 0, 0.15969223380088807)\",\"rgba(0, 0, 255, 0.1734786093235016)\",\"rgba(0, 0, 255, 0.5192788004875183)\",\"rgba(0, 0, 255, 0.5156277775764465)\",\"rgba(255, 0, 0, 0.17516806572675706)\",\"rgba(0, 0, 255, 0.3722387909889221)\",\"rgba(255, 0, 0, 0.17676905393600464)\",\"rgba(0, 0, 255, 0.35481775403022764)\",\"rgba(255, 0, 0, 0.18479847759008408)\",\"rgba(0, 0, 255, 0.11256333477795125)\",\"rgba(0, 0, 255, 0.17643774151802064)\",\"rgba(0, 0, 255, 0.32657302021980283)\",\"rgba(0, 0, 255, 0.44209170937538145)\",\"rgba(0, 0, 255, 0.41036651134490965)\",\"rgba(255, 0, 0, 0.2747374832630157)\",\"rgba(255, 0, 0, 0.2693179607391357)\",\"rgba(255, 0, 0, 0.2957087069749832)\",\"rgba(0, 0, 255, 0.16043727323412896)\",\"rgba(0, 0, 255, 0.10588635914027691)\",\"rgba(255, 0, 0, 0.2954487860202789)\",\"rgba(0, 0, 255, 0.5079583287239074)\",\"rgba(0, 0, 255, 0.2742736846208572)\",\"rgba(255, 0, 0, 0.22870168685913086)\",\"rgba(0, 0, 255, 0.37615121006965635)\",\"rgba(0, 0, 255, 0.2752258390188217)\",\"rgba(255, 0, 0, 0.10932643283158541)\",\"rgba(255, 0, 0, 0.299082151055336)\",\"rgba(0, 0, 255, 0.1544593557715416)\",\"rgba(255, 0, 0, 0.1878574475646019)\",\"rgba(255, 0, 0, 0.10299682095646859)\",\"rgba(0, 0, 255, 0.31407847106456754)\",\"rgba(0, 0, 255, 0.38076297044754026)\",\"rgba(0, 0, 255, 0.17920287549495698)\",\"rgba(0, 0, 255, 0.2862309873104095)\",\"rgba(0, 0, 255, 0.4426393866539001)\",\"rgba(255, 0, 0, 0.21012519896030427)\",\"rgba(0, 0, 255, 0.4459571897983551)\",\"rgba(255, 0, 0, 0.37126166224479673)\",\"rgba(255, 0, 0, 0.10629299692809582)\",\"rgba(0, 0, 255, 0.3153266370296478)\",\"rgba(0, 0, 255, 0.17139802724123002)\",\"rgba(0, 0, 255, 0.1725584849715233)\",\"rgba(0, 0, 255, 0.24965689182281495)\",\"rgba(0, 0, 255, 0.1810677632689476)\",\"rgba(0, 0, 255, 0.1727474093437195)\",\"rgba(255, 0, 0, 0.2172143742442131)\",\"rgba(255, 0, 0, 0.5470112025737762)\",\"rgba(255, 0, 0, 0.35271883606910703)\",\"rgba(255, 0, 0, 0.3498561441898346)\",\"rgba(0, 0, 255, 0.1630682334303856)\",\"rgba(0, 0, 255, 0.23590829074382783)\",\"rgba(0, 0, 255, 0.3403503060340881)\",\"rgba(0, 0, 255, 0.4222856521606445)\",\"rgba(255, 0, 0, 0.21231402307748795)\",\"rgba(0, 0, 255, 0.2133197695016861)\",\"rgba(0, 0, 255, 0.47085214257240293)\",\"rgba(0, 0, 255, 0.48222280144691465)\",\"rgba(0, 0, 255, 0.15390509217977524)\",\"rgba(0, 0, 255, 0.2591764390468597)\",\"rgba(255, 0, 0, 0.1489599294960499)\",\"rgba(0, 0, 255, 0.20857946425676346)\",\"rgba(255, 0, 0, 0.3309042364358902)\",\"rgba(0, 0, 255, 0.45697722434997556)\",\"rgba(255, 0, 0, 0.38950235247612)\",\"rgba(0, 0, 255, 0.3325397729873657)\",\"rgba(0, 0, 255, 0.27854142189025877)\",\"rgba(0, 0, 255, 0.46781996488571165)\",\"rgba(0, 0, 255, 0.23787814974784852)\",\"rgba(255, 0, 0, 0.1643598809838295)\",\"rgba(255, 0, 0, 0.15681707710027695)\",\"rgba(255, 0, 0, 0.2021089032292366)\",\"rgba(0, 0, 255, 0.3239493280649185)\",\"rgba(0, 0, 255, 0.2816739737987518)\",\"rgba(0, 0, 255, 0.41357854604721067)\",\"rgba(255, 0, 0, 0.2777995079755783)\",\"rgba(255, 0, 0, 0.2666818857192993)\",\"rgba(0, 0, 255, 0.42318800687789915)\",\"rgba(0, 0, 255, 0.18088799566030503)\",\"rgba(0, 0, 255, 0.2810082197189331)\",\"rgba(0, 0, 255, 0.28109431862831114)\",\"rgba(0, 0, 255, 0.4472943007946014)\",\"rgba(0, 0, 255, 0.27397598922252653)\",\"rgba(0, 0, 255, 0.24490857720375062)\",\"rgba(0, 0, 255, 0.46724478006362913)\",\"rgba(0, 0, 255, 0.4688847422599792)\",\"rgba(0, 0, 255, 0.13124589286744595)\",\"rgba(255, 0, 0, 0.187206169962883)\",\"rgba(255, 0, 0, 0.1550176203250885)\",\"rgba(0, 0, 255, 0.15971722677350045)\",\"rgba(0, 0, 255, 0.3590250968933105)\",\"rgba(0, 0, 255, 0.48915732502937315)\",\"rgba(0, 0, 255, 0.34438098073005674)\",\"rgba(0, 0, 255, 0.38259721398353574)\",\"rgba(0, 0, 255, 0.3597565948963165)\",\"rgba(255, 0, 0, 0.36958218216896055)\",\"rgba(255, 0, 0, 0.2608051061630249)\",\"rgba(0, 0, 255, 0.36007843017578123)\",\"rgba(255, 0, 0, 0.29213166832923887)\",\"rgba(0, 0, 255, 0.13488203138113022)\",\"rgba(255, 0, 0, 0.3189231365919113)\",\"rgba(255, 0, 0, 0.1583360865712166)\",\"rgba(255, 0, 0, 0.23929422199726105)\",\"rgba(0, 0, 255, 0.4234710395336151)\",\"rgba(255, 0, 0, 0.10799361150711775)\",\"rgba(255, 0, 0, 0.22437988817691804)\",\"rgba(255, 0, 0, 0.13250041976571084)\",\"rgba(0, 0, 255, 0.36733183860778806)\",\"rgba(255, 0, 0, 0.10533227529376746)\",\"rgba(0, 0, 255, 0.23108427822589875)\",\"rgba(0, 0, 255, 0.1269213728606701)\",\"rgba(0, 0, 255, 0.2178092733025551)\",\"rgba(0, 0, 255, 0.4475667834281921)\",\"rgba(0, 0, 255, 0.29243766367435453)\",\"rgba(255, 0, 0, 0.3232967615127563)\",\"rgba(0, 0, 255, 0.48328558206558225)\",\"rgba(0, 0, 255, 0.2952555626630783)\",\"rgba(255, 0, 0, 0.22201957404613495)\",\"rgba(0, 0, 255, 0.291690519452095)\",\"rgba(255, 0, 0, 0.3410115748643875)\",\"rgba(0, 0, 255, 0.23403376936912537)\",\"rgba(255, 0, 0, 0.1590900182723999)\",\"rgba(255, 0, 0, 0.23071313500404358)\",\"rgba(255, 0, 0, 0.16546325236558915)\",\"rgba(0, 0, 255, 0.5442004561424255)\",\"rgba(255, 0, 0, 0.10763260200619698)\",\"rgba(255, 0, 0, 0.3982511222362518)\",\"rgba(0, 0, 255, 0.3256711423397064)\",\"rgba(0, 0, 255, 0.28056963384151457)\",\"rgba(255, 0, 0, 0.3958565354347229)\",\"rgba(0, 0, 255, 0.23396245241165162)\",\"rgba(255, 0, 0, 0.11965508796274663)\",\"rgba(255, 0, 0, 0.36107057929039)\",\"rgba(0, 0, 255, 0.4305442631244659)\",\"rgba(255, 0, 0, 0.16671704351902009)\",\"rgba(0, 0, 255, 0.24037953317165375)\",\"rgba(0, 0, 255, 0.15226198583841324)\",\"rgba(0, 0, 255, 0.1386566072702408)\",\"rgba(255, 0, 0, 0.13595873713493348)\",\"rgba(255, 0, 0, 0.15670680478215218)\",\"rgba(255, 0, 0, 0.1724240615963936)\",\"rgba(255, 0, 0, 0.21617275327444077)\",\"rgba(0, 0, 255, 0.1054623312316835)\",\"rgba(0, 0, 255, 0.17781460136175156)\",\"rgba(0, 0, 255, 0.2550832390785217)\",\"rgba(0, 0, 255, 0.23532580435276032)\",\"rgba(0, 0, 255, 0.1271420180797577)\",\"rgba(0, 0, 255, 0.25483386814594267)\",\"rgba(0, 0, 255, 0.11958501152694226)\",\"rgba(255, 0, 0, 0.17505299150943757)\",\"rgba(255, 0, 0, 0.20065092146396638)\",\"rgba(255, 0, 0, 0.14416592419147492)\",\"rgba(255, 0, 0, 0.19682178497314454)\",\"rgba(255, 0, 0, 0.13787001967430115)\",\"rgba(0, 0, 255, 0.1209137424826622)\",\"rgba(0, 0, 255, 0.11684178784489632)\",\"rgba(0, 0, 255, 0.15851465463638306)\",\"rgba(255, 0, 0, 0.18223507553339005)\",\"rgba(0, 0, 255, 0.11689410023391247)\",\"rgba(0, 0, 255, 0.17545654475688935)\",\"rgba(0, 0, 255, 0.24069390296936036)\",\"rgba(0, 0, 255, 0.15251357331871987)\",\"rgba(0, 0, 255, 0.11536910142749549)\",\"rgba(0, 0, 255, 0.17129435986280442)\",\"rgba(0, 0, 255, 0.10941847078502179)\",\"rgba(255, 0, 0, 0.1767335817217827)\",\"rgba(0, 0, 255, 0.13535399213433266)\",\"rgba(0, 0, 255, 0.15336353927850724)\",\"rgba(0, 0, 255, 0.20378362983465195)\",\"rgba(0, 0, 255, 0.12714175544679165)\",\"rgba(0, 0, 255, 0.16998100876808167)\",\"rgba(0, 0, 255, 0.2746469140052795)\",\"rgba(0, 0, 255, 0.27548803985118864)\",\"rgba(0, 0, 255, 0.16393295526504517)\",\"rgba(255, 0, 0, 0.15806545540690423)\",\"rgba(0, 0, 255, 0.16643111258745194)\",\"rgba(0, 0, 255, 0.2083114892244339)\",\"rgba(255, 0, 0, 0.13128596022725106)\",\"rgba(0, 0, 255, 0.12906168811023236)\",\"rgba(0, 0, 255, 0.20891366004943848)\",\"rgba(255, 0, 0, 0.15957501381635666)\",\"rgba(255, 0, 0, 0.12259227074682713)\",\"rgba(255, 0, 0, 0.14415588453412057)\",\"rgba(0, 0, 255, 0.10972395390272141)\",\"rgba(0, 0, 255, 0.11184686925262213)\",\"rgba(255, 0, 0, 0.15321859195828438)\",\"rgba(0, 0, 255, 0.22908414006233216)\",\"rgba(0, 0, 255, 0.22326332926750184)\",\"rgba(0, 0, 255, 0.19815044701099396)\",\"rgba(255, 0, 0, 0.11162238139659167)\",\"rgba(0, 0, 255, 0.186253322660923)\",\"rgba(255, 0, 0, 0.14448411986231804)\",\"rgba(255, 0, 0, 0.1661178305745125)\",\"rgba(0, 0, 255, 0.20666831582784653)\",\"rgba(255, 0, 0, 0.12844959124922753)\",\"rgba(0, 0, 255, 0.11097518485039473)\",\"rgba(0, 0, 255, 0.1838935896754265)\",\"rgba(0, 0, 255, 0.19256912022829056)\",\"rgba(0, 0, 255, 0.21605315655469895)\",\"rgba(0, 0, 255, 0.2842900365591049)\",\"rgba(255, 0, 0, 0.12351627871394158)\",\"rgba(0, 0, 255, 0.15942964926362038)\",\"rgba(0, 0, 255, 0.19926258772611619)\",\"rgba(255, 0, 0, 0.1916508510708809)\",\"rgba(255, 0, 0, 0.1020223087631166)\",\"rgba(255, 0, 0, 0.12769693918526173)\",\"rgba(0, 0, 255, 0.12617848478257657)\",\"rgba(0, 0, 255, 0.2083066835999489)\",\"rgba(0, 0, 255, 0.2719566285610199)\",\"rgba(0, 0, 255, 0.24234213531017304)\",\"rgba(0, 0, 255, 0.28349374830722807)\",\"rgba(0, 0, 255, 0.1305425487458706)\",\"rgba(0, 0, 255, 0.11249254308640957)\",\"rgba(255, 0, 0, 0.1090659698471427)\",\"rgba(0, 0, 255, 0.1655781701207161)\",\"rgba(255, 0, 0, 0.21959099024534226)\",\"rgba(255, 0, 0, 0.13270576521754265)\",\"rgba(0, 0, 255, 0.1260148234665394)\",\"rgba(255, 0, 0, 0.16755132228136063)\",\"rgba(255, 0, 0, 0.16150597333908082)\",\"rgba(255, 0, 0, 0.19079410880804062)\",\"rgba(0, 0, 255, 0.16053490936756135)\",\"rgba(0, 0, 255, 0.1525706559419632)\",\"rgba(255, 0, 0, 0.16704683601856232)\",\"rgba(255, 0, 0, 0.1014490872854367)\",\"rgba(255, 0, 0, 0.1155973332002759)\",\"rgba(0, 0, 255, 0.12969427965581418)\",\"rgba(255, 0, 0, 0.19690987318754197)\",\"rgba(255, 0, 0, 0.1365249253809452)\",\"rgba(0, 0, 255, 0.16420420110225678)\",\"rgba(255, 0, 0, 0.12144586481153966)\",\"rgba(255, 0, 0, 0.19393659234046937)\",\"rgba(255, 0, 0, 0.10180329836439342)\",\"rgba(0, 0, 255, 0.15597561225295067)\",\"rgba(255, 0, 0, 0.11922614499926568)\",\"rgba(255, 0, 0, 0.1807197540998459)\",\"rgba(0, 0, 255, 0.13591635078191758)\",\"rgba(0, 0, 255, 0.12025427483022214)\",\"rgba(0, 0, 255, 0.10969479233026505)\",\"rgba(0, 0, 255, 0.13583026677370072)\",\"rgba(255, 0, 0, 0.10631299614906312)\",\"rgba(255, 0, 0, 0.14084687680006028)\",\"rgba(255, 0, 0, 0.12094217017292977)\",\"rgba(0, 0, 255, 0.17337053120136262)\",\"rgba(255, 0, 0, 0.12403955534100533)\",\"rgba(0, 0, 255, 0.10931183528155089)\",\"rgba(255, 0, 0, 0.1337904989719391)\",\"rgba(255, 0, 0, 0.14871316999197007)\",\"rgba(255, 0, 0, 0.13775223344564438)\",\"rgba(255, 0, 0, 0.15456002801656724)\",\"rgba(255, 0, 0, 0.15006642267107964)\",\"rgba(255, 0, 0, 0.11680560037493706)\",\"rgba(0, 0, 255, 0.1573251247406006)\",\"rgba(0, 0, 255, 0.15939288064837456)\",\"rgba(0, 0, 255, 0.16052596867084504)\",\"rgba(0, 0, 255, 0.15456433445215226)\",\"rgba(255, 0, 0, 0.12262217178940774)\",\"rgba(0, 0, 255, 0.11019854284822941)\",\"rgba(0, 0, 255, 0.15847012251615525)\",\"rgba(0, 0, 255, 0.16794290244579316)\",\"rgba(0, 0, 255, 0.10782784912735224)\",\"rgba(255, 0, 0, 0.15122115463018418)\",\"rgba(255, 0, 0, 0.15147096887230874)\",\"rgba(255, 0, 0, 0.11417598221451045)\",\"rgba(0, 0, 255, 0.19837329387664795)\",\"rgba(255, 0, 0, 0.12020259201526642)\",\"rgba(0, 0, 255, 0.2163897141814232)\",\"rgba(0, 0, 255, 0.20436511784791947)\",\"rgba(0, 0, 255, 0.21039837449789048)\",\"rgba(0, 0, 255, 0.2790644973516464)\",\"rgba(0, 0, 255, 0.22842857837677003)\",\"rgba(0, 0, 255, 0.25524275600910185)\",\"rgba(0, 0, 255, 0.19176838397979737)\",\"rgba(255, 0, 0, 0.11521802134811879)\",\"rgba(255, 0, 0, 0.24807364344596863)\",\"rgba(255, 0, 0, 0.14195474460721016)\",\"rgba(0, 0, 255, 0.21409225314855576)\",\"rgba(255, 0, 0, 0.1633474662899971)\",\"rgba(255, 0, 0, 0.16295283883810044)\",\"rgba(0, 0, 255, 0.14685645550489426)\",\"rgba(255, 0, 0, 0.14034655913710595)\",\"rgba(0, 0, 255, 0.10358967785723508)\",\"rgba(0, 0, 255, 0.1459873117506504)\",\"rgba(255, 0, 0, 0.12523482404649258)\",\"rgba(0, 0, 255, 0.15648961290717125)\",\"rgba(0, 0, 255, 0.1803705006837845)\",\"rgba(255, 0, 0, 0.22341751158237458)\",\"rgba(0, 0, 255, 0.17330399751663209)\",\"rgba(0, 0, 255, 0.11614801585674286)\",\"rgba(0, 0, 255, 0.13756815567612649)\",\"rgba(0, 0, 255, 0.1353468768298626)\",\"rgba(0, 0, 255, 0.12318404242396355)\",\"rgba(255, 0, 0, 0.1239439569413662)\",\"rgba(255, 0, 0, 0.15918757244944573)\",\"rgba(0, 0, 255, 0.24578624069690705)\",\"rgba(0, 0, 255, 0.15151745304465294)\",\"rgba(255, 0, 0, 0.12972494624555111)\",\"rgba(0, 0, 255, 0.2308891475200653)\",\"rgba(255, 0, 0, 0.10197818316519261)\",\"rgba(255, 0, 0, 0.23131056725978852)\",\"rgba(255, 0, 0, 0.12178245782852173)\",\"rgba(0, 0, 255, 0.14231640323996544)\",\"rgba(255, 0, 0, 0.21269105225801468)\",\"rgba(0, 0, 255, 0.11979728788137436)\",\"rgba(255, 0, 0, 0.11434784382581711)\",\"rgba(0, 0, 255, 0.17585982978343964)\",\"rgba(0, 0, 255, 0.2864547878503799)\",\"rgba(255, 0, 0, 0.19370731562376023)\",\"rgba(255, 0, 0, 0.11561188604682684)\",\"rgba(0, 0, 255, 0.17931382954120637)\",\"rgba(255, 0, 0, 0.18723654597997666)\",\"rgba(0, 0, 255, 0.1310098957270384)\",\"rgba(255, 0, 0, 0.10726854288950563)\",\"rgba(0, 0, 255, 0.12043144963681698)\",\"rgba(255, 0, 0, 0.14788494110107422)\",\"rgba(255, 0, 0, 0.19651190787553788)\",\"rgba(0, 0, 255, 0.10894980225712061)\",\"rgba(255, 0, 0, 0.11024332363158465)\",\"rgba(255, 0, 0, 0.1113961324095726)\",\"rgba(0, 0, 255, 0.19189245104789734)\",\"rgba(0, 0, 255, 0.1650133341550827)\",\"rgba(255, 0, 0, 0.12992463670670987)\",\"rgba(0, 0, 255, 0.20241249203681946)\",\"rgba(0, 0, 255, 0.1984674021601677)\",\"rgba(0, 0, 255, 0.13412004932761193)\",\"rgba(0, 0, 255, 0.16683219224214554)\",\"rgba(255, 0, 0, 0.12224020659923554)\",\"rgba(0, 0, 255, 0.12131464891135693)\",\"rgba(255, 0, 0, 0.12754815481603146)\",\"rgba(255, 0, 0, 0.21309650540351868)\",\"rgba(255, 0, 0, 0.1770358145236969)\",\"rgba(0, 0, 255, 0.11529336255043746)\",\"rgba(0, 0, 255, 0.20691694170236588)\",\"rgba(255, 0, 0, 0.14987320676445962)\",\"rgba(255, 0, 0, 0.1559365823864937)\",\"rgba(255, 0, 0, 0.1450836792588234)\",\"rgba(0, 0, 255, 0.22001517415046692)\",\"rgba(255, 0, 0, 0.20618361085653306)\",\"rgba(0, 0, 255, 0.14627396166324616)\",\"rgba(0, 0, 255, 0.28120065331459043)\",\"rgba(0, 0, 255, 0.11846693083643914)\",\"rgba(255, 0, 0, 0.17231620699167252)\",\"rgba(0, 0, 255, 0.13356506302952767)\",\"rgba(0, 0, 255, 0.10218037003651262)\",\"rgba(0, 0, 255, 0.21720247566699982)\",\"rgba(0, 0, 255, 0.1685992017388344)\",\"rgba(255, 0, 0, 0.14049494117498398)\",\"rgba(0, 0, 255, 0.19640442579984665)\",\"rgba(0, 0, 255, 0.2749678999185562)\",\"rgba(0, 0, 255, 0.10281565403565765)\",\"rgba(0, 0, 255, 0.10951499957591296)\",\"rgba(255, 0, 0, 0.11713052950799466)\",\"rgba(0, 0, 255, 0.13773346543312073)\",\"rgba(255, 0, 0, 0.1716003179550171)\",\"rgba(0, 0, 255, 0.17127328962087632)\",\"rgba(0, 0, 255, 0.12467096224427224)\",\"rgba(0, 0, 255, 0.19485531598329545)\",\"rgba(0, 0, 255, 0.14595022648572922)\",\"rgba(0, 0, 255, 0.2692528277635574)\",\"rgba(0, 0, 255, 0.10690880464389921)\",\"rgba(0, 0, 255, 0.20661641508340836)\",\"rgba(0, 0, 255, 0.15907241627573968)\",\"rgba(255, 0, 0, 0.14439767450094224)\",\"rgba(255, 0, 0, 0.1238613747060299)\",\"rgba(255, 0, 0, 0.19980181604623795)\",\"rgba(255, 0, 0, 0.14902878031134606)\",\"rgba(0, 0, 255, 0.14211461916565896)\",\"rgba(0, 0, 255, 0.256250849366188)\",\"rgba(0, 0, 255, 0.14176847636699677)\",\"rgba(0, 0, 255, 0.262353903055191)\",\"rgba(0, 0, 255, 0.18315620571374894)\",\"rgba(0, 0, 255, 0.12715127542614937)\",\"rgba(255, 0, 0, 0.11525964587926865)\",\"rgba(0, 0, 255, 0.1643814578652382)\",\"rgba(0, 0, 255, 0.19453471750020981)\",\"rgba(0, 0, 255, 0.11423598732799292)\",\"rgba(255, 0, 0, 0.1350361578166485)\",\"rgba(0, 0, 255, 0.24258751273155213)\",\"rgba(0, 0, 255, 0.21224818229675294)\",\"rgba(0, 0, 255, 0.12961379289627076)\",\"rgba(0, 0, 255, 0.2269052028656006)\",\"rgba(0, 0, 255, 0.23853234052658082)\",\"rgba(255, 0, 0, 0.20922084748744965)\",\"rgba(255, 0, 0, 0.22195508182048798)\",\"rgba(255, 0, 0, 0.21937642097473145)\",\"rgba(255, 0, 0, 0.18703076094388962)\",\"rgba(0, 0, 255, 0.19272335469722748)\",\"rgba(255, 0, 0, 0.2073676198720932)\",\"rgba(255, 0, 0, 0.11221138704568148)\",\"rgba(0, 0, 255, 0.20201732069253922)\",\"rgba(0, 0, 255, 0.15784472450613976)\",\"rgba(0, 0, 255, 0.13575390949845315)\",\"rgba(0, 0, 255, 0.17514524459838868)\",\"rgba(0, 0, 255, 0.10388252851553262)\",\"rgba(255, 0, 0, 0.11220337487757207)\",\"rgba(255, 0, 0, 0.20671365261077881)\",\"rgba(0, 0, 255, 0.14328499361872674)\",\"rgba(255, 0, 0, 0.12937498688697815)\",\"rgba(255, 0, 0, 0.1867907926440239)\",\"rgba(0, 0, 255, 0.1518893264234066)\",\"rgba(0, 0, 255, 0.12568084746599198)\",\"rgba(0, 0, 255, 0.10187984190415592)\",\"rgba(0, 0, 255, 0.20666269063949586)\",\"rgba(255, 0, 0, 0.1337924286723137)\",\"rgba(255, 0, 0, 0.12738125808537007)\",\"rgba(0, 0, 255, 0.23599146902561188)\",\"rgba(255, 0, 0, 0.1201748013496399)\",\"rgba(0, 0, 255, 0.1866667553782463)\",\"rgba(0, 0, 255, 0.16635120511054993)\",\"rgba(255, 0, 0, 0.12595003321766854)\",\"rgba(0, 0, 255, 0.11274861209094525)\",\"rgba(255, 0, 0, 0.1086600873619318)\",\"rgba(0, 0, 255, 0.17839718461036683)\",\"rgba(0, 0, 255, 0.14725469276309014)\",\"rgba(255, 0, 0, 0.10861964654177428)\",\"rgba(0, 0, 255, 0.21814913898706437)\",\"rgba(0, 0, 255, 0.1377427414059639)\",\"rgba(0, 0, 255, 0.13983725234866143)\",\"rgba(0, 0, 255, 0.24521875977516175)\",\"rgba(255, 0, 0, 0.11230627484619618)\",\"rgba(255, 0, 0, 0.1379784442484379)\",\"rgba(0, 0, 255, 0.11385879050940276)\",\"rgba(255, 0, 0, 0.1376894883811474)\",\"rgba(0, 0, 255, 0.16396060436964036)\",\"rgba(255, 0, 0, 0.19228187799453736)\",\"rgba(255, 0, 0, 0.16415797024965287)\",\"rgba(0, 0, 255, 0.15122511833906174)\",\"rgba(0, 0, 255, 0.2187347322702408)\",\"rgba(0, 0, 255, 0.15166426301002503)\",\"rgba(0, 0, 255, 0.2489807516336441)\",\"rgba(0, 0, 255, 0.11504863426089287)\",\"rgba(0, 0, 255, 0.1371341183781624)\",\"rgba(0, 0, 255, 0.18508042246103287)\",\"rgba(0, 0, 255, 0.18222902566194535)\",\"rgba(0, 0, 255, 0.22881997227668763)\",\"rgba(0, 0, 255, 0.23717131316661835)\",\"rgba(0, 0, 255, 0.16285112351179123)\",\"rgba(0, 0, 255, 0.11251801289618016)\",\"rgba(255, 0, 0, 0.12101685851812363)\",\"rgba(255, 0, 0, 0.15124419182538987)\",\"rgba(0, 0, 255, 0.1134767796844244)\",\"rgba(0, 0, 255, 0.12203266695141793)\",\"rgba(0, 0, 255, 0.10303762708790601)\",\"rgba(0, 0, 255, 0.2956202834844589)\",\"rgba(0, 0, 255, 0.14800493642687798)\",\"rgba(0, 0, 255, 0.17198162376880646)\",\"rgba(0, 0, 255, 0.12344769425690175)\",\"rgba(0, 0, 255, 0.10664820661768318)\",\"rgba(255, 0, 0, 0.10932208448648453)\",\"rgba(0, 0, 255, 0.11176549028605223)\",\"rgba(255, 0, 0, 0.1951811671257019)\",\"rgba(255, 0, 0, 0.23855357468128205)\",\"rgba(0, 0, 255, 0.19646214544773102)\",\"rgba(0, 0, 255, 0.23631613552570344)\",\"rgba(255, 0, 0, 0.13550074622035027)\",\"rgba(255, 0, 0, 0.13415663540363312)\",\"rgba(0, 0, 255, 0.10178810534998775)\",\"rgba(255, 0, 0, 0.10528963981196285)\",\"rgba(0, 0, 255, 0.11088422257453204)\",\"rgba(255, 0, 0, 0.11812226139009)\",\"rgba(255, 0, 0, 0.12855682745575905)\",\"rgba(255, 0, 0, 0.10194849295075983)\",\"rgba(0, 0, 255, 0.11335254777222872)\",\"rgba(255, 0, 0, 0.10477193295955659)\",\"rgba(0, 0, 255, 0.13525649756193162)\",\"rgba(255, 0, 0, 0.10138341111596674)\",\"rgba(255, 0, 0, 0.11898531429469586)\",\"rgba(255, 0, 0, 0.10669446168467403)\",\"rgba(0, 0, 255, 0.11909543946385384)\",\"rgba(0, 0, 255, 0.11284026727080346)\",\"rgba(0, 0, 255, 0.12494340389966965)\",\"rgba(255, 0, 0, 0.13423922508955002)\",\"rgba(0, 0, 255, 0.10875162612646819)\",\"rgba(0, 0, 255, 0.12378135360777379)\",\"rgba(0, 0, 255, 0.11301876362413169)\",\"rgba(0, 0, 255, 0.1392383858561516)\",\"rgba(255, 0, 0, 0.12818730287253857)\",\"rgba(255, 0, 0, 0.10952464435249568)\",\"rgba(255, 0, 0, 0.1302299577742815)\",\"rgba(0, 0, 255, 0.11377327833324671)\",\"rgba(255, 0, 0, 0.11041976548731328)\",\"rgba(255, 0, 0, 0.13126677870750428)\",\"rgba(0, 0, 255, 0.13708266839385033)\",\"rgba(0, 0, 255, 0.1256254728883505)\",\"rgba(255, 0, 0, 0.12527118287980557)\",\"rgba(255, 0, 0, 0.12645238488912583)\",\"rgba(0, 0, 255, 0.11784278266131878)\",\"rgba(0, 0, 255, 0.1245390459895134)\",\"rgba(255, 0, 0, 0.10199589133262635)\",\"rgba(0, 0, 255, 0.12229139767587185)\",\"rgba(255, 0, 0, 0.12321538887917996)\",\"rgba(255, 0, 0, 0.10967915635555983)\",\"rgba(0, 0, 255, 0.13354786708950997)\",\"rgba(255, 0, 0, 0.13497540578246117)\",\"rgba(255, 0, 0, 0.11177858095616103)\",\"rgba(0, 0, 255, 0.12067967876791955)\",\"rgba(0, 0, 255, 0.1145891271531582)\",\"rgba(255, 0, 0, 0.11227287389338017)\",\"rgba(255, 0, 0, 0.1227322205901146)\",\"rgba(0, 0, 255, 0.11777483895421029)\",\"rgba(255, 0, 0, 0.1210136566311121)\",\"rgba(0, 0, 255, 0.13480204567313195)\",\"rgba(0, 0, 255, 0.10357760721817613)\",\"rgba(255, 0, 0, 0.10157259195111693)\",\"rgba(255, 0, 0, 0.10464161392301322)\",\"rgba(0, 0, 255, 0.12674394473433495)\",\"rgba(255, 0, 0, 0.11254624594002963)\",\"rgba(255, 0, 0, 0.10764271104708314)\",\"rgba(0, 0, 255, 0.12714161574840546)\",\"rgba(255, 0, 0, 0.12564972266554833)\",\"rgba(255, 0, 0, 0.11082544773817063)\",\"rgba(255, 0, 0, 0.10330140697769821)\",\"rgba(255, 0, 0, 0.10202240608632565)\",\"rgba(0, 0, 255, 0.10889413338154555)\",\"rgba(255, 0, 0, 0.11328547578305007)\",\"rgba(255, 0, 0, 0.10853688456118107)\",\"rgba(255, 0, 0, 0.12781956270337105)\",\"rgba(0, 0, 255, 0.11289478410035372)\",\"rgba(0, 0, 255, 0.10882397033274174)\",\"rgba(0, 0, 255, 0.12341452799737454)\",\"rgba(255, 0, 0, 0.13452210128307343)\",\"rgba(0, 0, 255, 0.12166286669671536)\",\"rgba(0, 0, 255, 0.1051689762622118)\",\"rgba(255, 0, 0, 0.10501733785495163)\",\"rgba(0, 0, 255, 0.12685213834047318)\",\"rgba(255, 0, 0, 0.12051553502678872)\",\"rgba(255, 0, 0, 0.12066951431334019)\",\"rgba(255, 0, 0, 0.11311690732836724)\",\"rgba(255, 0, 0, 0.12540358901023865)\",\"rgba(0, 0, 255, 0.11209100801497698)\",\"rgba(0, 0, 255, 0.13927620872855187)\",\"rgba(0, 0, 255, 0.10848551187664271)\",\"rgba(0, 0, 255, 0.10279579334892333)\",\"rgba(255, 0, 0, 0.12267191745340825)\",\"rgba(255, 0, 0, 0.10763601623475552)\",\"rgba(0, 0, 255, 0.11573729328811169)\",\"rgba(255, 0, 0, 0.10125677592586727)\",\"rgba(255, 0, 0, 0.13239775821566582)\",\"rgba(255, 0, 0, 0.13136330842971802)\",\"rgba(255, 0, 0, 0.12779588103294373)\",\"rgba(255, 0, 0, 0.1357319675385952)\",\"rgba(255, 0, 0, 0.10869381800293923)\",\"rgba(255, 0, 0, 0.13561437502503396)\",\"rgba(255, 0, 0, 0.1225213747471571)\",\"rgba(255, 0, 0, 0.12315363846719266)\",\"rgba(0, 0, 255, 0.11590544171631337)\",\"rgba(255, 0, 0, 0.12157687954604626)\",\"rgba(0, 0, 255, 0.10577223906293512)\",\"rgba(255, 0, 0, 0.12614531479775906)\",\"rgba(0, 0, 255, 0.10851957499980927)\",\"rgba(0, 0, 255, 0.12348689734935761)\",\"rgba(0, 0, 255, 0.12644682675600052)\",\"rgba(0, 0, 255, 0.11854973286390305)\",\"rgba(255, 0, 0, 0.10309220398776234)\",\"rgba(0, 0, 255, 0.12290166728198529)\",\"rgba(0, 0, 255, 0.1170432385057211)\",\"rgba(0, 0, 255, 0.12599342912435532)\",\"rgba(255, 0, 0, 0.11203938946127892)\",\"rgba(255, 0, 0, 0.12202796190977097)\",\"rgba(255, 0, 0, 0.12837234921753407)\",\"rgba(255, 0, 0, 0.12956916391849518)\",\"rgba(0, 0, 255, 0.10923163164407015)\",\"rgba(0, 0, 255, 0.11519543025642634)\",\"rgba(0, 0, 255, 0.1216660387814045)\",\"rgba(0, 0, 255, 0.12724638395011426)\",\"rgba(255, 0, 0, 0.10837976671755314)\",\"rgba(0, 0, 255, 0.12760357223451138)\",\"rgba(255, 0, 0, 0.10698248436674476)\",\"rgba(255, 0, 0, 0.12589728683233262)\",\"rgba(255, 0, 0, 0.13548674285411835)\",\"rgba(255, 0, 0, 0.12117929048836232)\",\"rgba(255, 0, 0, 0.12281826175749302)\",\"rgba(0, 0, 255, 0.12560950629413128)\",\"rgba(255, 0, 0, 0.1332603193819523)\",\"rgba(255, 0, 0, 0.1105143990367651)\",\"rgba(0, 0, 255, 0.11511187013238669)\",\"rgba(255, 0, 0, 0.115893142670393)\",\"rgba(0, 0, 255, 0.11972759515047074)\",\"rgba(255, 0, 0, 0.130759422108531)\",\"rgba(255, 0, 0, 0.11499363873153925)\",\"rgba(0, 0, 255, 0.12377713657915593)\",\"rgba(255, 0, 0, 0.1070054205134511)\",\"rgba(255, 0, 0, 0.1233430676162243)\",\"rgba(0, 0, 255, 0.11888049766421319)\",\"rgba(0, 0, 255, 0.13450947627425194)\",\"rgba(255, 0, 0, 0.10393780786544085)\",\"rgba(0, 0, 255, 0.1307334214448929)\",\"rgba(255, 0, 0, 0.1209021419286728)\",\"rgba(255, 0, 0, 0.10316362059675158)\",\"rgba(0, 0, 255, 0.11180319860577584)\",\"rgba(0, 0, 255, 0.13188538178801537)\",\"rgba(0, 0, 255, 0.10275327498093248)\",\"rgba(0, 0, 255, 0.10653820391744376)\",\"rgba(0, 0, 255, 0.10739089958369732)\",\"rgba(0, 0, 255, 0.12424860559403897)\",\"rgba(255, 0, 0, 0.11453035324811936)\",\"rgba(0, 0, 255, 0.10300271180458367)\",\"rgba(0, 0, 255, 0.13130850940942765)\",\"rgba(0, 0, 255, 0.1298285335302353)\",\"rgba(0, 0, 255, 0.11188487280160189)\",\"rgba(0, 0, 255, 0.12178274281322957)\",\"rgba(255, 0, 0, 0.13709698840975762)\",\"rgba(0, 0, 255, 0.11990456692874432)\",\"rgba(0, 0, 255, 0.12315901406109334)\",\"rgba(255, 0, 0, 0.1294719133526087)\",\"rgba(0, 0, 255, 0.12622405998408795)\",\"rgba(0, 0, 255, 0.10833397079259158)\",\"rgba(255, 0, 0, 0.11352474559098483)\",\"rgba(0, 0, 255, 0.10798902846872807)\",\"rgba(0, 0, 255, 0.11840439066290856)\",\"rgba(0, 0, 255, 0.11246601063758135)\",\"rgba(0, 0, 255, 0.13183584660291672)\",\"rgba(255, 0, 0, 0.11004013419151307)\",\"rgba(0, 0, 255, 0.11885845139622689)\",\"rgba(255, 0, 0, 0.12595820650458336)\",\"rgba(0, 0, 255, 0.11622243598103524)\",\"rgba(255, 0, 0, 0.1310254879295826)\",\"rgba(255, 0, 0, 0.11859396696090699)\",\"rgba(255, 0, 0, 0.13564567118883133)\",\"rgba(0, 0, 255, 0.10313020590692759)\",\"rgba(0, 0, 255, 0.12493580430746079)\",\"rgba(0, 0, 255, 0.1075038094073534)\",\"rgba(0, 0, 255, 0.11264271512627602)\",\"rgba(255, 0, 0, 0.12367415465414525)\",\"rgba(255, 0, 0, 0.12898183465003968)\",\"rgba(255, 0, 0, 0.11142000779509545)\",\"rgba(0, 0, 255, 0.13165503963828087)\",\"rgba(0, 0, 255, 0.1350271873176098)\",\"rgba(255, 0, 0, 0.11390573754906655)\",\"rgba(0, 0, 255, 0.1333295352756977)\",\"rgba(0, 0, 255, 0.1265175923705101)\",\"rgba(0, 0, 255, 0.11086704060435296)\",\"rgba(255, 0, 0, 0.11598026417195798)\",\"rgba(255, 0, 0, 0.12957257628440857)\",\"rgba(255, 0, 0, 0.13771797567605973)\",\"rgba(255, 0, 0, 0.11726574636995793)\",\"rgba(255, 0, 0, 0.1128816943615675)\",\"rgba(0, 0, 255, 0.13586895763874055)\",\"rgba(0, 0, 255, 0.11412793342024088)\",\"rgba(0, 0, 255, 0.1373203605413437)\",\"rgba(255, 0, 0, 0.10446724426001311)\",\"rgba(255, 0, 0, 0.12678651548922062)\",\"rgba(255, 0, 0, 0.10116313295438886)\",\"rgba(0, 0, 255, 0.12944259718060494)\",\"rgba(0, 0, 255, 0.12917208522558213)\",\"rgba(0, 0, 255, 0.11909547299146653)\",\"rgba(255, 0, 0, 0.10127910147421063)\",\"rgba(255, 0, 0, 0.13052162006497384)\",\"rgba(0, 0, 255, 0.1019775559194386)\",\"rgba(0, 0, 255, 0.13780749812722207)\",\"rgba(0, 0, 255, 0.14757176339626313)\",\"rgba(255, 0, 0, 0.4003326326608658)\",\"rgba(255, 0, 0, 0.41093588918447493)\",\"rgba(255, 0, 0, 0.3339069217443466)\",\"rgba(255, 0, 0, 0.3273177847266197)\",\"rgba(0, 0, 255, 0.388435135781765)\",\"rgba(255, 0, 0, 0.4264832764863968)\",\"rgba(0, 0, 255, 0.3855177417397499)\",\"rgba(255, 0, 0, 0.4082920357584953)\",\"rgba(255, 0, 0, 0.3389418564736843)\",\"rgba(255, 0, 0, 0.38378174901008605)\"],\"size\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1.18646639585495,1.134598284959793,0.5,0.5,0.6296296566724777,0.8919081687927246,0.5,1.1262972950935364,0.5,0.5,1.257836401462555,1.2468833327293396,0.5,0.8167163729667664,0.5,0.764453262090683,0.5,0.5,0.5,0.6797190606594086,1.0262751281261444,0.931099534034729,0.5242124497890472,0.5079538822174072,0.5871261209249496,0.5,0.5,0.5863463580608368,1.2238749861717224,0.5228210538625717,0.5,0.8284536302089691,0.5256775170564651,0.5,0.597246453166008,0.5,0.5,0.5,0.6422354131937027,0.8422889113426208,0.5,0.5586929619312286,1.0279181599617004,0.5,1.0378715693950653,0.8137849867343903,0.5,0.6459799110889435,0.5,0.5,0.5,0.5,0.5,0.5,1.3410336077213287,0.7581565082073212,0.7495684325695038,0.5,0.5,0.7210509181022644,0.9668569564819336,0.5,0.5,1.1125564277172089,1.146668404340744,0.5,0.5,0.5,0.5,0.6927127093076706,1.0709316730499268,0.86850705742836,0.6976193189620972,0.5356242656707764,1.103459894657135,0.5,0.5,0.5,0.5,0.6718479841947556,0.5450219213962555,0.9407356381416321,0.5333985239267349,0.500045657157898,0.9695640206336975,0.5,0.5430246591567993,0.5432829558849335,1.0418829023838043,0.5219279676675797,0.5,1.1017343401908875,1.1066542267799377,0.5,0.5,0.5,0.5,0.7770752906799316,1.1674719750881195,0.7331429421901703,0.8477916419506073,0.7792697846889496,0.8087465465068817,0.5,0.7802352905273438,0.5763950049877167,0.5,0.656769409775734,0.5,0.5,0.9704131186008453,0.5,0.5,0.5,0.8019955158233643,0.5,0.5,0.5,0.5,1.0427003502845764,0.5773129910230637,0.669890284538269,1.1498567461967468,0.5857666879892349,0.5,0.5750715583562851,0.7230347245931625,0.5,0.5,0.5,0.5,1.3326013684272766,0.5,0.8947533667087555,0.6770134270191193,0.5417089015245438,0.8875696063041687,0.5,0.5,0.78321173787117,0.9916327893733978,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5239407420158386,0.526464119553566,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5528701096773148,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5158698856830597,0.5,0.5504812449216843,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5371934920549393,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5593643635511398,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5436019599437714,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5249036997556686,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5077584832906723,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5868608504533768,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,1,1,1,1,1,1,1,1,1,1]},\"mode\":\"lines\",\"text\":[\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Peso: 0.3955\",\"Peso: 0.3782\",\"Peso: 0.0447\",\"Peso: -0.0777\",\"Peso: 0.2099\",\"Peso: 0.2973\",\"Peso: -0.1524\",\"Peso: 0.3754\",\"Peso: -0.0597\",\"Peso: 0.0735\",\"Peso: 0.4193\",\"Peso: 0.4156\",\"Peso: -0.0752\",\"Peso: 0.2722\",\"Peso: -0.0768\",\"Peso: 0.2548\",\"Peso: -0.0848\",\"Peso: 0.0126\",\"Peso: 0.0764\",\"Peso: 0.2266\",\"Peso: 0.3421\",\"Peso: 0.3104\",\"Peso: -0.1747\",\"Peso: -0.1693\",\"Peso: -0.1957\",\"Peso: 0.0604\",\"Peso: 0.0059\",\"Peso: -0.1954\",\"Peso: 0.4080\",\"Peso: 0.1743\",\"Peso: -0.1287\",\"Peso: 0.2762\",\"Peso: 0.1752\",\"Peso: -0.0093\",\"Peso: -0.1991\",\"Peso: 0.0545\",\"Peso: -0.0879\",\"Peso: -0.0030\",\"Peso: 0.2141\",\"Peso: 0.2808\",\"Peso: 0.0792\",\"Peso: 0.1862\",\"Peso: 0.3426\",\"Peso: -0.1101\",\"Peso: 0.3460\",\"Peso: -0.2713\",\"Peso: -0.0063\",\"Peso: 0.2153\",\"Peso: 0.0714\",\"Peso: 0.0726\",\"Peso: 0.1497\",\"Peso: 0.0811\",\"Peso: 0.0727\",\"Peso: -0.1172\",\"Peso: -0.4470\",\"Peso: -0.2527\",\"Peso: -0.2499\",\"Peso: 0.0631\",\"Peso: 0.1359\",\"Peso: 0.2404\",\"Peso: 0.3223\",\"Peso: -0.1123\",\"Peso: 0.1133\",\"Peso: 0.3709\",\"Peso: 0.3822\",\"Peso: 0.0539\",\"Peso: 0.1592\",\"Peso: -0.0490\",\"Peso: 0.1086\",\"Peso: -0.2309\",\"Peso: 0.3570\",\"Peso: -0.2895\",\"Peso: 0.2325\",\"Peso: 0.1785\",\"Peso: 0.3678\",\"Peso: 0.1379\",\"Peso: -0.0644\",\"Peso: -0.0568\",\"Peso: -0.1021\",\"Peso: 0.2239\",\"Peso: 0.1817\",\"Peso: 0.3136\",\"Peso: -0.1778\",\"Peso: -0.1667\",\"Peso: 0.3232\",\"Peso: 0.0809\",\"Peso: 0.1810\",\"Peso: 0.1811\",\"Peso: 0.3473\",\"Peso: 0.1740\",\"Peso: 0.1449\",\"Peso: 0.3672\",\"Peso: 0.3689\",\"Peso: 0.0312\",\"Peso: -0.0872\",\"Peso: -0.0550\",\"Peso: 0.0597\",\"Peso: 0.2590\",\"Peso: 0.3892\",\"Peso: 0.2444\",\"Peso: 0.2826\",\"Peso: 0.2598\",\"Peso: -0.2696\",\"Peso: -0.1608\",\"Peso: 0.2601\",\"Peso: -0.1921\",\"Peso: 0.0349\",\"Peso: -0.2189\",\"Peso: -0.0583\",\"Peso: -0.1393\",\"Peso: 0.3235\",\"Peso: -0.0080\",\"Peso: -0.1244\",\"Peso: -0.0325\",\"Peso: 0.2673\",\"Peso: -0.0053\",\"Peso: 0.1311\",\"Peso: 0.0269\",\"Peso: 0.1178\",\"Peso: 0.3476\",\"Peso: 0.1924\",\"Peso: -0.2233\",\"Peso: 0.3833\",\"Peso: 0.1953\",\"Peso: -0.1220\",\"Peso: 0.1917\",\"Peso: -0.2410\",\"Peso: 0.1340\",\"Peso: -0.0591\",\"Peso: -0.1307\",\"Peso: -0.0655\",\"Peso: 0.4442\",\"Peso: -0.0076\",\"Peso: -0.2983\",\"Peso: 0.2257\",\"Peso: 0.1806\",\"Peso: -0.2959\",\"Peso: 0.1340\",\"Peso: -0.0197\",\"Peso: -0.2611\",\"Peso: 0.3305\",\"Peso: -0.0667\",\"Peso: 0.1404\",\"Peso: 0.0523\",\"Peso: 0.0387\",\"Peso: -0.0360\",\"Peso: -0.0567\",\"Peso: -0.0724\",\"Peso: -0.1162\",\"Peso: 0.0055\",\"Peso: 0.0778\",\"Peso: 0.1551\",\"Peso: 0.1353\",\"Peso: 0.0271\",\"Peso: 0.1548\",\"Peso: 0.0196\",\"Peso: -0.0751\",\"Peso: -0.1007\",\"Peso: -0.0442\",\"Peso: -0.0968\",\"Peso: -0.0379\",\"Peso: 0.0209\",\"Peso: 0.0168\",\"Peso: 0.0585\",\"Peso: -0.0822\",\"Peso: 0.0169\",\"Peso: 0.0755\",\"Peso: 0.1407\",\"Peso: 0.0525\",\"Peso: 0.0154\",\"Peso: 0.0713\",\"Peso: 0.0094\",\"Peso: -0.0767\",\"Peso: 0.0354\",\"Peso: 0.0534\",\"Peso: 0.1038\",\"Peso: 0.0271\",\"Peso: 0.0700\",\"Peso: 0.1746\",\"Peso: 0.1755\",\"Peso: 0.0639\",\"Peso: -0.0581\",\"Peso: 0.0664\",\"Peso: 0.1083\",\"Peso: -0.0313\",\"Peso: 0.0291\",\"Peso: 0.1089\",\"Peso: -0.0596\",\"Peso: -0.0226\",\"Peso: -0.0442\",\"Peso: 0.0097\",\"Peso: 0.0118\",\"Peso: -0.0532\",\"Peso: 0.1291\",\"Peso: 0.1233\",\"Peso: 0.0982\",\"Peso: -0.0116\",\"Peso: 0.0863\",\"Peso: -0.0445\",\"Peso: -0.0661\",\"Peso: 0.1067\",\"Peso: -0.0284\",\"Peso: 0.0110\",\"Peso: 0.0839\",\"Peso: 0.0926\",\"Peso: 0.1161\",\"Peso: 0.1843\",\"Peso: -0.0235\",\"Peso: 0.0594\",\"Peso: 0.0993\",\"Peso: -0.0917\",\"Peso: -0.0020\",\"Peso: -0.0277\",\"Peso: 0.0262\",\"Peso: 0.1083\",\"Peso: 0.1720\",\"Peso: 0.1423\",\"Peso: 0.1835\",\"Peso: 0.0305\",\"Peso: 0.0125\",\"Peso: -0.0091\",\"Peso: 0.0656\",\"Peso: -0.1196\",\"Peso: -0.0327\",\"Peso: 0.0260\",\"Peso: -0.0676\",\"Peso: -0.0615\",\"Peso: -0.0908\",\"Peso: 0.0605\",\"Peso: 0.0526\",\"Peso: -0.0670\",\"Peso: -0.0014\",\"Peso: -0.0156\",\"Peso: 0.0297\",\"Peso: -0.0969\",\"Peso: -0.0365\",\"Peso: 0.0642\",\"Peso: -0.0214\",\"Peso: -0.0939\",\"Peso: -0.0018\",\"Peso: 0.0560\",\"Peso: -0.0192\",\"Peso: -0.0807\",\"Peso: 0.0359\",\"Peso: 0.0203\",\"Peso: 0.0097\",\"Peso: 0.0358\",\"Peso: -0.0063\",\"Peso: -0.0408\",\"Peso: -0.0209\",\"Peso: 0.0734\",\"Peso: -0.0240\",\"Peso: 0.0093\",\"Peso: -0.0338\",\"Peso: -0.0487\",\"Peso: -0.0378\",\"Peso: -0.0546\",\"Peso: -0.0501\",\"Peso: -0.0168\",\"Peso: 0.0573\",\"Peso: 0.0594\",\"Peso: 0.0605\",\"Peso: 0.0546\",\"Peso: -0.0226\",\"Peso: 0.0102\",\"Peso: 0.0585\",\"Peso: 0.0679\",\"Peso: 0.0078\",\"Peso: -0.0512\",\"Peso: -0.0515\",\"Peso: -0.0142\",\"Peso: 0.0984\",\"Peso: -0.0202\",\"Peso: 0.1164\",\"Peso: 0.1044\",\"Peso: 0.1104\",\"Peso: 0.1791\",\"Peso: 0.1284\",\"Peso: 0.1552\",\"Peso: 0.0918\",\"Peso: -0.0152\",\"Peso: -0.1481\",\"Peso: -0.0420\",\"Peso: 0.1141\",\"Peso: -0.0633\",\"Peso: -0.0630\",\"Peso: 0.0469\",\"Peso: -0.0403\",\"Peso: 0.0036\",\"Peso: 0.0460\",\"Peso: -0.0252\",\"Peso: 0.0565\",\"Peso: 0.0804\",\"Peso: -0.1234\",\"Peso: 0.0733\",\"Peso: 0.0161\",\"Peso: 0.0376\",\"Peso: 0.0353\",\"Peso: 0.0232\",\"Peso: -0.0239\",\"Peso: -0.0592\",\"Peso: 0.1458\",\"Peso: 0.0515\",\"Peso: -0.0297\",\"Peso: 0.1309\",\"Peso: -0.0020\",\"Peso: -0.1313\",\"Peso: -0.0218\",\"Peso: 0.0423\",\"Peso: -0.1127\",\"Peso: 0.0198\",\"Peso: -0.0143\",\"Peso: 0.0759\",\"Peso: 0.1865\",\"Peso: -0.0937\",\"Peso: -0.0156\",\"Peso: 0.0793\",\"Peso: -0.0872\",\"Peso: 0.0310\",\"Peso: -0.0073\",\"Peso: 0.0204\",\"Peso: -0.0479\",\"Peso: -0.0965\",\"Peso: 0.0089\",\"Peso: -0.0102\",\"Peso: -0.0114\",\"Peso: 0.0919\",\"Peso: 0.0650\",\"Peso: -0.0299\",\"Peso: 0.1024\",\"Peso: 0.0985\",\"Peso: 0.0341\",\"Peso: 0.0668\",\"Peso: -0.0222\",\"Peso: 0.0213\",\"Peso: -0.0275\",\"Peso: -0.1131\",\"Peso: -0.0770\",\"Peso: 0.0153\",\"Peso: 0.1069\",\"Peso: -0.0499\",\"Peso: -0.0559\",\"Peso: -0.0451\",\"Peso: 0.1200\",\"Peso: -0.1062\",\"Peso: 0.0463\",\"Peso: 0.1812\",\"Peso: 0.0185\",\"Peso: -0.0723\",\"Peso: 0.0336\",\"Peso: 0.0022\",\"Peso: 0.1172\",\"Peso: 0.0686\",\"Peso: -0.0405\",\"Peso: 0.0964\",\"Peso: 0.1750\",\"Peso: 0.0028\",\"Peso: 0.0095\",\"Peso: -0.0171\",\"Peso: 0.0377\",\"Peso: -0.0716\",\"Peso: 0.0713\",\"Peso: 0.0247\",\"Peso: 0.0949\",\"Peso: 0.0460\",\"Peso: 0.1693\",\"Peso: 0.0069\",\"Peso: 0.1066\",\"Peso: 0.0591\",\"Peso: -0.0444\",\"Peso: -0.0239\",\"Peso: -0.0998\",\"Peso: -0.0490\",\"Peso: 0.0421\",\"Peso: 0.1563\",\"Peso: 0.0418\",\"Peso: 0.1624\",\"Peso: 0.0832\",\"Peso: 0.0272\",\"Peso: -0.0153\",\"Peso: 0.0644\",\"Peso: 0.0945\",\"Peso: 0.0142\",\"Peso: -0.0350\",\"Peso: 0.1426\",\"Peso: 0.1122\",\"Peso: 0.0296\",\"Peso: 0.1269\",\"Peso: 0.1385\",\"Peso: -0.1092\",\"Peso: -0.1220\",\"Peso: -0.1194\",\"Peso: -0.0870\",\"Peso: 0.0927\",\"Peso: -0.1074\",\"Peso: -0.0122\",\"Peso: 0.1020\",\"Peso: 0.0578\",\"Peso: 0.0358\",\"Peso: 0.0751\",\"Peso: 0.0039\",\"Peso: -0.0122\",\"Peso: -0.1067\",\"Peso: 0.0433\",\"Peso: -0.0294\",\"Peso: -0.0868\",\"Peso: 0.0519\",\"Peso: 0.0257\",\"Peso: 0.0019\",\"Peso: 0.1067\",\"Peso: -0.0338\",\"Peso: -0.0274\",\"Peso: 0.1360\",\"Peso: -0.0202\",\"Peso: 0.0867\",\"Peso: 0.0664\",\"Peso: -0.0260\",\"Peso: 0.0127\",\"Peso: -0.0087\",\"Peso: 0.0784\",\"Peso: 0.0473\",\"Peso: -0.0086\",\"Peso: 0.1181\",\"Peso: 0.0377\",\"Peso: 0.0398\",\"Peso: 0.1452\",\"Peso: -0.0123\",\"Peso: -0.0380\",\"Peso: 0.0139\",\"Peso: -0.0377\",\"Peso: 0.0640\",\"Peso: -0.0923\",\"Peso: -0.0642\",\"Peso: 0.0512\",\"Peso: 0.1187\",\"Peso: 0.0517\",\"Peso: 0.1490\",\"Peso: 0.0150\",\"Peso: 0.0371\",\"Peso: 0.0851\",\"Peso: 0.0822\",\"Peso: 0.1288\",\"Peso: 0.1372\",\"Peso: 0.0629\",\"Peso: 0.0125\",\"Peso: -0.0210\",\"Peso: -0.0512\",\"Peso: 0.0135\",\"Peso: 0.0220\",\"Peso: 0.0030\",\"Peso: 0.1956\",\"Peso: 0.0480\",\"Peso: 0.0720\",\"Peso: 0.0234\",\"Peso: 0.0066\",\"Peso: -0.0093\",\"Peso: 0.0118\",\"Peso: -0.0952\",\"Peso: -0.1386\",\"Peso: 0.0965\",\"Peso: 0.1363\",\"Peso: -0.0355\",\"Peso: -0.0342\",\"Peso: 0.0018\",\"Peso: -0.0053\",\"Peso: 0.0109\",\"Peso: -0.0181\",\"Peso: -0.0286\",\"Peso: -0.0019\",\"Peso: 0.0134\",\"Peso: -0.0048\",\"Peso: 0.0353\",\"Peso: -0.0014\",\"Peso: -0.0190\",\"Peso: -0.0067\",\"Peso: 0.0191\",\"Peso: 0.0128\",\"Peso: 0.0249\",\"Peso: -0.0342\",\"Peso: 0.0088\",\"Peso: 0.0238\",\"Peso: 0.0130\",\"Peso: 0.0392\",\"Peso: -0.0282\",\"Peso: -0.0095\",\"Peso: -0.0302\",\"Peso: 0.0138\",\"Peso: -0.0104\",\"Peso: -0.0313\",\"Peso: 0.0371\",\"Peso: 0.0256\",\"Peso: -0.0253\",\"Peso: -0.0265\",\"Peso: 0.0178\",\"Peso: 0.0245\",\"Peso: -0.0020\",\"Peso: 0.0223\",\"Peso: -0.0232\",\"Peso: -0.0097\",\"Peso: 0.0335\",\"Peso: -0.0350\",\"Peso: -0.0118\",\"Peso: 0.0207\",\"Peso: 0.0146\",\"Peso: -0.0123\",\"Peso: -0.0227\",\"Peso: 0.0178\",\"Peso: -0.0210\",\"Peso: 0.0348\",\"Peso: 0.0036\",\"Peso: -0.0016\",\"Peso: -0.0046\",\"Peso: 0.0267\",\"Peso: -0.0125\",\"Peso: -0.0076\",\"Peso: 0.0271\",\"Peso: -0.0256\",\"Peso: -0.0108\",\"Peso: -0.0033\",\"Peso: -0.0020\",\"Peso: 0.0089\",\"Peso: -0.0133\",\"Peso: -0.0085\",\"Peso: -0.0278\",\"Peso: 0.0129\",\"Peso: 0.0088\",\"Peso: 0.0234\",\"Peso: -0.0345\",\"Peso: 0.0217\",\"Peso: 0.0052\",\"Peso: -0.0050\",\"Peso: 0.0269\",\"Peso: -0.0205\",\"Peso: -0.0207\",\"Peso: -0.0131\",\"Peso: -0.0254\",\"Peso: 0.0121\",\"Peso: 0.0393\",\"Peso: 0.0085\",\"Peso: 0.0028\",\"Peso: -0.0227\",\"Peso: -0.0076\",\"Peso: 0.0157\",\"Peso: -0.0013\",\"Peso: -0.0324\",\"Peso: -0.0314\",\"Peso: -0.0278\",\"Peso: -0.0357\",\"Peso: -0.0087\",\"Peso: -0.0356\",\"Peso: -0.0225\",\"Peso: -0.0232\",\"Peso: 0.0159\",\"Peso: -0.0216\",\"Peso: 0.0058\",\"Peso: -0.0261\",\"Peso: 0.0085\",\"Peso: 0.0235\",\"Peso: 0.0264\",\"Peso: 0.0185\",\"Peso: -0.0031\",\"Peso: 0.0229\",\"Peso: 0.0170\",\"Peso: 0.0260\",\"Peso: -0.0120\",\"Peso: -0.0220\",\"Peso: -0.0284\",\"Peso: -0.0296\",\"Peso: 0.0092\",\"Peso: 0.0152\",\"Peso: 0.0217\",\"Peso: 0.0272\",\"Peso: -0.0084\",\"Peso: 0.0276\",\"Peso: -0.0070\",\"Peso: -0.0259\",\"Peso: -0.0355\",\"Peso: -0.0212\",\"Peso: -0.0228\",\"Peso: 0.0256\",\"Peso: -0.0333\",\"Peso: -0.0105\",\"Peso: 0.0151\",\"Peso: -0.0159\",\"Peso: 0.0197\",\"Peso: -0.0308\",\"Peso: -0.0150\",\"Peso: 0.0238\",\"Peso: -0.0070\",\"Peso: -0.0233\",\"Peso: 0.0189\",\"Peso: 0.0345\",\"Peso: -0.0039\",\"Peso: 0.0307\",\"Peso: -0.0209\",\"Peso: -0.0032\",\"Peso: 0.0118\",\"Peso: 0.0319\",\"Peso: 0.0028\",\"Peso: 0.0065\",\"Peso: 0.0074\",\"Peso: 0.0242\",\"Peso: -0.0145\",\"Peso: 0.0030\",\"Peso: 0.0313\",\"Peso: 0.0298\",\"Peso: 0.0119\",\"Peso: 0.0218\",\"Peso: -0.0371\",\"Peso: 0.0199\",\"Peso: 0.0232\",\"Peso: -0.0295\",\"Peso: 0.0262\",\"Peso: 0.0083\",\"Peso: -0.0135\",\"Peso: 0.0080\",\"Peso: 0.0184\",\"Peso: 0.0125\",\"Peso: 0.0318\",\"Peso: -0.0100\",\"Peso: 0.0189\",\"Peso: -0.0260\",\"Peso: 0.0162\",\"Peso: -0.0310\",\"Peso: -0.0186\",\"Peso: -0.0356\",\"Peso: 0.0031\",\"Peso: 0.0249\",\"Peso: 0.0075\",\"Peso: 0.0126\",\"Peso: -0.0237\",\"Peso: -0.0290\",\"Peso: -0.0114\",\"Peso: 0.0317\",\"Peso: 0.0350\",\"Peso: -0.0139\",\"Peso: 0.0333\",\"Peso: 0.0265\",\"Peso: 0.0109\",\"Peso: -0.0160\",\"Peso: -0.0296\",\"Peso: -0.0377\",\"Peso: -0.0173\",\"Peso: -0.0129\",\"Peso: 0.0359\",\"Peso: 0.0141\",\"Peso: 0.0373\",\"Peso: -0.0045\",\"Peso: -0.0268\",\"Peso: -0.0012\",\"Peso: 0.0294\",\"Peso: 0.0292\",\"Peso: 0.0191\",\"Peso: -0.0013\",\"Peso: -0.0305\",\"Peso: 0.0020\",\"Peso: 0.0378\",\"Peso: 0.0476\",\"Peso: -0.1003\",\"Peso: -0.1109\",\"Peso: -0.0339\",\"Peso: -0.0273\",\"Peso: 0.0884\",\"Peso: -0.1265\",\"Peso: 0.0855\",\"Peso: -0.1083\",\"Peso: -0.0389\",\"Peso: -0.0838\"],\"x\":[0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null],\"y\":[0,-4.6875,null,0,-4.0625,null,0,-3.4375,null,0,-2.8125,null,0,-2.1875,null,0,-1.5625,null,0,-0.9375,null,0,-0.3125,null,0,0.3125,null,0,0.9375,null,0,1.5625,null,0,2.1875,null,0,2.8125,null,0,3.4375,null,0,4.0625,null,0,4.6875,null,-4.6875,-4.6875,null,-4.6875,-4.0625,null,-4.6875,-3.4375,null,-4.6875,-2.8125,null,-4.6875,-2.1875,null,-4.6875,-1.5625,null,-4.6875,-0.9375,null,-4.6875,-0.3125,null,-4.6875,0.3125,null,-4.0625,-4.6875,null,-4.0625,-4.0625,null,-4.0625,-3.4375,null,-4.0625,-2.8125,null,-4.0625,-2.1875,null,-4.0625,-1.5625,null,-4.0625,-0.9375,null,-4.0625,-0.3125,null,-4.0625,0.3125,null,-3.4375,-4.6875,null,-3.4375,-4.0625,null,-3.4375,-3.4375,null,-3.4375,-2.8125,null,-3.4375,-2.1875,null,-3.4375,-1.5625,null,-3.4375,-0.9375,null,-3.4375,-0.3125,null,-3.4375,0.3125,null,-2.8125,-4.6875,null,-2.8125,-4.0625,null,-2.8125,-3.4375,null,-2.8125,-2.8125,null,-2.8125,-2.1875,null,-2.8125,-1.5625,null,-2.8125,-0.9375,null,-2.8125,-0.3125,null,-2.8125,0.3125,null,-2.1875,-4.6875,null,-2.1875,-4.0625,null,-2.1875,-3.4375,null,-2.1875,-2.8125,null,-2.1875,-2.1875,null,-2.1875,-1.5625,null,-2.1875,-0.9375,null,-2.1875,-0.3125,null,-2.1875,0.3125,null,-1.5625,-4.6875,null,-1.5625,-4.0625,null,-1.5625,-3.4375,null,-1.5625,-2.8125,null,-1.5625,-2.1875,null,-1.5625,-1.5625,null,-1.5625,-0.9375,null,-1.5625,-0.3125,null,-1.5625,0.3125,null,-0.9375,-4.6875,null,-0.9375,-4.0625,null,-0.9375,-3.4375,null,-0.9375,-2.8125,null,-0.9375,-2.1875,null,-0.9375,-1.5625,null,-0.9375,-0.9375,null,-0.9375,-0.3125,null,-0.9375,0.3125,null,-0.3125,-4.6875,null,-0.3125,-4.0625,null,-0.3125,-3.4375,null,-0.3125,-2.8125,null,-0.3125,-2.1875,null,-0.3125,-1.5625,null,-0.3125,-0.9375,null,-0.3125,-0.3125,null,-0.3125,0.3125,null,0.3125,-4.6875,null,0.3125,-4.0625,null,0.3125,-3.4375,null,0.3125,-2.8125,null,0.3125,-2.1875,null,0.3125,-1.5625,null,0.3125,-0.9375,null,0.3125,-0.3125,null,0.3125,0.3125,null,0.9375,-4.6875,null,0.9375,-4.0625,null,0.9375,-3.4375,null,0.9375,-2.8125,null,0.9375,-2.1875,null,0.9375,-1.5625,null,0.9375,-0.9375,null,0.9375,-0.3125,null,0.9375,0.3125,null,1.5625,-4.6875,null,1.5625,-4.0625,null,1.5625,-3.4375,null,1.5625,-2.8125,null,1.5625,-2.1875,null,1.5625,-0.9375,null,1.5625,-0.3125,null,1.5625,0.3125,null,2.1875,-4.6875,null,2.1875,-4.0625,null,2.1875,-3.4375,null,2.1875,-2.8125,null,2.1875,-2.1875,null,2.1875,-1.5625,null,2.1875,-0.9375,null,2.1875,-0.3125,null,2.1875,0.3125,null,2.8125,-4.6875,null,2.8125,-4.0625,null,2.8125,-3.4375,null,2.8125,-2.8125,null,2.8125,-2.1875,null,2.8125,-1.5625,null,2.8125,-0.9375,null,2.8125,-0.3125,null,2.8125,0.3125,null,3.4375,-4.6875,null,3.4375,-4.0625,null,3.4375,-3.4375,null,3.4375,-2.8125,null,3.4375,-2.1875,null,3.4375,-1.5625,null,3.4375,-0.9375,null,3.4375,-0.3125,null,3.4375,0.3125,null,4.0625,-4.6875,null,4.0625,-4.0625,null,4.0625,-3.4375,null,4.0625,-2.8125,null,4.0625,-2.1875,null,4.0625,-1.5625,null,4.0625,-0.9375,null,4.0625,-0.3125,null,4.0625,0.3125,null,4.6875,-4.6875,null,4.6875,-4.0625,null,4.6875,-3.4375,null,4.6875,-2.8125,null,4.6875,-2.1875,null,4.6875,-1.5625,null,4.6875,-0.9375,null,4.6875,-0.3125,null,4.6875,0.3125,null,-4.6875,-4.916666666666666,null,-4.6875,-4.75,null,-4.6875,-4.583333333333333,null,-4.6875,-4.416666666666666,null,-4.6875,-4.25,null,-4.6875,-4.083333333333333,null,-4.6875,-3.9166666666666665,null,-4.6875,-3.75,null,-4.6875,-3.583333333333333,null,-4.6875,-3.4166666666666665,null,-4.6875,-3.25,null,-4.6875,-3.083333333333333,null,-4.6875,-2.9166666666666665,null,-4.6875,-2.75,null,-4.6875,-2.583333333333333,null,-4.6875,-2.4166666666666665,null,-4.6875,-2.25,null,-4.6875,-2.083333333333333,null,-4.6875,-1.9166666666666665,null,-4.6875,-1.75,null,-4.0625,-4.916666666666666,null,-4.0625,-4.75,null,-4.0625,-4.583333333333333,null,-4.0625,-4.416666666666666,null,-4.0625,-4.25,null,-4.0625,-4.083333333333333,null,-4.0625,-3.9166666666666665,null,-4.0625,-3.75,null,-4.0625,-3.4166666666666665,null,-4.0625,-3.25,null,-4.0625,-3.083333333333333,null,-4.0625,-2.9166666666666665,null,-4.0625,-2.75,null,-4.0625,-2.583333333333333,null,-4.0625,-2.4166666666666665,null,-4.0625,-2.25,null,-4.0625,-2.083333333333333,null,-4.0625,-1.9166666666666665,null,-4.0625,-1.75,null,-3.4375,-4.916666666666666,null,-3.4375,-4.75,null,-3.4375,-4.583333333333333,null,-3.4375,-4.416666666666666,null,-3.4375,-4.25,null,-3.4375,-4.083333333333333,null,-3.4375,-3.9166666666666665,null,-3.4375,-3.75,null,-3.4375,-3.583333333333333,null,-3.4375,-3.4166666666666665,null,-3.4375,-3.25,null,-3.4375,-3.083333333333333,null,-3.4375,-2.9166666666666665,null,-3.4375,-2.75,null,-3.4375,-2.583333333333333,null,-3.4375,-2.4166666666666665,null,-3.4375,-2.25,null,-3.4375,-1.9166666666666665,null,-3.4375,-1.75,null,-2.8125,-4.916666666666666,null,-2.8125,-4.75,null,-2.8125,-4.583333333333333,null,-2.8125,-4.416666666666666,null,-2.8125,-4.25,null,-2.8125,-4.083333333333333,null,-2.8125,-3.9166666666666665,null,-2.8125,-3.75,null,-2.8125,-3.583333333333333,null,-2.8125,-3.4166666666666665,null,-2.8125,-3.25,null,-2.8125,-3.083333333333333,null,-2.8125,-2.9166666666666665,null,-2.8125,-2.75,null,-2.8125,-2.583333333333333,null,-2.8125,-2.4166666666666665,null,-2.8125,-2.25,null,-2.8125,-2.083333333333333,null,-2.8125,-1.9166666666666665,null,-2.8125,-1.75,null,-2.1875,-4.916666666666666,null,-2.1875,-4.75,null,-2.1875,-4.583333333333333,null,-2.1875,-4.416666666666666,null,-2.1875,-4.25,null,-2.1875,-4.083333333333333,null,-2.1875,-3.9166666666666665,null,-2.1875,-3.75,null,-2.1875,-3.583333333333333,null,-2.1875,-3.4166666666666665,null,-2.1875,-3.25,null,-2.1875,-3.083333333333333,null,-2.1875,-2.9166666666666665,null,-2.1875,-2.75,null,-2.1875,-2.583333333333333,null,-2.1875,-2.4166666666666665,null,-2.1875,-2.25,null,-2.1875,-2.083333333333333,null,-2.1875,-1.9166666666666665,null,-2.1875,-1.75,null,-1.5625,-4.916666666666666,null,-1.5625,-4.75,null,-1.5625,-4.583333333333333,null,-1.5625,-4.416666666666666,null,-1.5625,-4.25,null,-1.5625,-4.083333333333333,null,-1.5625,-3.9166666666666665,null,-1.5625,-3.75,null,-1.5625,-3.583333333333333,null,-1.5625,-3.4166666666666665,null,-1.5625,-3.25,null,-1.5625,-3.083333333333333,null,-1.5625,-2.9166666666666665,null,-1.5625,-2.75,null,-1.5625,-2.583333333333333,null,-1.5625,-2.4166666666666665,null,-1.5625,-2.25,null,-1.5625,-2.083333333333333,null,-1.5625,-1.9166666666666665,null,-1.5625,-1.75,null,-0.9375,-4.916666666666666,null,-0.9375,-4.75,null,-0.9375,-4.583333333333333,null,-0.9375,-4.416666666666666,null,-0.9375,-4.25,null,-0.9375,-4.083333333333333,null,-0.9375,-3.9166666666666665,null,-0.9375,-3.75,null,-0.9375,-3.583333333333333,null,-0.9375,-3.4166666666666665,null,-0.9375,-3.25,null,-0.9375,-3.083333333333333,null,-0.9375,-2.9166666666666665,null,-0.9375,-2.75,null,-0.9375,-2.583333333333333,null,-0.9375,-2.4166666666666665,null,-0.9375,-2.25,null,-0.9375,-2.083333333333333,null,-0.9375,-1.9166666666666665,null,-0.9375,-1.75,null,-0.3125,-4.916666666666666,null,-0.3125,-4.75,null,-0.3125,-4.583333333333333,null,-0.3125,-4.416666666666666,null,-0.3125,-4.25,null,-0.3125,-4.083333333333333,null,-0.3125,-3.9166666666666665,null,-0.3125,-3.75,null,-0.3125,-3.583333333333333,null,-0.3125,-3.4166666666666665,null,-0.3125,-3.25,null,-0.3125,-3.083333333333333,null,-0.3125,-2.9166666666666665,null,-0.3125,-2.75,null,-0.3125,-2.583333333333333,null,-0.3125,-2.4166666666666665,null,-0.3125,-2.25,null,-0.3125,-2.083333333333333,null,-0.3125,-1.9166666666666665,null,-0.3125,-1.75,null,0.3125,-4.916666666666666,null,0.3125,-4.75,null,0.3125,-4.583333333333333,null,0.3125,-4.416666666666666,null,0.3125,-4.25,null,0.3125,-4.083333333333333,null,0.3125,-3.9166666666666665,null,0.3125,-3.75,null,0.3125,-3.583333333333333,null,0.3125,-3.4166666666666665,null,0.3125,-3.25,null,0.3125,-3.083333333333333,null,0.3125,-2.9166666666666665,null,0.3125,-2.75,null,0.3125,-2.583333333333333,null,0.3125,-2.4166666666666665,null,0.3125,-2.25,null,0.3125,-2.083333333333333,null,0.3125,-1.9166666666666665,null,0.3125,-1.75,null,0.9375,-4.916666666666666,null,0.9375,-4.75,null,0.9375,-4.583333333333333,null,0.9375,-4.416666666666666,null,0.9375,-4.25,null,0.9375,-4.083333333333333,null,0.9375,-3.9166666666666665,null,0.9375,-3.75,null,0.9375,-3.583333333333333,null,0.9375,-3.4166666666666665,null,0.9375,-3.25,null,0.9375,-3.083333333333333,null,0.9375,-2.9166666666666665,null,0.9375,-2.75,null,0.9375,-2.583333333333333,null,0.9375,-2.4166666666666665,null,0.9375,-2.25,null,0.9375,-2.083333333333333,null,0.9375,-1.9166666666666665,null,0.9375,-1.75,null,1.5625,-4.916666666666666,null,1.5625,-4.75,null,1.5625,-4.583333333333333,null,1.5625,-4.416666666666666,null,1.5625,-4.25,null,1.5625,-4.083333333333333,null,1.5625,-3.9166666666666665,null,1.5625,-3.75,null,1.5625,-3.583333333333333,null,1.5625,-3.4166666666666665,null,1.5625,-3.25,null,1.5625,-3.083333333333333,null,1.5625,-2.9166666666666665,null,1.5625,-2.75,null,1.5625,-2.583333333333333,null,1.5625,-2.4166666666666665,null,1.5625,-2.25,null,1.5625,-2.083333333333333,null,1.5625,-1.9166666666666665,null,1.5625,-1.75,null,2.1875,-4.916666666666666,null,2.1875,-4.75,null,2.1875,-4.583333333333333,null,2.1875,-4.416666666666666,null,2.1875,-4.25,null,2.1875,-4.083333333333333,null,2.1875,-3.9166666666666665,null,2.1875,-3.75,null,2.1875,-3.583333333333333,null,2.1875,-3.4166666666666665,null,2.1875,-3.25,null,2.1875,-3.083333333333333,null,2.1875,-2.9166666666666665,null,2.1875,-2.75,null,2.1875,-2.583333333333333,null,2.1875,-2.4166666666666665,null,2.1875,-2.25,null,2.1875,-2.083333333333333,null,2.1875,-1.9166666666666665,null,2.1875,-1.75,null,2.8125,-4.916666666666666,null,2.8125,-4.75,null,2.8125,-4.583333333333333,null,2.8125,-4.416666666666666,null,2.8125,-4.25,null,2.8125,-4.083333333333333,null,2.8125,-3.9166666666666665,null,2.8125,-3.75,null,2.8125,-3.583333333333333,null,2.8125,-3.4166666666666665,null,2.8125,-3.25,null,2.8125,-3.083333333333333,null,2.8125,-2.9166666666666665,null,2.8125,-2.75,null,2.8125,-2.583333333333333,null,2.8125,-2.4166666666666665,null,2.8125,-2.25,null,2.8125,-2.083333333333333,null,2.8125,-1.9166666666666665,null,2.8125,-1.75,null,3.4375,-4.916666666666666,null,3.4375,-4.75,null,3.4375,-4.583333333333333,null,3.4375,-4.416666666666666,null,3.4375,-4.25,null,3.4375,-4.083333333333333,null,3.4375,-3.9166666666666665,null,3.4375,-3.75,null,3.4375,-3.583333333333333,null,3.4375,-3.4166666666666665,null,3.4375,-3.25,null,3.4375,-3.083333333333333,null,3.4375,-2.9166666666666665,null,3.4375,-2.75,null,3.4375,-2.583333333333333,null,3.4375,-2.4166666666666665,null,3.4375,-2.25,null,3.4375,-2.083333333333333,null,3.4375,-1.9166666666666665,null,3.4375,-1.75,null,4.0625,-4.916666666666666,null,4.0625,-4.75,null,4.0625,-4.583333333333333,null,4.0625,-4.416666666666666,null,4.0625,-4.25,null,4.0625,-4.083333333333333,null,4.0625,-3.9166666666666665,null,4.0625,-3.75,null,4.0625,-3.583333333333333,null,4.0625,-3.4166666666666665,null,4.0625,-3.25,null,4.0625,-3.083333333333333,null,4.0625,-2.9166666666666665,null,4.0625,-2.75,null,4.0625,-2.583333333333333,null,4.0625,-2.4166666666666665,null,4.0625,-2.25,null,4.0625,-2.083333333333333,null,4.0625,-1.9166666666666665,null,4.0625,-1.75,null,4.6875,-4.916666666666666,null,4.6875,-4.75,null,4.6875,-4.583333333333333,null,4.6875,-4.416666666666666,null,4.6875,-4.25,null,4.6875,-4.083333333333333,null,4.6875,-3.9166666666666665,null,4.6875,-3.75,null,4.6875,-3.583333333333333,null,4.6875,-3.4166666666666665,null,4.6875,-3.25,null,4.6875,-3.083333333333333,null,4.6875,-2.9166666666666665,null,4.6875,-2.75,null,4.6875,-2.583333333333333,null,4.6875,-2.4166666666666665,null,4.6875,-2.25,null,4.6875,-2.083333333333333,null,4.6875,-1.9166666666666665,null,4.6875,-1.75,null,-4.916666666666666,-4.5,null,-4.916666666666666,-3.5,null,-4.916666666666666,-2.5,null,-4.916666666666666,-1.5,null,-4.916666666666666,-0.5,null,-4.916666666666666,0.5,null,-4.916666666666666,1.5,null,-4.916666666666666,2.5,null,-4.916666666666666,3.5,null,-4.916666666666666,4.5,null,-4.75,-4.5,null,-4.75,-3.5,null,-4.75,-2.5,null,-4.75,-1.5,null,-4.75,-0.5,null,-4.75,0.5,null,-4.75,1.5,null,-4.75,2.5,null,-4.75,3.5,null,-4.75,4.5,null,-4.583333333333333,-4.5,null,-4.583333333333333,-3.5,null,-4.583333333333333,-2.5,null,-4.583333333333333,-1.5,null,-4.583333333333333,-0.5,null,-4.583333333333333,0.5,null,-4.583333333333333,1.5,null,-4.583333333333333,2.5,null,-4.583333333333333,3.5,null,-4.583333333333333,4.5,null,-4.416666666666666,-4.5,null,-4.416666666666666,-3.5,null,-4.416666666666666,-2.5,null,-4.416666666666666,-1.5,null,-4.416666666666666,-0.5,null,-4.416666666666666,0.5,null,-4.416666666666666,1.5,null,-4.416666666666666,2.5,null,-4.416666666666666,3.5,null,-4.416666666666666,4.5,null,-4.25,-4.5,null,-4.25,-3.5,null,-4.25,-2.5,null,-4.25,-1.5,null,-4.25,-0.5,null,-4.25,0.5,null,-4.25,1.5,null,-4.25,2.5,null,-4.25,3.5,null,-4.25,4.5,null,-4.083333333333333,-4.5,null,-4.083333333333333,-3.5,null,-4.083333333333333,-2.5,null,-4.083333333333333,-1.5,null,-4.083333333333333,-0.5,null,-4.083333333333333,1.5,null,-4.083333333333333,2.5,null,-4.083333333333333,3.5,null,-4.083333333333333,4.5,null,-3.9166666666666665,-4.5,null,-3.9166666666666665,-3.5,null,-3.9166666666666665,-2.5,null,-3.9166666666666665,-1.5,null,-3.9166666666666665,-0.5,null,-3.9166666666666665,0.5,null,-3.9166666666666665,1.5,null,-3.9166666666666665,2.5,null,-3.9166666666666665,3.5,null,-3.9166666666666665,4.5,null,-3.75,-4.5,null,-3.75,-3.5,null,-3.75,-2.5,null,-3.75,-1.5,null,-3.75,-0.5,null,-3.75,0.5,null,-3.75,1.5,null,-3.75,2.5,null,-3.75,3.5,null,-3.75,4.5,null,-3.583333333333333,-4.5,null,-3.583333333333333,-2.5,null,-3.583333333333333,-1.5,null,-3.583333333333333,-0.5,null,-3.583333333333333,0.5,null,-3.583333333333333,1.5,null,-3.583333333333333,2.5,null,-3.583333333333333,3.5,null,-3.583333333333333,4.5,null,-3.4166666666666665,-4.5,null,-3.4166666666666665,-3.5,null,-3.4166666666666665,-2.5,null,-3.4166666666666665,-1.5,null,-3.4166666666666665,-0.5,null,-3.4166666666666665,0.5,null,-3.4166666666666665,1.5,null,-3.4166666666666665,2.5,null,-3.4166666666666665,3.5,null,-3.4166666666666665,4.5,null,-3.25,-4.5,null,-3.25,-3.5,null,-3.25,-2.5,null,-3.25,-1.5,null,-3.25,-0.5,null,-3.25,0.5,null,-3.25,1.5,null,-3.25,2.5,null,-3.25,3.5,null,-3.25,4.5,null,-3.083333333333333,-4.5,null,-3.083333333333333,-3.5,null,-3.083333333333333,-1.5,null,-3.083333333333333,-0.5,null,-3.083333333333333,0.5,null,-3.083333333333333,1.5,null,-3.083333333333333,2.5,null,-3.083333333333333,3.5,null,-3.083333333333333,4.5,null,-2.9166666666666665,-4.5,null,-2.9166666666666665,-3.5,null,-2.9166666666666665,-2.5,null,-2.9166666666666665,-1.5,null,-2.9166666666666665,-0.5,null,-2.9166666666666665,0.5,null,-2.9166666666666665,1.5,null,-2.9166666666666665,2.5,null,-2.9166666666666665,3.5,null,-2.9166666666666665,4.5,null,-2.75,-4.5,null,-2.75,-3.5,null,-2.75,-2.5,null,-2.75,-1.5,null,-2.75,-0.5,null,-2.75,0.5,null,-2.75,1.5,null,-2.75,2.5,null,-2.75,3.5,null,-2.75,4.5,null,-2.583333333333333,-4.5,null,-2.583333333333333,-3.5,null,-2.583333333333333,-2.5,null,-2.583333333333333,-1.5,null,-2.583333333333333,-0.5,null,-2.583333333333333,0.5,null,-2.583333333333333,1.5,null,-2.583333333333333,2.5,null,-2.583333333333333,3.5,null,-2.583333333333333,4.5,null,-2.4166666666666665,-4.5,null,-2.4166666666666665,-3.5,null,-2.4166666666666665,-2.5,null,-2.4166666666666665,-1.5,null,-2.4166666666666665,-0.5,null,-2.4166666666666665,0.5,null,-2.4166666666666665,1.5,null,-2.4166666666666665,2.5,null,-2.4166666666666665,3.5,null,-2.4166666666666665,4.5,null,-2.25,-4.5,null,-2.25,-3.5,null,-2.25,-2.5,null,-2.25,-1.5,null,-2.25,-0.5,null,-2.25,0.5,null,-2.25,1.5,null,-2.25,2.5,null,-2.25,3.5,null,-2.25,4.5,null,-2.083333333333333,-4.5,null,-2.083333333333333,-3.5,null,-2.083333333333333,-2.5,null,-2.083333333333333,-1.5,null,-2.083333333333333,-0.5,null,-2.083333333333333,0.5,null,-2.083333333333333,1.5,null,-2.083333333333333,2.5,null,-2.083333333333333,3.5,null,-2.083333333333333,4.5,null,-1.9166666666666665,-4.5,null,-1.9166666666666665,-3.5,null,-1.9166666666666665,-2.5,null,-1.9166666666666665,-1.5,null,-1.9166666666666665,-0.5,null,-1.9166666666666665,0.5,null,-1.9166666666666665,1.5,null,-1.9166666666666665,2.5,null,-1.9166666666666665,3.5,null,-1.9166666666666665,4.5,null,-1.75,-4.5,null,-1.75,-3.5,null,-1.75,-2.5,null,-1.75,-1.5,null,-1.75,-0.5,null,-1.75,0.5,null,-1.75,1.5,null,-1.75,2.5,null,-1.75,3.5,null,-1.75,4.5,null,-4.5,0,null,-3.5,0,null,-2.5,0,null,-1.5,0,null,-0.5,0,null,0.5,0,null,1.5,0,null,2.5,0,null,3.5,0,null,4.5,0,null],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":[\"rgba(200, 200, 200, 0.8)\",\"rgba(0, 0, 140, 0.8)\",\"rgba(0, 0, 144, 0.8)\",\"rgba(0, 0, 120, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 139, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 148, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 116, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 134, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 139, 0.8)\",\"rgba(0, 0, 208, 0.8)\",\"rgba(0, 0, 154, 0.8)\",\"rgba(0, 0, 201, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(141, 0, 0, 0.8)\",\"rgba(0, 0, 204, 0.8)\",\"rgba(0, 0, 141, 0.8)\",\"rgba(0, 0, 167, 0.8)\",\"rgba(0, 0, 143, 0.8)\",\"rgba(0, 0, 168, 0.8)\",\"rgba(0, 0, 182, 0.8)\",\"rgba(0, 0, 175, 0.8)\",\"rgba(0, 0, 167, 0.8)\",\"rgba(0, 0, 182, 0.8)\",\"rgba(0, 0, 157, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(210, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(123, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(213, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(177, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 122, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(136, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 235, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(212, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 200, 0, 0.8)\"],\"line\":{\"color\":\"white\",\"width\":2},\"showscale\":false,\"size\":[25,13.88634741306305,14.348962306976318,11.999486684799194,10.555323660373688,11.408439129590988,13.807395696640015,10.486983824521303,14.647832363843918,11.303671263158321,11.559007056057453,11.20350707322359,13.295377418398857,10.006376649835147,11.234190985560417,10.039519821293652,10.405925959348679,13.837686330080032,20.5106383562088,15.235490947961807,19.838036000728607,10.726175960153341,14.052293002605438,20.11831521987915,13.986605256795883,16.491978019475937,14.168067425489426,16.58760815858841,17.94460654258728,17.3120079934597,16.492707580327988,17.95680344104767,15.589025765657425,96.59568309783936,28.638747334480286,20.739554166793823,25.92975914478302,33.74471664428711,12.322102561593056,45.83423376083374,35.449321269989014,73.17787408828735,48.813241720199585,20.95939427614212,30.629366636276245,46.45243287086487,91.59271955490112,25.92244565486908,120.7052230834961,10.464288871735334,58.719562292099,11.058234944939613,11.074413359165192,11.435852199792862,25.80473244190216,28.537744283676147,110.7080078125,113.49835634231567,25.465238094329834,112.69098997116089,100.31800985336304,31.222227811813354,81.60214900970459,28.295798301696777,52.715394496917725,84.69929933547974,102.27987766265869,121.51640892028809,79.8470687866211,102.42294311523438,17.454589754343033,42.448906898498535,42.032835483551025,12.187219634652138,35.299920439720154,10.526635758578777,13.578399866819382,92.47470140457153,111.37942552566528,38.032774925231934,50.34090280532837,35.56330442428589,100.59713840484619,23.08141142129898,35.344661474227905,32.07637548446655,112.37473964691162,33.964534401893616,28.570131063461304,87.93549537658691,33.401604890823364,40.17289876937866,20.83968460559845,89.96939659118652,172.39634037017822,63.06716203689575,105.80376386642456,75.027596950531,74.42272663116455,77.57152557373047,34.92941617965698,105.27300119400024,70.61537742614746,25]},\"mode\":\"markers\",\"text\":[\"Imagen de entrada\",\"conv1 neurona 0\\u003cbr\\u003eActivación: 0.2591\",\"conv1 neurona 1\\u003cbr\\u003eActivación: 0.2899\",\"conv1 neurona 2\\u003cbr\\u003eActivación: 0.1333\",\"conv1 neurona 3\\u003cbr\\u003eActivación: 0.0370\",\"conv1 neurona 4\\u003cbr\\u003eActivación: 0.0939\",\"conv1 neurona 5\\u003cbr\\u003eActivación: 0.2538\",\"conv1 neurona 6\\u003cbr\\u003eActivación: 0.0325\",\"conv1 neurona 7\\u003cbr\\u003eActivación: 0.3099\",\"conv1 neurona 8\\u003cbr\\u003eActivación: 0.0869\",\"conv1 neurona 9\\u003cbr\\u003eActivación: 0.1039\",\"conv1 neurona 10\\u003cbr\\u003eActivación: 0.0802\",\"conv1 neurona 11\\u003cbr\\u003eActivación: 0.2197\",\"conv1 neurona 12\\u003cbr\\u003eActivación: 0.0004\",\"conv1 neurona 13\\u003cbr\\u003eActivación: 0.0823\",\"conv1 neurona 14\\u003cbr\\u003eActivación: -0.0026\",\"conv1 neurona 15\\u003cbr\\u003eActivación: 0.0271\",\"conv2 neurona 0\\u003cbr\\u003eActivación: 0.2558\",\"conv2 neurona 1\\u003cbr\\u003eActivación: 0.7007\",\"conv2 neurona 2\\u003cbr\\u003eActivación: 0.3490\",\"conv2 neurona 3\\u003cbr\\u003eActivación: 0.6559\",\"conv2 neurona 4\\u003cbr\\u003eActivación: -0.0484\",\"conv2 neurona 5\\u003cbr\\u003eActivación: -0.2702\",\"conv2 neurona 6\\u003cbr\\u003eActivación: 0.6746\",\"conv2 neurona 7\\u003cbr\\u003eActivación: 0.2658\",\"conv2 neurona 8\\u003cbr\\u003eActivación: 0.4328\",\"conv2 neurona 9\\u003cbr\\u003eActivación: 0.2779\",\"conv2 neurona 10\\u003cbr\\u003eActivación: 0.4392\",\"conv2 neurona 11\\u003cbr\\u003eActivación: 0.5296\",\"conv2 neurona 12\\u003cbr\\u003eActivación: 0.4875\",\"conv2 neurona 13\\u003cbr\\u003eActivación: 0.4328\",\"conv2 neurona 14\\u003cbr\\u003eActivación: 0.5305\",\"conv2 neurona 15\\u003cbr\\u003eActivación: 0.3726\",\"fc1 neurona 0\\u003cbr\\u003eActivación: 5.7730\",\"fc1 neurona 1\\u003cbr\\u003eActivación: -1.2426\",\"fc1 neurona 2\\u003cbr\\u003eActivación: -0.7160\",\"fc1 neurona 3\\u003cbr\\u003eActivación: -1.0620\",\"fc1 neurona 4\\u003cbr\\u003eActivación: -1.5830\",\"fc1 neurona 5\\u003cbr\\u003eActivación: -0.1548\",\"fc1 neurona 6\\u003cbr\\u003eActivación: -2.3889\",\"fc1 neurona 7\\u003cbr\\u003eActivación: -1.6966\",\"fc1 neurona 8\\u003cbr\\u003eActivación: 4.2119\",\"fc1 neurona 9\\u003cbr\\u003eActivación: -2.5875\",\"fc1 neurona 10\\u003cbr\\u003eActivación: -0.7306\",\"fc1 neurona 11\\u003cbr\\u003eActivación: -1.3753\",\"fc1 neurona 12\\u003cbr\\u003eActivación: -2.4302\",\"fc1 neurona 13\\u003cbr\\u003eActivación: 5.4395\",\"fc1 neurona 14\\u003cbr\\u003eActivación: -1.0615\",\"fc1 neurona 15\\u003cbr\\u003eActivación: 7.3803\",\"fc1 neurona 16\\u003cbr\\u003eActivación: -0.0310\",\"fc1 neurona 17\\u003cbr\\u003eActivación: 3.2480\",\"fc1 neurona 18\\u003cbr\\u003eActivación: -0.0705\",\"fc1 neurona 19\\u003cbr\\u003eActivación: -0.0716\",\"fc1 neurona 20\\u003cbr\\u003eActivación: -0.0957\",\"fc1 neurona 21\\u003cbr\\u003eActivación: -1.0536\",\"fc1 neurona 22\\u003cbr\\u003eActivación: -1.2358\",\"fc1 neurona 23\\u003cbr\\u003eActivación: 6.7139\",\"fc1 neurona 24\\u003cbr\\u003eActivación: 6.8999\",\"fc1 neurona 25\\u003cbr\\u003eActivación: 1.0310\",\"fc1 neurona 26\\u003cbr\\u003eActivación: 6.8461\",\"fc1 neurona 27\\u003cbr\\u003eActivación: 6.0212\",\"fc1 neurona 28\\u003cbr\\u003eActivación: -1.4148\",\"fc1 neurona 29\\u003cbr\\u003eActivación: 4.7735\",\"fc1 neurona 30\\u003cbr\\u003eActivación: 1.2197\",\"fc1 neurona 31\\u003cbr\\u003eActivación: -2.8477\",\"fc1 neurona 32\\u003cbr\\u003eActivación: 4.9800\",\"fc1 neurona 33\\u003cbr\\u003eActivación: 6.1520\",\"fc1 neurona 34\\u003cbr\\u003eActivación: 7.4344\",\"fc1 neurona 35\\u003cbr\\u003eActivación: 4.6565\",\"fc1 neurona 36\\u003cbr\\u003eActivación: 6.1615\",\"fc1 neurona 37\\u003cbr\\u003eActivación: -0.4970\",\"fc1 neurona 38\\u003cbr\\u003eActivación: -2.1633\",\"fc1 neurona 39\\u003cbr\\u003eActivación: -2.1355\",\"fc1 neurona 40\\u003cbr\\u003eActivación: 0.1458\",\"fc1 neurona 41\\u003cbr\\u003eActivación: -1.6867\",\"fc1 neurona 42\\u003cbr\\u003eActivación: 0.0351\",\"fc1 neurona 43\\u003cbr\\u003eActivación: -0.2386\",\"fc1 neurona 44\\u003cbr\\u003eActivación: 5.4983\",\"fc1 neurona 45\\u003cbr\\u003eActivación: 6.7586\",\"fc1 neurona 46\\u003cbr\\u003eActivación: -1.8689\",\"fc1 neurona 47\\u003cbr\\u003eActivación: -2.6894\",\"fc1 neurona 48\\u003cbr\\u003eActivación: -1.7042\",\"fc1 neurona 49\\u003cbr\\u003eActivación: 6.0398\",\"fc1 neurona 50\\u003cbr\\u003eActivación: 0.8721\",\"fc1 neurona 51\\u003cbr\\u003eActivación: -1.6896\",\"fc1 neurona 52\\u003cbr\\u003eActivación: -1.4718\",\"fc1 neurona 53\\u003cbr\\u003eActivación: 6.8250\",\"fc1 neurona 54\\u003cbr\\u003eActivación: -1.5976\",\"fc1 neurona 55\\u003cbr\\u003eActivación: -1.2380\",\"fc1 neurona 56\\u003cbr\\u003eActivación: 5.1957\",\"fc1 neurona 57\\u003cbr\\u003eActivación: 1.5601\",\"fc1 neurona 58\\u003cbr\\u003eActivación: -2.0115\",\"fc1 neurona 59\\u003cbr\\u003eActivación: -0.7226\",\"fc2 neurona 0\\u003cbr\\u003eActivación: -5.3313\",\"fc2 neurona 1\\u003cbr\\u003eActivación: 10.8264\",\"fc2 neurona 2\\u003cbr\\u003eActivación: -3.5378\",\"fc2 neurona 3\\u003cbr\\u003eActivación: -6.3869\",\"fc2 neurona 4\\u003cbr\\u003eActivación: -4.3352\",\"fc2 neurona 5\\u003cbr\\u003eActivación: -4.2948\",\"fc2 neurona 6\\u003cbr\\u003eActivación: -4.5048\",\"fc2 neurona 7\\u003cbr\\u003eActivación: -1.6620\",\"fc2 neurona 8\\u003cbr\\u003eActivación: -6.3515\",\"fc2 neurona 9\\u003cbr\\u003eActivación: 4.0410\",\"Clase predicha: 1\"],\"x\":[0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5],\"y\":[0,-4.6875,-4.0625,-3.4375,-2.8125,-2.1875,-1.5625,-0.9375,-0.3125,0.3125,0.9375,1.5625,2.1875,2.8125,3.4375,4.0625,4.6875,-4.6875,-4.0625,-3.4375,-2.8125,-2.1875,-1.5625,-0.9375,-0.3125,0.3125,0.9375,1.5625,2.1875,2.8125,3.4375,4.0625,4.6875,-4.916666666666666,-4.75,-4.583333333333333,-4.416666666666666,-4.25,-4.083333333333333,-3.9166666666666665,-3.75,-3.583333333333333,-3.4166666666666665,-3.25,-3.083333333333333,-2.9166666666666665,-2.75,-2.583333333333333,-2.4166666666666665,-2.25,-2.083333333333333,-1.9166666666666665,-1.75,-1.5833333333333333,-1.4166666666666665,-1.25,-1.0833333333333333,-0.9166666666666666,-0.75,-0.5833333333333333,-0.41666666666666663,-0.25,-0.08333333333333333,0.08333333333333333,0.25,0.41666666666666663,0.5833333333333333,0.75,0.9166666666666666,1.0833333333333333,1.25,1.4166666666666665,1.5833333333333333,1.75,1.9166666666666665,2.083333333333333,2.25,2.4166666666666665,2.583333333333333,2.75,2.9166666666666665,3.083333333333333,3.25,3.4166666666666665,3.583333333333333,3.75,3.9166666666666665,4.083333333333333,4.25,4.416666666666666,4.583333333333333,4.75,4.916666666666666,-4.5,-3.5,-2.5,-1.5,-0.5,0.5,1.5,2.5,3.5,4.5,0],\"type\":\"scatter\"}],                        {\"annotations\":[{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"INPUT\\u003cbr\\u003e(1 neuronas)\",\"x\":0,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"CONV1\\u003cbr\\u003e(16 neuronas)\",\"x\":1,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"CONV2\\u003cbr\\u003e(16 neuronas)\",\"x\":2,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"FC1\\u003cbr\\u003e(60 neuronas)\",\"x\":3,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"FC2\\u003cbr\\u003e(10 neuronas)\",\"x\":4,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"OUTPUT\\u003cbr\\u003e(1 neuronas)\",\"x\":5,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"align\":\"left\",\"showarrow\":false,\"text\":\"\\u003cb\\u003eLeyenda:\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eColor de nodo:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• \\u003cspan style='color:blue'\\u003eAzul\\u003c\\u002fspan\\u003e: Activación positiva\\u003cbr\\u003e• \\u003cspan style='color:red'\\u003eRojo\\u003c\\u002fspan\\u003e: Activación negativa\\u003cbr\\u003e• \\u003cspan style='color:gray'\\u003eGris\\u003c\\u002fspan\\u003e: Activación cercana a cero\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eTamaño de nodo:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• Mayor tamaño = Mayor magnitud de activación\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eColor de conexión:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• \\u003cspan style='color:blue'\\u003eAzul\\u003c\\u002fspan\\u003e: Peso positivo\\u003cbr\\u003e• \\u003cspan style='color:red'\\u003eRojo\\u003c\\u002fspan\\u003e: Peso negativo\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eGrosor de conexión:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• Mayor grosor = Mayor magnitud del peso\",\"x\":1.02,\"xref\":\"paper\",\"y\":0.5,\"yref\":\"paper\"},{\"align\":\"left\",\"showarrow\":false,\"text\":\"\\u003cb\\u003e¿Por qué hay activaciones negativas?\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eEn redes neuronales, las activaciones negativas\\u003cbr\\u003eocurren cuando el input a una neurona produce\\u003cbr\\u003eun valor negativo. Esto es común en capas con\\u003cbr\\u003efunciones de activación como ReLU, tanh o\\u003cbr\\u003efunciones lineales. Las activaciones negativas\\u003cbr\\u003epueden indicar inhibición o respuesta contraria\\u003cbr\\u003ea ciertas características de entrada.\",\"x\":1.02,\"xref\":\"paper\",\"y\":0.05,\"yref\":\"paper\"}],\"height\":1000,\"hovermode\":\"closest\",\"margin\":{\"b\":20,\"l\":5,\"r\":5,\"t\":40},\"showlegend\":false,\"title\":{\"font\":{\"size\":16},\"text\":\"Red Neuronal - Imagen 5\\u003cbr\\u003eEtiqueta real: 1, Predicción: 1\"},\"width\":1600,\"xaxis\":{\"showgrid\":false,\"showticklabels\":false,\"zeroline\":false},\"yaxis\":{\"showgrid\":false,\"showticklabels\":false,\"zeroline\":false},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f7d0e9c6-25de-45df-8723-98c0d574a9b5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = visualizar_red_neuronal_interactiva(per_image_neuron_data, imagen_index=8)\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMCBtOMt-mwI",
        "outputId": "04405d8c-ec8f-49b4-d736-6e7fbb5fc46d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"e556b0dd-f79a-4115-9310-f29c1d56a7ad\" class=\"plotly-graph-div\" style=\"height:1000px; width:1600px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e556b0dd-f79a-4115-9310-f29c1d56a7ad\")) {                    Plotly.newPlot(                        \"e556b0dd-f79a-4115-9310-f29c1d56a7ad\",                        [{\"hoverinfo\":\"text\",\"line\":{\"width\":0.5},\"marker\":{\"color\":[\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(0, 0, 255, 0.49548879861831663)\",\"rgba(0, 0, 255, 0.478199428319931)\",\"rgba(0, 0, 255, 0.14466232284903527)\",\"rgba(255, 0, 0, 0.177683687210083)\",\"rgba(0, 0, 255, 0.3098765522241592)\",\"rgba(0, 0, 255, 0.3973027229309082)\",\"rgba(255, 0, 0, 0.2523659825325012)\",\"rgba(0, 0, 255, 0.47543243169784544)\",\"rgba(255, 0, 0, 0.15969223380088807)\",\"rgba(0, 0, 255, 0.1734786093235016)\",\"rgba(0, 0, 255, 0.5192788004875183)\",\"rgba(0, 0, 255, 0.5156277775764465)\",\"rgba(255, 0, 0, 0.17516806572675706)\",\"rgba(0, 0, 255, 0.3722387909889221)\",\"rgba(255, 0, 0, 0.17676905393600464)\",\"rgba(0, 0, 255, 0.35481775403022764)\",\"rgba(255, 0, 0, 0.18479847759008408)\",\"rgba(0, 0, 255, 0.11256333477795125)\",\"rgba(0, 0, 255, 0.17643774151802064)\",\"rgba(0, 0, 255, 0.32657302021980283)\",\"rgba(0, 0, 255, 0.44209170937538145)\",\"rgba(0, 0, 255, 0.41036651134490965)\",\"rgba(255, 0, 0, 0.2747374832630157)\",\"rgba(255, 0, 0, 0.2693179607391357)\",\"rgba(255, 0, 0, 0.2957087069749832)\",\"rgba(0, 0, 255, 0.16043727323412896)\",\"rgba(0, 0, 255, 0.10588635914027691)\",\"rgba(255, 0, 0, 0.2954487860202789)\",\"rgba(0, 0, 255, 0.5079583287239074)\",\"rgba(0, 0, 255, 0.2742736846208572)\",\"rgba(255, 0, 0, 0.22870168685913086)\",\"rgba(0, 0, 255, 0.37615121006965635)\",\"rgba(0, 0, 255, 0.2752258390188217)\",\"rgba(255, 0, 0, 0.10932643283158541)\",\"rgba(255, 0, 0, 0.299082151055336)\",\"rgba(0, 0, 255, 0.1544593557715416)\",\"rgba(255, 0, 0, 0.1878574475646019)\",\"rgba(255, 0, 0, 0.10299682095646859)\",\"rgba(0, 0, 255, 0.31407847106456754)\",\"rgba(0, 0, 255, 0.38076297044754026)\",\"rgba(0, 0, 255, 0.17920287549495698)\",\"rgba(0, 0, 255, 0.2862309873104095)\",\"rgba(0, 0, 255, 0.4426393866539001)\",\"rgba(255, 0, 0, 0.21012519896030427)\",\"rgba(0, 0, 255, 0.4459571897983551)\",\"rgba(255, 0, 0, 0.37126166224479673)\",\"rgba(255, 0, 0, 0.10629299692809582)\",\"rgba(0, 0, 255, 0.3153266370296478)\",\"rgba(0, 0, 255, 0.17139802724123002)\",\"rgba(0, 0, 255, 0.1725584849715233)\",\"rgba(0, 0, 255, 0.24965689182281495)\",\"rgba(0, 0, 255, 0.1810677632689476)\",\"rgba(0, 0, 255, 0.1727474093437195)\",\"rgba(255, 0, 0, 0.2172143742442131)\",\"rgba(255, 0, 0, 0.5470112025737762)\",\"rgba(255, 0, 0, 0.35271883606910703)\",\"rgba(255, 0, 0, 0.3498561441898346)\",\"rgba(0, 0, 255, 0.1630682334303856)\",\"rgba(0, 0, 255, 0.23590829074382783)\",\"rgba(0, 0, 255, 0.3403503060340881)\",\"rgba(0, 0, 255, 0.4222856521606445)\",\"rgba(255, 0, 0, 0.21231402307748795)\",\"rgba(0, 0, 255, 0.2133197695016861)\",\"rgba(0, 0, 255, 0.47085214257240293)\",\"rgba(0, 0, 255, 0.48222280144691465)\",\"rgba(0, 0, 255, 0.15390509217977524)\",\"rgba(0, 0, 255, 0.2591764390468597)\",\"rgba(255, 0, 0, 0.1489599294960499)\",\"rgba(0, 0, 255, 0.20857946425676346)\",\"rgba(255, 0, 0, 0.3309042364358902)\",\"rgba(0, 0, 255, 0.45697722434997556)\",\"rgba(255, 0, 0, 0.38950235247612)\",\"rgba(0, 0, 255, 0.3325397729873657)\",\"rgba(0, 0, 255, 0.27854142189025877)\",\"rgba(0, 0, 255, 0.46781996488571165)\",\"rgba(0, 0, 255, 0.23787814974784852)\",\"rgba(255, 0, 0, 0.1643598809838295)\",\"rgba(255, 0, 0, 0.15681707710027695)\",\"rgba(255, 0, 0, 0.2021089032292366)\",\"rgba(0, 0, 255, 0.3239493280649185)\",\"rgba(0, 0, 255, 0.2816739737987518)\",\"rgba(0, 0, 255, 0.41357854604721067)\",\"rgba(255, 0, 0, 0.2777995079755783)\",\"rgba(255, 0, 0, 0.2666818857192993)\",\"rgba(0, 0, 255, 0.42318800687789915)\",\"rgba(0, 0, 255, 0.18088799566030503)\",\"rgba(0, 0, 255, 0.2810082197189331)\",\"rgba(0, 0, 255, 0.28109431862831114)\",\"rgba(0, 0, 255, 0.4472943007946014)\",\"rgba(0, 0, 255, 0.27397598922252653)\",\"rgba(0, 0, 255, 0.24490857720375062)\",\"rgba(0, 0, 255, 0.46724478006362913)\",\"rgba(0, 0, 255, 0.4688847422599792)\",\"rgba(0, 0, 255, 0.13124589286744595)\",\"rgba(255, 0, 0, 0.187206169962883)\",\"rgba(255, 0, 0, 0.1550176203250885)\",\"rgba(0, 0, 255, 0.15971722677350045)\",\"rgba(0, 0, 255, 0.3590250968933105)\",\"rgba(0, 0, 255, 0.48915732502937315)\",\"rgba(0, 0, 255, 0.34438098073005674)\",\"rgba(0, 0, 255, 0.38259721398353574)\",\"rgba(0, 0, 255, 0.3597565948963165)\",\"rgba(255, 0, 0, 0.36958218216896055)\",\"rgba(255, 0, 0, 0.2608051061630249)\",\"rgba(0, 0, 255, 0.36007843017578123)\",\"rgba(255, 0, 0, 0.29213166832923887)\",\"rgba(0, 0, 255, 0.13488203138113022)\",\"rgba(255, 0, 0, 0.3189231365919113)\",\"rgba(255, 0, 0, 0.1583360865712166)\",\"rgba(255, 0, 0, 0.23929422199726105)\",\"rgba(0, 0, 255, 0.4234710395336151)\",\"rgba(255, 0, 0, 0.10799361150711775)\",\"rgba(255, 0, 0, 0.22437988817691804)\",\"rgba(255, 0, 0, 0.13250041976571084)\",\"rgba(0, 0, 255, 0.36733183860778806)\",\"rgba(255, 0, 0, 0.10533227529376746)\",\"rgba(0, 0, 255, 0.23108427822589875)\",\"rgba(0, 0, 255, 0.1269213728606701)\",\"rgba(0, 0, 255, 0.2178092733025551)\",\"rgba(0, 0, 255, 0.4475667834281921)\",\"rgba(0, 0, 255, 0.29243766367435453)\",\"rgba(255, 0, 0, 0.3232967615127563)\",\"rgba(0, 0, 255, 0.48328558206558225)\",\"rgba(0, 0, 255, 0.2952555626630783)\",\"rgba(255, 0, 0, 0.22201957404613495)\",\"rgba(0, 0, 255, 0.291690519452095)\",\"rgba(255, 0, 0, 0.3410115748643875)\",\"rgba(0, 0, 255, 0.23403376936912537)\",\"rgba(255, 0, 0, 0.1590900182723999)\",\"rgba(255, 0, 0, 0.23071313500404358)\",\"rgba(255, 0, 0, 0.16546325236558915)\",\"rgba(0, 0, 255, 0.5442004561424255)\",\"rgba(255, 0, 0, 0.10763260200619698)\",\"rgba(255, 0, 0, 0.3982511222362518)\",\"rgba(0, 0, 255, 0.3256711423397064)\",\"rgba(0, 0, 255, 0.28056963384151457)\",\"rgba(255, 0, 0, 0.3958565354347229)\",\"rgba(0, 0, 255, 0.23396245241165162)\",\"rgba(255, 0, 0, 0.11965508796274663)\",\"rgba(255, 0, 0, 0.36107057929039)\",\"rgba(0, 0, 255, 0.4305442631244659)\",\"rgba(255, 0, 0, 0.16671704351902009)\",\"rgba(0, 0, 255, 0.24037953317165375)\",\"rgba(0, 0, 255, 0.15226198583841324)\",\"rgba(0, 0, 255, 0.1386566072702408)\",\"rgba(255, 0, 0, 0.13595873713493348)\",\"rgba(255, 0, 0, 0.15670680478215218)\",\"rgba(255, 0, 0, 0.1724240615963936)\",\"rgba(255, 0, 0, 0.21617275327444077)\",\"rgba(0, 0, 255, 0.1054623312316835)\",\"rgba(0, 0, 255, 0.17781460136175156)\",\"rgba(0, 0, 255, 0.2550832390785217)\",\"rgba(0, 0, 255, 0.23532580435276032)\",\"rgba(0, 0, 255, 0.1271420180797577)\",\"rgba(0, 0, 255, 0.25483386814594267)\",\"rgba(0, 0, 255, 0.11958501152694226)\",\"rgba(255, 0, 0, 0.17505299150943757)\",\"rgba(255, 0, 0, 0.20065092146396638)\",\"rgba(255, 0, 0, 0.14416592419147492)\",\"rgba(255, 0, 0, 0.19682178497314454)\",\"rgba(255, 0, 0, 0.13787001967430115)\",\"rgba(0, 0, 255, 0.1209137424826622)\",\"rgba(0, 0, 255, 0.11684178784489632)\",\"rgba(0, 0, 255, 0.15851465463638306)\",\"rgba(255, 0, 0, 0.18223507553339005)\",\"rgba(0, 0, 255, 0.11689410023391247)\",\"rgba(0, 0, 255, 0.17545654475688935)\",\"rgba(0, 0, 255, 0.24069390296936036)\",\"rgba(0, 0, 255, 0.15251357331871987)\",\"rgba(0, 0, 255, 0.11536910142749549)\",\"rgba(0, 0, 255, 0.17129435986280442)\",\"rgba(0, 0, 255, 0.10941847078502179)\",\"rgba(255, 0, 0, 0.1767335817217827)\",\"rgba(0, 0, 255, 0.13535399213433266)\",\"rgba(0, 0, 255, 0.15336353927850724)\",\"rgba(0, 0, 255, 0.20378362983465195)\",\"rgba(0, 0, 255, 0.12714175544679165)\",\"rgba(0, 0, 255, 0.16998100876808167)\",\"rgba(0, 0, 255, 0.2746469140052795)\",\"rgba(0, 0, 255, 0.27548803985118864)\",\"rgba(0, 0, 255, 0.16393295526504517)\",\"rgba(255, 0, 0, 0.15806545540690423)\",\"rgba(0, 0, 255, 0.16643111258745194)\",\"rgba(0, 0, 255, 0.2083114892244339)\",\"rgba(255, 0, 0, 0.13128596022725106)\",\"rgba(0, 0, 255, 0.12906168811023236)\",\"rgba(0, 0, 255, 0.20891366004943848)\",\"rgba(255, 0, 0, 0.15957501381635666)\",\"rgba(255, 0, 0, 0.12259227074682713)\",\"rgba(255, 0, 0, 0.14415588453412057)\",\"rgba(0, 0, 255, 0.10972395390272141)\",\"rgba(0, 0, 255, 0.11184686925262213)\",\"rgba(255, 0, 0, 0.15321859195828438)\",\"rgba(0, 0, 255, 0.22908414006233216)\",\"rgba(0, 0, 255, 0.22326332926750184)\",\"rgba(0, 0, 255, 0.19815044701099396)\",\"rgba(255, 0, 0, 0.11162238139659167)\",\"rgba(0, 0, 255, 0.186253322660923)\",\"rgba(255, 0, 0, 0.14448411986231804)\",\"rgba(255, 0, 0, 0.1661178305745125)\",\"rgba(0, 0, 255, 0.20666831582784653)\",\"rgba(255, 0, 0, 0.12844959124922753)\",\"rgba(0, 0, 255, 0.11097518485039473)\",\"rgba(0, 0, 255, 0.1838935896754265)\",\"rgba(0, 0, 255, 0.19256912022829056)\",\"rgba(0, 0, 255, 0.21605315655469895)\",\"rgba(0, 0, 255, 0.2842900365591049)\",\"rgba(255, 0, 0, 0.12351627871394158)\",\"rgba(0, 0, 255, 0.15942964926362038)\",\"rgba(0, 0, 255, 0.19926258772611619)\",\"rgba(255, 0, 0, 0.1916508510708809)\",\"rgba(255, 0, 0, 0.1020223087631166)\",\"rgba(255, 0, 0, 0.12769693918526173)\",\"rgba(0, 0, 255, 0.12617848478257657)\",\"rgba(0, 0, 255, 0.2083066835999489)\",\"rgba(0, 0, 255, 0.2719566285610199)\",\"rgba(0, 0, 255, 0.24234213531017304)\",\"rgba(0, 0, 255, 0.28349374830722807)\",\"rgba(0, 0, 255, 0.1305425487458706)\",\"rgba(0, 0, 255, 0.11249254308640957)\",\"rgba(255, 0, 0, 0.1090659698471427)\",\"rgba(0, 0, 255, 0.1655781701207161)\",\"rgba(255, 0, 0, 0.21959099024534226)\",\"rgba(255, 0, 0, 0.13270576521754265)\",\"rgba(0, 0, 255, 0.1260148234665394)\",\"rgba(255, 0, 0, 0.16755132228136063)\",\"rgba(255, 0, 0, 0.16150597333908082)\",\"rgba(255, 0, 0, 0.19079410880804062)\",\"rgba(0, 0, 255, 0.16053490936756135)\",\"rgba(0, 0, 255, 0.1525706559419632)\",\"rgba(255, 0, 0, 0.16704683601856232)\",\"rgba(255, 0, 0, 0.1014490872854367)\",\"rgba(255, 0, 0, 0.1155973332002759)\",\"rgba(0, 0, 255, 0.12969427965581418)\",\"rgba(255, 0, 0, 0.19690987318754197)\",\"rgba(255, 0, 0, 0.1365249253809452)\",\"rgba(0, 0, 255, 0.16420420110225678)\",\"rgba(255, 0, 0, 0.12144586481153966)\",\"rgba(255, 0, 0, 0.19393659234046937)\",\"rgba(255, 0, 0, 0.10180329836439342)\",\"rgba(0, 0, 255, 0.15597561225295067)\",\"rgba(255, 0, 0, 0.11922614499926568)\",\"rgba(255, 0, 0, 0.1807197540998459)\",\"rgba(0, 0, 255, 0.13591635078191758)\",\"rgba(0, 0, 255, 0.12025427483022214)\",\"rgba(0, 0, 255, 0.10969479233026505)\",\"rgba(0, 0, 255, 0.13583026677370072)\",\"rgba(255, 0, 0, 0.10631299614906312)\",\"rgba(255, 0, 0, 0.14084687680006028)\",\"rgba(255, 0, 0, 0.12094217017292977)\",\"rgba(0, 0, 255, 0.17337053120136262)\",\"rgba(255, 0, 0, 0.12403955534100533)\",\"rgba(0, 0, 255, 0.10931183528155089)\",\"rgba(255, 0, 0, 0.1337904989719391)\",\"rgba(255, 0, 0, 0.14871316999197007)\",\"rgba(255, 0, 0, 0.13775223344564438)\",\"rgba(255, 0, 0, 0.15456002801656724)\",\"rgba(255, 0, 0, 0.15006642267107964)\",\"rgba(255, 0, 0, 0.11680560037493706)\",\"rgba(0, 0, 255, 0.1573251247406006)\",\"rgba(0, 0, 255, 0.15939288064837456)\",\"rgba(0, 0, 255, 0.16052596867084504)\",\"rgba(0, 0, 255, 0.15456433445215226)\",\"rgba(255, 0, 0, 0.12262217178940774)\",\"rgba(0, 0, 255, 0.11019854284822941)\",\"rgba(0, 0, 255, 0.15847012251615525)\",\"rgba(0, 0, 255, 0.16794290244579316)\",\"rgba(0, 0, 255, 0.10782784912735224)\",\"rgba(255, 0, 0, 0.15122115463018418)\",\"rgba(255, 0, 0, 0.15147096887230874)\",\"rgba(255, 0, 0, 0.11417598221451045)\",\"rgba(0, 0, 255, 0.19837329387664795)\",\"rgba(255, 0, 0, 0.12020259201526642)\",\"rgba(0, 0, 255, 0.2163897141814232)\",\"rgba(0, 0, 255, 0.20436511784791947)\",\"rgba(0, 0, 255, 0.21039837449789048)\",\"rgba(0, 0, 255, 0.2790644973516464)\",\"rgba(0, 0, 255, 0.22842857837677003)\",\"rgba(0, 0, 255, 0.25524275600910185)\",\"rgba(0, 0, 255, 0.19176838397979737)\",\"rgba(255, 0, 0, 0.11521802134811879)\",\"rgba(255, 0, 0, 0.24807364344596863)\",\"rgba(255, 0, 0, 0.14195474460721016)\",\"rgba(0, 0, 255, 0.21409225314855576)\",\"rgba(255, 0, 0, 0.1633474662899971)\",\"rgba(255, 0, 0, 0.16295283883810044)\",\"rgba(0, 0, 255, 0.14685645550489426)\",\"rgba(255, 0, 0, 0.14034655913710595)\",\"rgba(0, 0, 255, 0.10358967785723508)\",\"rgba(0, 0, 255, 0.1459873117506504)\",\"rgba(255, 0, 0, 0.12523482404649258)\",\"rgba(0, 0, 255, 0.15648961290717125)\",\"rgba(0, 0, 255, 0.1803705006837845)\",\"rgba(255, 0, 0, 0.22341751158237458)\",\"rgba(0, 0, 255, 0.17330399751663209)\",\"rgba(0, 0, 255, 0.11614801585674286)\",\"rgba(0, 0, 255, 0.13756815567612649)\",\"rgba(0, 0, 255, 0.1353468768298626)\",\"rgba(0, 0, 255, 0.12318404242396355)\",\"rgba(255, 0, 0, 0.1239439569413662)\",\"rgba(255, 0, 0, 0.15918757244944573)\",\"rgba(0, 0, 255, 0.24578624069690705)\",\"rgba(0, 0, 255, 0.15151745304465294)\",\"rgba(255, 0, 0, 0.12972494624555111)\",\"rgba(0, 0, 255, 0.2308891475200653)\",\"rgba(255, 0, 0, 0.10197818316519261)\",\"rgba(255, 0, 0, 0.23131056725978852)\",\"rgba(255, 0, 0, 0.12178245782852173)\",\"rgba(0, 0, 255, 0.14231640323996544)\",\"rgba(255, 0, 0, 0.21269105225801468)\",\"rgba(0, 0, 255, 0.11979728788137436)\",\"rgba(255, 0, 0, 0.11434784382581711)\",\"rgba(0, 0, 255, 0.17585982978343964)\",\"rgba(0, 0, 255, 0.2864547878503799)\",\"rgba(255, 0, 0, 0.19370731562376023)\",\"rgba(255, 0, 0, 0.11561188604682684)\",\"rgba(0, 0, 255, 0.17931382954120637)\",\"rgba(255, 0, 0, 0.18723654597997666)\",\"rgba(0, 0, 255, 0.1310098957270384)\",\"rgba(255, 0, 0, 0.10726854288950563)\",\"rgba(0, 0, 255, 0.12043144963681698)\",\"rgba(255, 0, 0, 0.14788494110107422)\",\"rgba(255, 0, 0, 0.19651190787553788)\",\"rgba(0, 0, 255, 0.10894980225712061)\",\"rgba(255, 0, 0, 0.11024332363158465)\",\"rgba(255, 0, 0, 0.1113961324095726)\",\"rgba(0, 0, 255, 0.19189245104789734)\",\"rgba(0, 0, 255, 0.1650133341550827)\",\"rgba(255, 0, 0, 0.12992463670670987)\",\"rgba(0, 0, 255, 0.20241249203681946)\",\"rgba(0, 0, 255, 0.1984674021601677)\",\"rgba(0, 0, 255, 0.13412004932761193)\",\"rgba(0, 0, 255, 0.16683219224214554)\",\"rgba(255, 0, 0, 0.12224020659923554)\",\"rgba(0, 0, 255, 0.12131464891135693)\",\"rgba(255, 0, 0, 0.12754815481603146)\",\"rgba(255, 0, 0, 0.21309650540351868)\",\"rgba(255, 0, 0, 0.1770358145236969)\",\"rgba(0, 0, 255, 0.11529336255043746)\",\"rgba(0, 0, 255, 0.20691694170236588)\",\"rgba(255, 0, 0, 0.14987320676445962)\",\"rgba(255, 0, 0, 0.1559365823864937)\",\"rgba(255, 0, 0, 0.1450836792588234)\",\"rgba(0, 0, 255, 0.22001517415046692)\",\"rgba(255, 0, 0, 0.20618361085653306)\",\"rgba(0, 0, 255, 0.14627396166324616)\",\"rgba(0, 0, 255, 0.28120065331459043)\",\"rgba(0, 0, 255, 0.11846693083643914)\",\"rgba(255, 0, 0, 0.17231620699167252)\",\"rgba(0, 0, 255, 0.13356506302952767)\",\"rgba(0, 0, 255, 0.10218037003651262)\",\"rgba(0, 0, 255, 0.21720247566699982)\",\"rgba(0, 0, 255, 0.1685992017388344)\",\"rgba(255, 0, 0, 0.14049494117498398)\",\"rgba(0, 0, 255, 0.19640442579984665)\",\"rgba(0, 0, 255, 0.2749678999185562)\",\"rgba(0, 0, 255, 0.10281565403565765)\",\"rgba(0, 0, 255, 0.10951499957591296)\",\"rgba(255, 0, 0, 0.11713052950799466)\",\"rgba(0, 0, 255, 0.13773346543312073)\",\"rgba(255, 0, 0, 0.1716003179550171)\",\"rgba(0, 0, 255, 0.17127328962087632)\",\"rgba(0, 0, 255, 0.12467096224427224)\",\"rgba(0, 0, 255, 0.19485531598329545)\",\"rgba(0, 0, 255, 0.14595022648572922)\",\"rgba(0, 0, 255, 0.2692528277635574)\",\"rgba(0, 0, 255, 0.10690880464389921)\",\"rgba(0, 0, 255, 0.20661641508340836)\",\"rgba(0, 0, 255, 0.15907241627573968)\",\"rgba(255, 0, 0, 0.14439767450094224)\",\"rgba(255, 0, 0, 0.1238613747060299)\",\"rgba(255, 0, 0, 0.19980181604623795)\",\"rgba(255, 0, 0, 0.14902878031134606)\",\"rgba(0, 0, 255, 0.14211461916565896)\",\"rgba(0, 0, 255, 0.256250849366188)\",\"rgba(0, 0, 255, 0.14176847636699677)\",\"rgba(0, 0, 255, 0.262353903055191)\",\"rgba(0, 0, 255, 0.18315620571374894)\",\"rgba(0, 0, 255, 0.12715127542614937)\",\"rgba(255, 0, 0, 0.11525964587926865)\",\"rgba(0, 0, 255, 0.1643814578652382)\",\"rgba(0, 0, 255, 0.19453471750020981)\",\"rgba(0, 0, 255, 0.11423598732799292)\",\"rgba(255, 0, 0, 0.1350361578166485)\",\"rgba(0, 0, 255, 0.24258751273155213)\",\"rgba(0, 0, 255, 0.21224818229675294)\",\"rgba(0, 0, 255, 0.12961379289627076)\",\"rgba(0, 0, 255, 0.2269052028656006)\",\"rgba(0, 0, 255, 0.23853234052658082)\",\"rgba(255, 0, 0, 0.20922084748744965)\",\"rgba(255, 0, 0, 0.22195508182048798)\",\"rgba(255, 0, 0, 0.21937642097473145)\",\"rgba(255, 0, 0, 0.18703076094388962)\",\"rgba(0, 0, 255, 0.19272335469722748)\",\"rgba(255, 0, 0, 0.2073676198720932)\",\"rgba(255, 0, 0, 0.11221138704568148)\",\"rgba(0, 0, 255, 0.20201732069253922)\",\"rgba(0, 0, 255, 0.15784472450613976)\",\"rgba(0, 0, 255, 0.13575390949845315)\",\"rgba(0, 0, 255, 0.17514524459838868)\",\"rgba(0, 0, 255, 0.10388252851553262)\",\"rgba(255, 0, 0, 0.11220337487757207)\",\"rgba(255, 0, 0, 0.20671365261077881)\",\"rgba(0, 0, 255, 0.14328499361872674)\",\"rgba(255, 0, 0, 0.12937498688697815)\",\"rgba(255, 0, 0, 0.1867907926440239)\",\"rgba(0, 0, 255, 0.1518893264234066)\",\"rgba(0, 0, 255, 0.12568084746599198)\",\"rgba(0, 0, 255, 0.10187984190415592)\",\"rgba(0, 0, 255, 0.20666269063949586)\",\"rgba(255, 0, 0, 0.1337924286723137)\",\"rgba(255, 0, 0, 0.12738125808537007)\",\"rgba(0, 0, 255, 0.23599146902561188)\",\"rgba(255, 0, 0, 0.1201748013496399)\",\"rgba(0, 0, 255, 0.1866667553782463)\",\"rgba(0, 0, 255, 0.16635120511054993)\",\"rgba(255, 0, 0, 0.12595003321766854)\",\"rgba(0, 0, 255, 0.11274861209094525)\",\"rgba(255, 0, 0, 0.1086600873619318)\",\"rgba(0, 0, 255, 0.17839718461036683)\",\"rgba(0, 0, 255, 0.14725469276309014)\",\"rgba(255, 0, 0, 0.10861964654177428)\",\"rgba(0, 0, 255, 0.21814913898706437)\",\"rgba(0, 0, 255, 0.1377427414059639)\",\"rgba(0, 0, 255, 0.13983725234866143)\",\"rgba(0, 0, 255, 0.24521875977516175)\",\"rgba(255, 0, 0, 0.11230627484619618)\",\"rgba(255, 0, 0, 0.1379784442484379)\",\"rgba(0, 0, 255, 0.11385879050940276)\",\"rgba(255, 0, 0, 0.1376894883811474)\",\"rgba(0, 0, 255, 0.16396060436964036)\",\"rgba(255, 0, 0, 0.19228187799453736)\",\"rgba(255, 0, 0, 0.16415797024965287)\",\"rgba(0, 0, 255, 0.15122511833906174)\",\"rgba(0, 0, 255, 0.2187347322702408)\",\"rgba(0, 0, 255, 0.15166426301002503)\",\"rgba(0, 0, 255, 0.2489807516336441)\",\"rgba(0, 0, 255, 0.11504863426089287)\",\"rgba(0, 0, 255, 0.1371341183781624)\",\"rgba(0, 0, 255, 0.18508042246103287)\",\"rgba(0, 0, 255, 0.18222902566194535)\",\"rgba(0, 0, 255, 0.22881997227668763)\",\"rgba(0, 0, 255, 0.23717131316661835)\",\"rgba(0, 0, 255, 0.16285112351179123)\",\"rgba(0, 0, 255, 0.11251801289618016)\",\"rgba(255, 0, 0, 0.12101685851812363)\",\"rgba(255, 0, 0, 0.15124419182538987)\",\"rgba(0, 0, 255, 0.1134767796844244)\",\"rgba(0, 0, 255, 0.12203266695141793)\",\"rgba(0, 0, 255, 0.10303762708790601)\",\"rgba(0, 0, 255, 0.2956202834844589)\",\"rgba(0, 0, 255, 0.14800493642687798)\",\"rgba(0, 0, 255, 0.17198162376880646)\",\"rgba(0, 0, 255, 0.12344769425690175)\",\"rgba(0, 0, 255, 0.10664820661768318)\",\"rgba(255, 0, 0, 0.10932208448648453)\",\"rgba(0, 0, 255, 0.11176549028605223)\",\"rgba(255, 0, 0, 0.1951811671257019)\",\"rgba(255, 0, 0, 0.23855357468128205)\",\"rgba(0, 0, 255, 0.19646214544773102)\",\"rgba(0, 0, 255, 0.23631613552570344)\",\"rgba(255, 0, 0, 0.13550074622035027)\",\"rgba(255, 0, 0, 0.13415663540363312)\",\"rgba(0, 0, 255, 0.10178810534998775)\",\"rgba(255, 0, 0, 0.10528963981196285)\",\"rgba(0, 0, 255, 0.11088422257453204)\",\"rgba(255, 0, 0, 0.11812226139009)\",\"rgba(255, 0, 0, 0.12855682745575905)\",\"rgba(255, 0, 0, 0.10194849295075983)\",\"rgba(0, 0, 255, 0.11335254777222872)\",\"rgba(255, 0, 0, 0.10477193295955659)\",\"rgba(0, 0, 255, 0.13525649756193162)\",\"rgba(255, 0, 0, 0.10138341111596674)\",\"rgba(255, 0, 0, 0.11898531429469586)\",\"rgba(255, 0, 0, 0.10669446168467403)\",\"rgba(0, 0, 255, 0.11909543946385384)\",\"rgba(0, 0, 255, 0.11284026727080346)\",\"rgba(0, 0, 255, 0.12494340389966965)\",\"rgba(255, 0, 0, 0.13423922508955002)\",\"rgba(0, 0, 255, 0.10875162612646819)\",\"rgba(0, 0, 255, 0.12378135360777379)\",\"rgba(0, 0, 255, 0.11301876362413169)\",\"rgba(0, 0, 255, 0.1392383858561516)\",\"rgba(255, 0, 0, 0.12818730287253857)\",\"rgba(255, 0, 0, 0.10952464435249568)\",\"rgba(255, 0, 0, 0.1302299577742815)\",\"rgba(0, 0, 255, 0.11377327833324671)\",\"rgba(255, 0, 0, 0.11041976548731328)\",\"rgba(255, 0, 0, 0.13126677870750428)\",\"rgba(0, 0, 255, 0.13708266839385033)\",\"rgba(0, 0, 255, 0.1256254728883505)\",\"rgba(255, 0, 0, 0.12527118287980557)\",\"rgba(255, 0, 0, 0.12645238488912583)\",\"rgba(0, 0, 255, 0.11784278266131878)\",\"rgba(0, 0, 255, 0.1245390459895134)\",\"rgba(255, 0, 0, 0.10199589133262635)\",\"rgba(0, 0, 255, 0.12229139767587185)\",\"rgba(255, 0, 0, 0.12321538887917996)\",\"rgba(255, 0, 0, 0.10967915635555983)\",\"rgba(0, 0, 255, 0.13354786708950997)\",\"rgba(255, 0, 0, 0.13497540578246117)\",\"rgba(255, 0, 0, 0.11177858095616103)\",\"rgba(0, 0, 255, 0.12067967876791955)\",\"rgba(0, 0, 255, 0.1145891271531582)\",\"rgba(255, 0, 0, 0.11227287389338017)\",\"rgba(255, 0, 0, 0.1227322205901146)\",\"rgba(0, 0, 255, 0.11777483895421029)\",\"rgba(255, 0, 0, 0.1210136566311121)\",\"rgba(0, 0, 255, 0.13480204567313195)\",\"rgba(0, 0, 255, 0.10357760721817613)\",\"rgba(255, 0, 0, 0.10157259195111693)\",\"rgba(255, 0, 0, 0.10464161392301322)\",\"rgba(0, 0, 255, 0.12674394473433495)\",\"rgba(255, 0, 0, 0.11254624594002963)\",\"rgba(255, 0, 0, 0.10764271104708314)\",\"rgba(0, 0, 255, 0.12714161574840546)\",\"rgba(255, 0, 0, 0.12564972266554833)\",\"rgba(255, 0, 0, 0.11082544773817063)\",\"rgba(255, 0, 0, 0.10330140697769821)\",\"rgba(255, 0, 0, 0.10202240608632565)\",\"rgba(0, 0, 255, 0.10889413338154555)\",\"rgba(255, 0, 0, 0.11328547578305007)\",\"rgba(255, 0, 0, 0.10853688456118107)\",\"rgba(255, 0, 0, 0.12781956270337105)\",\"rgba(0, 0, 255, 0.11289478410035372)\",\"rgba(0, 0, 255, 0.10882397033274174)\",\"rgba(0, 0, 255, 0.12341452799737454)\",\"rgba(255, 0, 0, 0.13452210128307343)\",\"rgba(0, 0, 255, 0.12166286669671536)\",\"rgba(0, 0, 255, 0.1051689762622118)\",\"rgba(255, 0, 0, 0.10501733785495163)\",\"rgba(0, 0, 255, 0.12685213834047318)\",\"rgba(255, 0, 0, 0.12051553502678872)\",\"rgba(255, 0, 0, 0.12066951431334019)\",\"rgba(255, 0, 0, 0.11311690732836724)\",\"rgba(255, 0, 0, 0.12540358901023865)\",\"rgba(0, 0, 255, 0.11209100801497698)\",\"rgba(0, 0, 255, 0.13927620872855187)\",\"rgba(0, 0, 255, 0.10848551187664271)\",\"rgba(0, 0, 255, 0.10279579334892333)\",\"rgba(255, 0, 0, 0.12267191745340825)\",\"rgba(255, 0, 0, 0.10763601623475552)\",\"rgba(0, 0, 255, 0.11573729328811169)\",\"rgba(255, 0, 0, 0.10125677592586727)\",\"rgba(255, 0, 0, 0.13239775821566582)\",\"rgba(255, 0, 0, 0.13136330842971802)\",\"rgba(255, 0, 0, 0.12779588103294373)\",\"rgba(255, 0, 0, 0.1357319675385952)\",\"rgba(255, 0, 0, 0.10869381800293923)\",\"rgba(255, 0, 0, 0.13561437502503396)\",\"rgba(255, 0, 0, 0.1225213747471571)\",\"rgba(255, 0, 0, 0.12315363846719266)\",\"rgba(0, 0, 255, 0.11590544171631337)\",\"rgba(255, 0, 0, 0.12157687954604626)\",\"rgba(0, 0, 255, 0.10577223906293512)\",\"rgba(255, 0, 0, 0.12614531479775906)\",\"rgba(0, 0, 255, 0.10851957499980927)\",\"rgba(0, 0, 255, 0.12348689734935761)\",\"rgba(0, 0, 255, 0.12644682675600052)\",\"rgba(0, 0, 255, 0.11854973286390305)\",\"rgba(255, 0, 0, 0.10309220398776234)\",\"rgba(0, 0, 255, 0.12290166728198529)\",\"rgba(0, 0, 255, 0.1170432385057211)\",\"rgba(0, 0, 255, 0.12599342912435532)\",\"rgba(255, 0, 0, 0.11203938946127892)\",\"rgba(255, 0, 0, 0.12202796190977097)\",\"rgba(255, 0, 0, 0.12837234921753407)\",\"rgba(255, 0, 0, 0.12956916391849518)\",\"rgba(0, 0, 255, 0.10923163164407015)\",\"rgba(0, 0, 255, 0.11519543025642634)\",\"rgba(0, 0, 255, 0.1216660387814045)\",\"rgba(0, 0, 255, 0.12724638395011426)\",\"rgba(255, 0, 0, 0.10837976671755314)\",\"rgba(0, 0, 255, 0.12760357223451138)\",\"rgba(255, 0, 0, 0.10698248436674476)\",\"rgba(255, 0, 0, 0.12589728683233262)\",\"rgba(255, 0, 0, 0.13548674285411835)\",\"rgba(255, 0, 0, 0.12117929048836232)\",\"rgba(255, 0, 0, 0.12281826175749302)\",\"rgba(0, 0, 255, 0.12560950629413128)\",\"rgba(255, 0, 0, 0.1332603193819523)\",\"rgba(255, 0, 0, 0.1105143990367651)\",\"rgba(0, 0, 255, 0.11511187013238669)\",\"rgba(255, 0, 0, 0.115893142670393)\",\"rgba(0, 0, 255, 0.11972759515047074)\",\"rgba(255, 0, 0, 0.130759422108531)\",\"rgba(255, 0, 0, 0.11499363873153925)\",\"rgba(0, 0, 255, 0.12377713657915593)\",\"rgba(255, 0, 0, 0.1070054205134511)\",\"rgba(255, 0, 0, 0.1233430676162243)\",\"rgba(0, 0, 255, 0.11888049766421319)\",\"rgba(0, 0, 255, 0.13450947627425194)\",\"rgba(255, 0, 0, 0.10393780786544085)\",\"rgba(0, 0, 255, 0.1307334214448929)\",\"rgba(255, 0, 0, 0.1209021419286728)\",\"rgba(255, 0, 0, 0.10316362059675158)\",\"rgba(0, 0, 255, 0.11180319860577584)\",\"rgba(0, 0, 255, 0.13188538178801537)\",\"rgba(0, 0, 255, 0.10275327498093248)\",\"rgba(0, 0, 255, 0.10653820391744376)\",\"rgba(0, 0, 255, 0.10739089958369732)\",\"rgba(0, 0, 255, 0.12424860559403897)\",\"rgba(255, 0, 0, 0.11453035324811936)\",\"rgba(0, 0, 255, 0.10300271180458367)\",\"rgba(0, 0, 255, 0.13130850940942765)\",\"rgba(0, 0, 255, 0.1298285335302353)\",\"rgba(0, 0, 255, 0.11188487280160189)\",\"rgba(0, 0, 255, 0.12178274281322957)\",\"rgba(255, 0, 0, 0.13709698840975762)\",\"rgba(0, 0, 255, 0.11990456692874432)\",\"rgba(0, 0, 255, 0.12315901406109334)\",\"rgba(255, 0, 0, 0.1294719133526087)\",\"rgba(0, 0, 255, 0.12622405998408795)\",\"rgba(0, 0, 255, 0.10833397079259158)\",\"rgba(255, 0, 0, 0.11352474559098483)\",\"rgba(0, 0, 255, 0.10798902846872807)\",\"rgba(0, 0, 255, 0.11840439066290856)\",\"rgba(0, 0, 255, 0.11246601063758135)\",\"rgba(0, 0, 255, 0.13183584660291672)\",\"rgba(255, 0, 0, 0.11004013419151307)\",\"rgba(0, 0, 255, 0.11885845139622689)\",\"rgba(255, 0, 0, 0.12595820650458336)\",\"rgba(0, 0, 255, 0.11622243598103524)\",\"rgba(255, 0, 0, 0.1310254879295826)\",\"rgba(255, 0, 0, 0.11859396696090699)\",\"rgba(255, 0, 0, 0.13564567118883133)\",\"rgba(0, 0, 255, 0.10313020590692759)\",\"rgba(0, 0, 255, 0.12493580430746079)\",\"rgba(0, 0, 255, 0.1075038094073534)\",\"rgba(0, 0, 255, 0.11264271512627602)\",\"rgba(255, 0, 0, 0.12367415465414525)\",\"rgba(255, 0, 0, 0.12898183465003968)\",\"rgba(255, 0, 0, 0.11142000779509545)\",\"rgba(0, 0, 255, 0.13165503963828087)\",\"rgba(0, 0, 255, 0.1350271873176098)\",\"rgba(255, 0, 0, 0.11390573754906655)\",\"rgba(0, 0, 255, 0.1333295352756977)\",\"rgba(0, 0, 255, 0.1265175923705101)\",\"rgba(0, 0, 255, 0.11086704060435296)\",\"rgba(255, 0, 0, 0.11598026417195798)\",\"rgba(255, 0, 0, 0.12957257628440857)\",\"rgba(255, 0, 0, 0.13771797567605973)\",\"rgba(255, 0, 0, 0.11726574636995793)\",\"rgba(255, 0, 0, 0.1128816943615675)\",\"rgba(0, 0, 255, 0.13586895763874055)\",\"rgba(0, 0, 255, 0.11412793342024088)\",\"rgba(0, 0, 255, 0.1373203605413437)\",\"rgba(255, 0, 0, 0.10446724426001311)\",\"rgba(255, 0, 0, 0.12678651548922062)\",\"rgba(255, 0, 0, 0.10116313295438886)\",\"rgba(0, 0, 255, 0.12944259718060494)\",\"rgba(0, 0, 255, 0.12917208522558213)\",\"rgba(0, 0, 255, 0.11909547299146653)\",\"rgba(255, 0, 0, 0.10127910147421063)\",\"rgba(255, 0, 0, 0.13052162006497384)\",\"rgba(0, 0, 255, 0.1019775559194386)\",\"rgba(0, 0, 255, 0.13780749812722207)\",\"rgba(0, 0, 255, 0.14757176339626313)\",\"rgba(0, 0, 255, 0.37348280400037764)\",\"rgba(0, 0, 255, 0.36263965368270873)\",\"rgba(255, 0, 0, 0.33358931466937064)\",\"rgba(0, 0, 255, 0.3411991812288761)\",\"rgba(255, 0, 0, 0.3570509172976017)\",\"rgba(255, 0, 0, 0.369317989051342)\",\"rgba(0, 0, 255, 0.30326083065010606)\",\"rgba(0, 0, 255, 0.3178831312805414)\",\"rgba(0, 0, 255, 0.3597972050309181)\",\"rgba(255, 0, 0, 0.32757588028907775)\"],\"size\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1.18646639585495,1.134598284959793,0.5,0.5,0.6296296566724777,0.8919081687927246,0.5,1.1262972950935364,0.5,0.5,1.257836401462555,1.2468833327293396,0.5,0.8167163729667664,0.5,0.764453262090683,0.5,0.5,0.5,0.6797190606594086,1.0262751281261444,0.931099534034729,0.5242124497890472,0.5079538822174072,0.5871261209249496,0.5,0.5,0.5863463580608368,1.2238749861717224,0.5228210538625717,0.5,0.8284536302089691,0.5256775170564651,0.5,0.597246453166008,0.5,0.5,0.5,0.6422354131937027,0.8422889113426208,0.5,0.5586929619312286,1.0279181599617004,0.5,1.0378715693950653,0.8137849867343903,0.5,0.6459799110889435,0.5,0.5,0.5,0.5,0.5,0.5,1.3410336077213287,0.7581565082073212,0.7495684325695038,0.5,0.5,0.7210509181022644,0.9668569564819336,0.5,0.5,1.1125564277172089,1.146668404340744,0.5,0.5,0.5,0.5,0.6927127093076706,1.0709316730499268,0.86850705742836,0.6976193189620972,0.5356242656707764,1.103459894657135,0.5,0.5,0.5,0.5,0.6718479841947556,0.5450219213962555,0.9407356381416321,0.5333985239267349,0.500045657157898,0.9695640206336975,0.5,0.5430246591567993,0.5432829558849335,1.0418829023838043,0.5219279676675797,0.5,1.1017343401908875,1.1066542267799377,0.5,0.5,0.5,0.5,0.7770752906799316,1.1674719750881195,0.7331429421901703,0.8477916419506073,0.7792697846889496,0.8087465465068817,0.5,0.7802352905273438,0.5763950049877167,0.5,0.656769409775734,0.5,0.5,0.9704131186008453,0.5,0.5,0.5,0.8019955158233643,0.5,0.5,0.5,0.5,1.0427003502845764,0.5773129910230637,0.669890284538269,1.1498567461967468,0.5857666879892349,0.5,0.5750715583562851,0.7230347245931625,0.5,0.5,0.5,0.5,1.3326013684272766,0.5,0.8947533667087555,0.6770134270191193,0.5417089015245438,0.8875696063041687,0.5,0.5,0.78321173787117,0.9916327893733978,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5239407420158386,0.526464119553566,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5528701096773148,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5158698856830597,0.5,0.5504812449216843,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5371934920549393,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5593643635511398,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5436019599437714,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5249036997556686,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5077584832906723,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5868608504533768,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,1,1,1,1,1,1,1,1,1,1]},\"mode\":\"lines\",\"text\":[\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Peso: 0.3955\",\"Peso: 0.3782\",\"Peso: 0.0447\",\"Peso: -0.0777\",\"Peso: 0.2099\",\"Peso: 0.2973\",\"Peso: -0.1524\",\"Peso: 0.3754\",\"Peso: -0.0597\",\"Peso: 0.0735\",\"Peso: 0.4193\",\"Peso: 0.4156\",\"Peso: -0.0752\",\"Peso: 0.2722\",\"Peso: -0.0768\",\"Peso: 0.2548\",\"Peso: -0.0848\",\"Peso: 0.0126\",\"Peso: 0.0764\",\"Peso: 0.2266\",\"Peso: 0.3421\",\"Peso: 0.3104\",\"Peso: -0.1747\",\"Peso: -0.1693\",\"Peso: -0.1957\",\"Peso: 0.0604\",\"Peso: 0.0059\",\"Peso: -0.1954\",\"Peso: 0.4080\",\"Peso: 0.1743\",\"Peso: -0.1287\",\"Peso: 0.2762\",\"Peso: 0.1752\",\"Peso: -0.0093\",\"Peso: -0.1991\",\"Peso: 0.0545\",\"Peso: -0.0879\",\"Peso: -0.0030\",\"Peso: 0.2141\",\"Peso: 0.2808\",\"Peso: 0.0792\",\"Peso: 0.1862\",\"Peso: 0.3426\",\"Peso: -0.1101\",\"Peso: 0.3460\",\"Peso: -0.2713\",\"Peso: -0.0063\",\"Peso: 0.2153\",\"Peso: 0.0714\",\"Peso: 0.0726\",\"Peso: 0.1497\",\"Peso: 0.0811\",\"Peso: 0.0727\",\"Peso: -0.1172\",\"Peso: -0.4470\",\"Peso: -0.2527\",\"Peso: -0.2499\",\"Peso: 0.0631\",\"Peso: 0.1359\",\"Peso: 0.2404\",\"Peso: 0.3223\",\"Peso: -0.1123\",\"Peso: 0.1133\",\"Peso: 0.3709\",\"Peso: 0.3822\",\"Peso: 0.0539\",\"Peso: 0.1592\",\"Peso: -0.0490\",\"Peso: 0.1086\",\"Peso: -0.2309\",\"Peso: 0.3570\",\"Peso: -0.2895\",\"Peso: 0.2325\",\"Peso: 0.1785\",\"Peso: 0.3678\",\"Peso: 0.1379\",\"Peso: -0.0644\",\"Peso: -0.0568\",\"Peso: -0.1021\",\"Peso: 0.2239\",\"Peso: 0.1817\",\"Peso: 0.3136\",\"Peso: -0.1778\",\"Peso: -0.1667\",\"Peso: 0.3232\",\"Peso: 0.0809\",\"Peso: 0.1810\",\"Peso: 0.1811\",\"Peso: 0.3473\",\"Peso: 0.1740\",\"Peso: 0.1449\",\"Peso: 0.3672\",\"Peso: 0.3689\",\"Peso: 0.0312\",\"Peso: -0.0872\",\"Peso: -0.0550\",\"Peso: 0.0597\",\"Peso: 0.2590\",\"Peso: 0.3892\",\"Peso: 0.2444\",\"Peso: 0.2826\",\"Peso: 0.2598\",\"Peso: -0.2696\",\"Peso: -0.1608\",\"Peso: 0.2601\",\"Peso: -0.1921\",\"Peso: 0.0349\",\"Peso: -0.2189\",\"Peso: -0.0583\",\"Peso: -0.1393\",\"Peso: 0.3235\",\"Peso: -0.0080\",\"Peso: -0.1244\",\"Peso: -0.0325\",\"Peso: 0.2673\",\"Peso: -0.0053\",\"Peso: 0.1311\",\"Peso: 0.0269\",\"Peso: 0.1178\",\"Peso: 0.3476\",\"Peso: 0.1924\",\"Peso: -0.2233\",\"Peso: 0.3833\",\"Peso: 0.1953\",\"Peso: -0.1220\",\"Peso: 0.1917\",\"Peso: -0.2410\",\"Peso: 0.1340\",\"Peso: -0.0591\",\"Peso: -0.1307\",\"Peso: -0.0655\",\"Peso: 0.4442\",\"Peso: -0.0076\",\"Peso: -0.2983\",\"Peso: 0.2257\",\"Peso: 0.1806\",\"Peso: -0.2959\",\"Peso: 0.1340\",\"Peso: -0.0197\",\"Peso: -0.2611\",\"Peso: 0.3305\",\"Peso: -0.0667\",\"Peso: 0.1404\",\"Peso: 0.0523\",\"Peso: 0.0387\",\"Peso: -0.0360\",\"Peso: -0.0567\",\"Peso: -0.0724\",\"Peso: -0.1162\",\"Peso: 0.0055\",\"Peso: 0.0778\",\"Peso: 0.1551\",\"Peso: 0.1353\",\"Peso: 0.0271\",\"Peso: 0.1548\",\"Peso: 0.0196\",\"Peso: -0.0751\",\"Peso: -0.1007\",\"Peso: -0.0442\",\"Peso: -0.0968\",\"Peso: -0.0379\",\"Peso: 0.0209\",\"Peso: 0.0168\",\"Peso: 0.0585\",\"Peso: -0.0822\",\"Peso: 0.0169\",\"Peso: 0.0755\",\"Peso: 0.1407\",\"Peso: 0.0525\",\"Peso: 0.0154\",\"Peso: 0.0713\",\"Peso: 0.0094\",\"Peso: -0.0767\",\"Peso: 0.0354\",\"Peso: 0.0534\",\"Peso: 0.1038\",\"Peso: 0.0271\",\"Peso: 0.0700\",\"Peso: 0.1746\",\"Peso: 0.1755\",\"Peso: 0.0639\",\"Peso: -0.0581\",\"Peso: 0.0664\",\"Peso: 0.1083\",\"Peso: -0.0313\",\"Peso: 0.0291\",\"Peso: 0.1089\",\"Peso: -0.0596\",\"Peso: -0.0226\",\"Peso: -0.0442\",\"Peso: 0.0097\",\"Peso: 0.0118\",\"Peso: -0.0532\",\"Peso: 0.1291\",\"Peso: 0.1233\",\"Peso: 0.0982\",\"Peso: -0.0116\",\"Peso: 0.0863\",\"Peso: -0.0445\",\"Peso: -0.0661\",\"Peso: 0.1067\",\"Peso: -0.0284\",\"Peso: 0.0110\",\"Peso: 0.0839\",\"Peso: 0.0926\",\"Peso: 0.1161\",\"Peso: 0.1843\",\"Peso: -0.0235\",\"Peso: 0.0594\",\"Peso: 0.0993\",\"Peso: -0.0917\",\"Peso: -0.0020\",\"Peso: -0.0277\",\"Peso: 0.0262\",\"Peso: 0.1083\",\"Peso: 0.1720\",\"Peso: 0.1423\",\"Peso: 0.1835\",\"Peso: 0.0305\",\"Peso: 0.0125\",\"Peso: -0.0091\",\"Peso: 0.0656\",\"Peso: -0.1196\",\"Peso: -0.0327\",\"Peso: 0.0260\",\"Peso: -0.0676\",\"Peso: -0.0615\",\"Peso: -0.0908\",\"Peso: 0.0605\",\"Peso: 0.0526\",\"Peso: -0.0670\",\"Peso: -0.0014\",\"Peso: -0.0156\",\"Peso: 0.0297\",\"Peso: -0.0969\",\"Peso: -0.0365\",\"Peso: 0.0642\",\"Peso: -0.0214\",\"Peso: -0.0939\",\"Peso: -0.0018\",\"Peso: 0.0560\",\"Peso: -0.0192\",\"Peso: -0.0807\",\"Peso: 0.0359\",\"Peso: 0.0203\",\"Peso: 0.0097\",\"Peso: 0.0358\",\"Peso: -0.0063\",\"Peso: -0.0408\",\"Peso: -0.0209\",\"Peso: 0.0734\",\"Peso: -0.0240\",\"Peso: 0.0093\",\"Peso: -0.0338\",\"Peso: -0.0487\",\"Peso: -0.0378\",\"Peso: -0.0546\",\"Peso: -0.0501\",\"Peso: -0.0168\",\"Peso: 0.0573\",\"Peso: 0.0594\",\"Peso: 0.0605\",\"Peso: 0.0546\",\"Peso: -0.0226\",\"Peso: 0.0102\",\"Peso: 0.0585\",\"Peso: 0.0679\",\"Peso: 0.0078\",\"Peso: -0.0512\",\"Peso: -0.0515\",\"Peso: -0.0142\",\"Peso: 0.0984\",\"Peso: -0.0202\",\"Peso: 0.1164\",\"Peso: 0.1044\",\"Peso: 0.1104\",\"Peso: 0.1791\",\"Peso: 0.1284\",\"Peso: 0.1552\",\"Peso: 0.0918\",\"Peso: -0.0152\",\"Peso: -0.1481\",\"Peso: -0.0420\",\"Peso: 0.1141\",\"Peso: -0.0633\",\"Peso: -0.0630\",\"Peso: 0.0469\",\"Peso: -0.0403\",\"Peso: 0.0036\",\"Peso: 0.0460\",\"Peso: -0.0252\",\"Peso: 0.0565\",\"Peso: 0.0804\",\"Peso: -0.1234\",\"Peso: 0.0733\",\"Peso: 0.0161\",\"Peso: 0.0376\",\"Peso: 0.0353\",\"Peso: 0.0232\",\"Peso: -0.0239\",\"Peso: -0.0592\",\"Peso: 0.1458\",\"Peso: 0.0515\",\"Peso: -0.0297\",\"Peso: 0.1309\",\"Peso: -0.0020\",\"Peso: -0.1313\",\"Peso: -0.0218\",\"Peso: 0.0423\",\"Peso: -0.1127\",\"Peso: 0.0198\",\"Peso: -0.0143\",\"Peso: 0.0759\",\"Peso: 0.1865\",\"Peso: -0.0937\",\"Peso: -0.0156\",\"Peso: 0.0793\",\"Peso: -0.0872\",\"Peso: 0.0310\",\"Peso: -0.0073\",\"Peso: 0.0204\",\"Peso: -0.0479\",\"Peso: -0.0965\",\"Peso: 0.0089\",\"Peso: -0.0102\",\"Peso: -0.0114\",\"Peso: 0.0919\",\"Peso: 0.0650\",\"Peso: -0.0299\",\"Peso: 0.1024\",\"Peso: 0.0985\",\"Peso: 0.0341\",\"Peso: 0.0668\",\"Peso: -0.0222\",\"Peso: 0.0213\",\"Peso: -0.0275\",\"Peso: -0.1131\",\"Peso: -0.0770\",\"Peso: 0.0153\",\"Peso: 0.1069\",\"Peso: -0.0499\",\"Peso: -0.0559\",\"Peso: -0.0451\",\"Peso: 0.1200\",\"Peso: -0.1062\",\"Peso: 0.0463\",\"Peso: 0.1812\",\"Peso: 0.0185\",\"Peso: -0.0723\",\"Peso: 0.0336\",\"Peso: 0.0022\",\"Peso: 0.1172\",\"Peso: 0.0686\",\"Peso: -0.0405\",\"Peso: 0.0964\",\"Peso: 0.1750\",\"Peso: 0.0028\",\"Peso: 0.0095\",\"Peso: -0.0171\",\"Peso: 0.0377\",\"Peso: -0.0716\",\"Peso: 0.0713\",\"Peso: 0.0247\",\"Peso: 0.0949\",\"Peso: 0.0460\",\"Peso: 0.1693\",\"Peso: 0.0069\",\"Peso: 0.1066\",\"Peso: 0.0591\",\"Peso: -0.0444\",\"Peso: -0.0239\",\"Peso: -0.0998\",\"Peso: -0.0490\",\"Peso: 0.0421\",\"Peso: 0.1563\",\"Peso: 0.0418\",\"Peso: 0.1624\",\"Peso: 0.0832\",\"Peso: 0.0272\",\"Peso: -0.0153\",\"Peso: 0.0644\",\"Peso: 0.0945\",\"Peso: 0.0142\",\"Peso: -0.0350\",\"Peso: 0.1426\",\"Peso: 0.1122\",\"Peso: 0.0296\",\"Peso: 0.1269\",\"Peso: 0.1385\",\"Peso: -0.1092\",\"Peso: -0.1220\",\"Peso: -0.1194\",\"Peso: -0.0870\",\"Peso: 0.0927\",\"Peso: -0.1074\",\"Peso: -0.0122\",\"Peso: 0.1020\",\"Peso: 0.0578\",\"Peso: 0.0358\",\"Peso: 0.0751\",\"Peso: 0.0039\",\"Peso: -0.0122\",\"Peso: -0.1067\",\"Peso: 0.0433\",\"Peso: -0.0294\",\"Peso: -0.0868\",\"Peso: 0.0519\",\"Peso: 0.0257\",\"Peso: 0.0019\",\"Peso: 0.1067\",\"Peso: -0.0338\",\"Peso: -0.0274\",\"Peso: 0.1360\",\"Peso: -0.0202\",\"Peso: 0.0867\",\"Peso: 0.0664\",\"Peso: -0.0260\",\"Peso: 0.0127\",\"Peso: -0.0087\",\"Peso: 0.0784\",\"Peso: 0.0473\",\"Peso: -0.0086\",\"Peso: 0.1181\",\"Peso: 0.0377\",\"Peso: 0.0398\",\"Peso: 0.1452\",\"Peso: -0.0123\",\"Peso: -0.0380\",\"Peso: 0.0139\",\"Peso: -0.0377\",\"Peso: 0.0640\",\"Peso: -0.0923\",\"Peso: -0.0642\",\"Peso: 0.0512\",\"Peso: 0.1187\",\"Peso: 0.0517\",\"Peso: 0.1490\",\"Peso: 0.0150\",\"Peso: 0.0371\",\"Peso: 0.0851\",\"Peso: 0.0822\",\"Peso: 0.1288\",\"Peso: 0.1372\",\"Peso: 0.0629\",\"Peso: 0.0125\",\"Peso: -0.0210\",\"Peso: -0.0512\",\"Peso: 0.0135\",\"Peso: 0.0220\",\"Peso: 0.0030\",\"Peso: 0.1956\",\"Peso: 0.0480\",\"Peso: 0.0720\",\"Peso: 0.0234\",\"Peso: 0.0066\",\"Peso: -0.0093\",\"Peso: 0.0118\",\"Peso: -0.0952\",\"Peso: -0.1386\",\"Peso: 0.0965\",\"Peso: 0.1363\",\"Peso: -0.0355\",\"Peso: -0.0342\",\"Peso: 0.0018\",\"Peso: -0.0053\",\"Peso: 0.0109\",\"Peso: -0.0181\",\"Peso: -0.0286\",\"Peso: -0.0019\",\"Peso: 0.0134\",\"Peso: -0.0048\",\"Peso: 0.0353\",\"Peso: -0.0014\",\"Peso: -0.0190\",\"Peso: -0.0067\",\"Peso: 0.0191\",\"Peso: 0.0128\",\"Peso: 0.0249\",\"Peso: -0.0342\",\"Peso: 0.0088\",\"Peso: 0.0238\",\"Peso: 0.0130\",\"Peso: 0.0392\",\"Peso: -0.0282\",\"Peso: -0.0095\",\"Peso: -0.0302\",\"Peso: 0.0138\",\"Peso: -0.0104\",\"Peso: -0.0313\",\"Peso: 0.0371\",\"Peso: 0.0256\",\"Peso: -0.0253\",\"Peso: -0.0265\",\"Peso: 0.0178\",\"Peso: 0.0245\",\"Peso: -0.0020\",\"Peso: 0.0223\",\"Peso: -0.0232\",\"Peso: -0.0097\",\"Peso: 0.0335\",\"Peso: -0.0350\",\"Peso: -0.0118\",\"Peso: 0.0207\",\"Peso: 0.0146\",\"Peso: -0.0123\",\"Peso: -0.0227\",\"Peso: 0.0178\",\"Peso: -0.0210\",\"Peso: 0.0348\",\"Peso: 0.0036\",\"Peso: -0.0016\",\"Peso: -0.0046\",\"Peso: 0.0267\",\"Peso: -0.0125\",\"Peso: -0.0076\",\"Peso: 0.0271\",\"Peso: -0.0256\",\"Peso: -0.0108\",\"Peso: -0.0033\",\"Peso: -0.0020\",\"Peso: 0.0089\",\"Peso: -0.0133\",\"Peso: -0.0085\",\"Peso: -0.0278\",\"Peso: 0.0129\",\"Peso: 0.0088\",\"Peso: 0.0234\",\"Peso: -0.0345\",\"Peso: 0.0217\",\"Peso: 0.0052\",\"Peso: -0.0050\",\"Peso: 0.0269\",\"Peso: -0.0205\",\"Peso: -0.0207\",\"Peso: -0.0131\",\"Peso: -0.0254\",\"Peso: 0.0121\",\"Peso: 0.0393\",\"Peso: 0.0085\",\"Peso: 0.0028\",\"Peso: -0.0227\",\"Peso: -0.0076\",\"Peso: 0.0157\",\"Peso: -0.0013\",\"Peso: -0.0324\",\"Peso: -0.0314\",\"Peso: -0.0278\",\"Peso: -0.0357\",\"Peso: -0.0087\",\"Peso: -0.0356\",\"Peso: -0.0225\",\"Peso: -0.0232\",\"Peso: 0.0159\",\"Peso: -0.0216\",\"Peso: 0.0058\",\"Peso: -0.0261\",\"Peso: 0.0085\",\"Peso: 0.0235\",\"Peso: 0.0264\",\"Peso: 0.0185\",\"Peso: -0.0031\",\"Peso: 0.0229\",\"Peso: 0.0170\",\"Peso: 0.0260\",\"Peso: -0.0120\",\"Peso: -0.0220\",\"Peso: -0.0284\",\"Peso: -0.0296\",\"Peso: 0.0092\",\"Peso: 0.0152\",\"Peso: 0.0217\",\"Peso: 0.0272\",\"Peso: -0.0084\",\"Peso: 0.0276\",\"Peso: -0.0070\",\"Peso: -0.0259\",\"Peso: -0.0355\",\"Peso: -0.0212\",\"Peso: -0.0228\",\"Peso: 0.0256\",\"Peso: -0.0333\",\"Peso: -0.0105\",\"Peso: 0.0151\",\"Peso: -0.0159\",\"Peso: 0.0197\",\"Peso: -0.0308\",\"Peso: -0.0150\",\"Peso: 0.0238\",\"Peso: -0.0070\",\"Peso: -0.0233\",\"Peso: 0.0189\",\"Peso: 0.0345\",\"Peso: -0.0039\",\"Peso: 0.0307\",\"Peso: -0.0209\",\"Peso: -0.0032\",\"Peso: 0.0118\",\"Peso: 0.0319\",\"Peso: 0.0028\",\"Peso: 0.0065\",\"Peso: 0.0074\",\"Peso: 0.0242\",\"Peso: -0.0145\",\"Peso: 0.0030\",\"Peso: 0.0313\",\"Peso: 0.0298\",\"Peso: 0.0119\",\"Peso: 0.0218\",\"Peso: -0.0371\",\"Peso: 0.0199\",\"Peso: 0.0232\",\"Peso: -0.0295\",\"Peso: 0.0262\",\"Peso: 0.0083\",\"Peso: -0.0135\",\"Peso: 0.0080\",\"Peso: 0.0184\",\"Peso: 0.0125\",\"Peso: 0.0318\",\"Peso: -0.0100\",\"Peso: 0.0189\",\"Peso: -0.0260\",\"Peso: 0.0162\",\"Peso: -0.0310\",\"Peso: -0.0186\",\"Peso: -0.0356\",\"Peso: 0.0031\",\"Peso: 0.0249\",\"Peso: 0.0075\",\"Peso: 0.0126\",\"Peso: -0.0237\",\"Peso: -0.0290\",\"Peso: -0.0114\",\"Peso: 0.0317\",\"Peso: 0.0350\",\"Peso: -0.0139\",\"Peso: 0.0333\",\"Peso: 0.0265\",\"Peso: 0.0109\",\"Peso: -0.0160\",\"Peso: -0.0296\",\"Peso: -0.0377\",\"Peso: -0.0173\",\"Peso: -0.0129\",\"Peso: 0.0359\",\"Peso: 0.0141\",\"Peso: 0.0373\",\"Peso: -0.0045\",\"Peso: -0.0268\",\"Peso: -0.0012\",\"Peso: 0.0294\",\"Peso: 0.0292\",\"Peso: 0.0191\",\"Peso: -0.0013\",\"Peso: -0.0305\",\"Peso: 0.0020\",\"Peso: 0.0378\",\"Peso: 0.0476\",\"Peso: 0.0735\",\"Peso: 0.0626\",\"Peso: -0.0336\",\"Peso: 0.0412\",\"Peso: -0.0571\",\"Peso: -0.0693\",\"Peso: 0.0033\",\"Peso: 0.0179\",\"Peso: 0.0598\",\"Peso: -0.0276\"],\"x\":[0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null],\"y\":[0,-4.6875,null,0,-4.0625,null,0,-3.4375,null,0,-2.8125,null,0,-2.1875,null,0,-1.5625,null,0,-0.9375,null,0,-0.3125,null,0,0.3125,null,0,0.9375,null,0,1.5625,null,0,2.1875,null,0,2.8125,null,0,3.4375,null,0,4.0625,null,0,4.6875,null,-4.6875,-4.6875,null,-4.6875,-4.0625,null,-4.6875,-3.4375,null,-4.6875,-2.8125,null,-4.6875,-2.1875,null,-4.6875,-1.5625,null,-4.6875,-0.9375,null,-4.6875,-0.3125,null,-4.6875,0.3125,null,-4.0625,-4.6875,null,-4.0625,-4.0625,null,-4.0625,-3.4375,null,-4.0625,-2.8125,null,-4.0625,-2.1875,null,-4.0625,-1.5625,null,-4.0625,-0.9375,null,-4.0625,-0.3125,null,-4.0625,0.3125,null,-3.4375,-4.6875,null,-3.4375,-4.0625,null,-3.4375,-3.4375,null,-3.4375,-2.8125,null,-3.4375,-2.1875,null,-3.4375,-1.5625,null,-3.4375,-0.9375,null,-3.4375,-0.3125,null,-3.4375,0.3125,null,-2.8125,-4.6875,null,-2.8125,-4.0625,null,-2.8125,-3.4375,null,-2.8125,-2.8125,null,-2.8125,-2.1875,null,-2.8125,-1.5625,null,-2.8125,-0.9375,null,-2.8125,-0.3125,null,-2.8125,0.3125,null,-2.1875,-4.6875,null,-2.1875,-4.0625,null,-2.1875,-3.4375,null,-2.1875,-2.8125,null,-2.1875,-2.1875,null,-2.1875,-1.5625,null,-2.1875,-0.9375,null,-2.1875,-0.3125,null,-2.1875,0.3125,null,-1.5625,-4.6875,null,-1.5625,-4.0625,null,-1.5625,-3.4375,null,-1.5625,-2.8125,null,-1.5625,-2.1875,null,-1.5625,-1.5625,null,-1.5625,-0.9375,null,-1.5625,-0.3125,null,-1.5625,0.3125,null,-0.9375,-4.6875,null,-0.9375,-4.0625,null,-0.9375,-3.4375,null,-0.9375,-2.8125,null,-0.9375,-2.1875,null,-0.9375,-1.5625,null,-0.9375,-0.9375,null,-0.9375,-0.3125,null,-0.9375,0.3125,null,-0.3125,-4.6875,null,-0.3125,-4.0625,null,-0.3125,-3.4375,null,-0.3125,-2.8125,null,-0.3125,-2.1875,null,-0.3125,-1.5625,null,-0.3125,-0.9375,null,-0.3125,-0.3125,null,-0.3125,0.3125,null,0.3125,-4.6875,null,0.3125,-4.0625,null,0.3125,-3.4375,null,0.3125,-2.8125,null,0.3125,-2.1875,null,0.3125,-1.5625,null,0.3125,-0.9375,null,0.3125,-0.3125,null,0.3125,0.3125,null,0.9375,-4.6875,null,0.9375,-4.0625,null,0.9375,-3.4375,null,0.9375,-2.8125,null,0.9375,-2.1875,null,0.9375,-1.5625,null,0.9375,-0.9375,null,0.9375,-0.3125,null,0.9375,0.3125,null,1.5625,-4.6875,null,1.5625,-4.0625,null,1.5625,-3.4375,null,1.5625,-2.8125,null,1.5625,-2.1875,null,1.5625,-0.9375,null,1.5625,-0.3125,null,1.5625,0.3125,null,2.1875,-4.6875,null,2.1875,-4.0625,null,2.1875,-3.4375,null,2.1875,-2.8125,null,2.1875,-2.1875,null,2.1875,-1.5625,null,2.1875,-0.9375,null,2.1875,-0.3125,null,2.1875,0.3125,null,2.8125,-4.6875,null,2.8125,-4.0625,null,2.8125,-3.4375,null,2.8125,-2.8125,null,2.8125,-2.1875,null,2.8125,-1.5625,null,2.8125,-0.9375,null,2.8125,-0.3125,null,2.8125,0.3125,null,3.4375,-4.6875,null,3.4375,-4.0625,null,3.4375,-3.4375,null,3.4375,-2.8125,null,3.4375,-2.1875,null,3.4375,-1.5625,null,3.4375,-0.9375,null,3.4375,-0.3125,null,3.4375,0.3125,null,4.0625,-4.6875,null,4.0625,-4.0625,null,4.0625,-3.4375,null,4.0625,-2.8125,null,4.0625,-2.1875,null,4.0625,-1.5625,null,4.0625,-0.9375,null,4.0625,-0.3125,null,4.0625,0.3125,null,4.6875,-4.6875,null,4.6875,-4.0625,null,4.6875,-3.4375,null,4.6875,-2.8125,null,4.6875,-2.1875,null,4.6875,-1.5625,null,4.6875,-0.9375,null,4.6875,-0.3125,null,4.6875,0.3125,null,-4.6875,-4.916666666666666,null,-4.6875,-4.75,null,-4.6875,-4.583333333333333,null,-4.6875,-4.416666666666666,null,-4.6875,-4.25,null,-4.6875,-4.083333333333333,null,-4.6875,-3.9166666666666665,null,-4.6875,-3.75,null,-4.6875,-3.583333333333333,null,-4.6875,-3.4166666666666665,null,-4.6875,-3.25,null,-4.6875,-3.083333333333333,null,-4.6875,-2.9166666666666665,null,-4.6875,-2.75,null,-4.6875,-2.583333333333333,null,-4.6875,-2.4166666666666665,null,-4.6875,-2.25,null,-4.6875,-2.083333333333333,null,-4.6875,-1.9166666666666665,null,-4.6875,-1.75,null,-4.0625,-4.916666666666666,null,-4.0625,-4.75,null,-4.0625,-4.583333333333333,null,-4.0625,-4.416666666666666,null,-4.0625,-4.25,null,-4.0625,-4.083333333333333,null,-4.0625,-3.9166666666666665,null,-4.0625,-3.75,null,-4.0625,-3.4166666666666665,null,-4.0625,-3.25,null,-4.0625,-3.083333333333333,null,-4.0625,-2.9166666666666665,null,-4.0625,-2.75,null,-4.0625,-2.583333333333333,null,-4.0625,-2.4166666666666665,null,-4.0625,-2.25,null,-4.0625,-2.083333333333333,null,-4.0625,-1.9166666666666665,null,-4.0625,-1.75,null,-3.4375,-4.916666666666666,null,-3.4375,-4.75,null,-3.4375,-4.583333333333333,null,-3.4375,-4.416666666666666,null,-3.4375,-4.25,null,-3.4375,-4.083333333333333,null,-3.4375,-3.9166666666666665,null,-3.4375,-3.75,null,-3.4375,-3.583333333333333,null,-3.4375,-3.4166666666666665,null,-3.4375,-3.25,null,-3.4375,-3.083333333333333,null,-3.4375,-2.9166666666666665,null,-3.4375,-2.75,null,-3.4375,-2.583333333333333,null,-3.4375,-2.4166666666666665,null,-3.4375,-2.25,null,-3.4375,-1.9166666666666665,null,-3.4375,-1.75,null,-2.8125,-4.916666666666666,null,-2.8125,-4.75,null,-2.8125,-4.583333333333333,null,-2.8125,-4.416666666666666,null,-2.8125,-4.25,null,-2.8125,-4.083333333333333,null,-2.8125,-3.9166666666666665,null,-2.8125,-3.75,null,-2.8125,-3.583333333333333,null,-2.8125,-3.4166666666666665,null,-2.8125,-3.25,null,-2.8125,-3.083333333333333,null,-2.8125,-2.9166666666666665,null,-2.8125,-2.75,null,-2.8125,-2.583333333333333,null,-2.8125,-2.4166666666666665,null,-2.8125,-2.25,null,-2.8125,-2.083333333333333,null,-2.8125,-1.9166666666666665,null,-2.8125,-1.75,null,-2.1875,-4.916666666666666,null,-2.1875,-4.75,null,-2.1875,-4.583333333333333,null,-2.1875,-4.416666666666666,null,-2.1875,-4.25,null,-2.1875,-4.083333333333333,null,-2.1875,-3.9166666666666665,null,-2.1875,-3.75,null,-2.1875,-3.583333333333333,null,-2.1875,-3.4166666666666665,null,-2.1875,-3.25,null,-2.1875,-3.083333333333333,null,-2.1875,-2.9166666666666665,null,-2.1875,-2.75,null,-2.1875,-2.583333333333333,null,-2.1875,-2.4166666666666665,null,-2.1875,-2.25,null,-2.1875,-2.083333333333333,null,-2.1875,-1.9166666666666665,null,-2.1875,-1.75,null,-1.5625,-4.916666666666666,null,-1.5625,-4.75,null,-1.5625,-4.583333333333333,null,-1.5625,-4.416666666666666,null,-1.5625,-4.25,null,-1.5625,-4.083333333333333,null,-1.5625,-3.9166666666666665,null,-1.5625,-3.75,null,-1.5625,-3.583333333333333,null,-1.5625,-3.4166666666666665,null,-1.5625,-3.25,null,-1.5625,-3.083333333333333,null,-1.5625,-2.9166666666666665,null,-1.5625,-2.75,null,-1.5625,-2.583333333333333,null,-1.5625,-2.4166666666666665,null,-1.5625,-2.25,null,-1.5625,-2.083333333333333,null,-1.5625,-1.9166666666666665,null,-1.5625,-1.75,null,-0.9375,-4.916666666666666,null,-0.9375,-4.75,null,-0.9375,-4.583333333333333,null,-0.9375,-4.416666666666666,null,-0.9375,-4.25,null,-0.9375,-4.083333333333333,null,-0.9375,-3.9166666666666665,null,-0.9375,-3.75,null,-0.9375,-3.583333333333333,null,-0.9375,-3.4166666666666665,null,-0.9375,-3.25,null,-0.9375,-3.083333333333333,null,-0.9375,-2.9166666666666665,null,-0.9375,-2.75,null,-0.9375,-2.583333333333333,null,-0.9375,-2.4166666666666665,null,-0.9375,-2.25,null,-0.9375,-2.083333333333333,null,-0.9375,-1.9166666666666665,null,-0.9375,-1.75,null,-0.3125,-4.916666666666666,null,-0.3125,-4.75,null,-0.3125,-4.583333333333333,null,-0.3125,-4.416666666666666,null,-0.3125,-4.25,null,-0.3125,-4.083333333333333,null,-0.3125,-3.9166666666666665,null,-0.3125,-3.75,null,-0.3125,-3.583333333333333,null,-0.3125,-3.4166666666666665,null,-0.3125,-3.25,null,-0.3125,-3.083333333333333,null,-0.3125,-2.9166666666666665,null,-0.3125,-2.75,null,-0.3125,-2.583333333333333,null,-0.3125,-2.4166666666666665,null,-0.3125,-2.25,null,-0.3125,-2.083333333333333,null,-0.3125,-1.9166666666666665,null,-0.3125,-1.75,null,0.3125,-4.916666666666666,null,0.3125,-4.75,null,0.3125,-4.583333333333333,null,0.3125,-4.416666666666666,null,0.3125,-4.25,null,0.3125,-4.083333333333333,null,0.3125,-3.9166666666666665,null,0.3125,-3.75,null,0.3125,-3.583333333333333,null,0.3125,-3.4166666666666665,null,0.3125,-3.25,null,0.3125,-3.083333333333333,null,0.3125,-2.9166666666666665,null,0.3125,-2.75,null,0.3125,-2.583333333333333,null,0.3125,-2.4166666666666665,null,0.3125,-2.25,null,0.3125,-2.083333333333333,null,0.3125,-1.9166666666666665,null,0.3125,-1.75,null,0.9375,-4.916666666666666,null,0.9375,-4.75,null,0.9375,-4.583333333333333,null,0.9375,-4.416666666666666,null,0.9375,-4.25,null,0.9375,-4.083333333333333,null,0.9375,-3.9166666666666665,null,0.9375,-3.75,null,0.9375,-3.583333333333333,null,0.9375,-3.4166666666666665,null,0.9375,-3.25,null,0.9375,-3.083333333333333,null,0.9375,-2.9166666666666665,null,0.9375,-2.75,null,0.9375,-2.583333333333333,null,0.9375,-2.4166666666666665,null,0.9375,-2.25,null,0.9375,-2.083333333333333,null,0.9375,-1.9166666666666665,null,0.9375,-1.75,null,1.5625,-4.916666666666666,null,1.5625,-4.75,null,1.5625,-4.583333333333333,null,1.5625,-4.416666666666666,null,1.5625,-4.25,null,1.5625,-4.083333333333333,null,1.5625,-3.9166666666666665,null,1.5625,-3.75,null,1.5625,-3.583333333333333,null,1.5625,-3.4166666666666665,null,1.5625,-3.25,null,1.5625,-3.083333333333333,null,1.5625,-2.9166666666666665,null,1.5625,-2.75,null,1.5625,-2.583333333333333,null,1.5625,-2.4166666666666665,null,1.5625,-2.25,null,1.5625,-2.083333333333333,null,1.5625,-1.9166666666666665,null,1.5625,-1.75,null,2.1875,-4.916666666666666,null,2.1875,-4.75,null,2.1875,-4.583333333333333,null,2.1875,-4.416666666666666,null,2.1875,-4.25,null,2.1875,-4.083333333333333,null,2.1875,-3.9166666666666665,null,2.1875,-3.75,null,2.1875,-3.583333333333333,null,2.1875,-3.4166666666666665,null,2.1875,-3.25,null,2.1875,-3.083333333333333,null,2.1875,-2.9166666666666665,null,2.1875,-2.75,null,2.1875,-2.583333333333333,null,2.1875,-2.4166666666666665,null,2.1875,-2.25,null,2.1875,-2.083333333333333,null,2.1875,-1.9166666666666665,null,2.1875,-1.75,null,2.8125,-4.916666666666666,null,2.8125,-4.75,null,2.8125,-4.583333333333333,null,2.8125,-4.416666666666666,null,2.8125,-4.25,null,2.8125,-4.083333333333333,null,2.8125,-3.9166666666666665,null,2.8125,-3.75,null,2.8125,-3.583333333333333,null,2.8125,-3.4166666666666665,null,2.8125,-3.25,null,2.8125,-3.083333333333333,null,2.8125,-2.9166666666666665,null,2.8125,-2.75,null,2.8125,-2.583333333333333,null,2.8125,-2.4166666666666665,null,2.8125,-2.25,null,2.8125,-2.083333333333333,null,2.8125,-1.9166666666666665,null,2.8125,-1.75,null,3.4375,-4.916666666666666,null,3.4375,-4.75,null,3.4375,-4.583333333333333,null,3.4375,-4.416666666666666,null,3.4375,-4.25,null,3.4375,-4.083333333333333,null,3.4375,-3.9166666666666665,null,3.4375,-3.75,null,3.4375,-3.583333333333333,null,3.4375,-3.4166666666666665,null,3.4375,-3.25,null,3.4375,-3.083333333333333,null,3.4375,-2.9166666666666665,null,3.4375,-2.75,null,3.4375,-2.583333333333333,null,3.4375,-2.4166666666666665,null,3.4375,-2.25,null,3.4375,-2.083333333333333,null,3.4375,-1.9166666666666665,null,3.4375,-1.75,null,4.0625,-4.916666666666666,null,4.0625,-4.75,null,4.0625,-4.583333333333333,null,4.0625,-4.416666666666666,null,4.0625,-4.25,null,4.0625,-4.083333333333333,null,4.0625,-3.9166666666666665,null,4.0625,-3.75,null,4.0625,-3.583333333333333,null,4.0625,-3.4166666666666665,null,4.0625,-3.25,null,4.0625,-3.083333333333333,null,4.0625,-2.9166666666666665,null,4.0625,-2.75,null,4.0625,-2.583333333333333,null,4.0625,-2.4166666666666665,null,4.0625,-2.25,null,4.0625,-2.083333333333333,null,4.0625,-1.9166666666666665,null,4.0625,-1.75,null,4.6875,-4.916666666666666,null,4.6875,-4.75,null,4.6875,-4.583333333333333,null,4.6875,-4.416666666666666,null,4.6875,-4.25,null,4.6875,-4.083333333333333,null,4.6875,-3.9166666666666665,null,4.6875,-3.75,null,4.6875,-3.583333333333333,null,4.6875,-3.4166666666666665,null,4.6875,-3.25,null,4.6875,-3.083333333333333,null,4.6875,-2.9166666666666665,null,4.6875,-2.75,null,4.6875,-2.583333333333333,null,4.6875,-2.4166666666666665,null,4.6875,-2.25,null,4.6875,-2.083333333333333,null,4.6875,-1.9166666666666665,null,4.6875,-1.75,null,-4.916666666666666,-4.5,null,-4.916666666666666,-3.5,null,-4.916666666666666,-2.5,null,-4.916666666666666,-1.5,null,-4.916666666666666,-0.5,null,-4.916666666666666,0.5,null,-4.916666666666666,1.5,null,-4.916666666666666,2.5,null,-4.916666666666666,3.5,null,-4.916666666666666,4.5,null,-4.75,-4.5,null,-4.75,-3.5,null,-4.75,-2.5,null,-4.75,-1.5,null,-4.75,-0.5,null,-4.75,0.5,null,-4.75,1.5,null,-4.75,2.5,null,-4.75,3.5,null,-4.75,4.5,null,-4.583333333333333,-4.5,null,-4.583333333333333,-3.5,null,-4.583333333333333,-2.5,null,-4.583333333333333,-1.5,null,-4.583333333333333,-0.5,null,-4.583333333333333,0.5,null,-4.583333333333333,1.5,null,-4.583333333333333,2.5,null,-4.583333333333333,3.5,null,-4.583333333333333,4.5,null,-4.416666666666666,-4.5,null,-4.416666666666666,-3.5,null,-4.416666666666666,-2.5,null,-4.416666666666666,-1.5,null,-4.416666666666666,-0.5,null,-4.416666666666666,0.5,null,-4.416666666666666,1.5,null,-4.416666666666666,2.5,null,-4.416666666666666,3.5,null,-4.416666666666666,4.5,null,-4.25,-4.5,null,-4.25,-3.5,null,-4.25,-2.5,null,-4.25,-1.5,null,-4.25,-0.5,null,-4.25,0.5,null,-4.25,1.5,null,-4.25,2.5,null,-4.25,3.5,null,-4.25,4.5,null,-4.083333333333333,-4.5,null,-4.083333333333333,-3.5,null,-4.083333333333333,-2.5,null,-4.083333333333333,-1.5,null,-4.083333333333333,-0.5,null,-4.083333333333333,1.5,null,-4.083333333333333,2.5,null,-4.083333333333333,3.5,null,-4.083333333333333,4.5,null,-3.9166666666666665,-4.5,null,-3.9166666666666665,-3.5,null,-3.9166666666666665,-2.5,null,-3.9166666666666665,-1.5,null,-3.9166666666666665,-0.5,null,-3.9166666666666665,0.5,null,-3.9166666666666665,1.5,null,-3.9166666666666665,2.5,null,-3.9166666666666665,3.5,null,-3.9166666666666665,4.5,null,-3.75,-4.5,null,-3.75,-3.5,null,-3.75,-2.5,null,-3.75,-1.5,null,-3.75,-0.5,null,-3.75,0.5,null,-3.75,1.5,null,-3.75,2.5,null,-3.75,3.5,null,-3.75,4.5,null,-3.583333333333333,-4.5,null,-3.583333333333333,-2.5,null,-3.583333333333333,-1.5,null,-3.583333333333333,-0.5,null,-3.583333333333333,0.5,null,-3.583333333333333,1.5,null,-3.583333333333333,2.5,null,-3.583333333333333,3.5,null,-3.583333333333333,4.5,null,-3.4166666666666665,-4.5,null,-3.4166666666666665,-3.5,null,-3.4166666666666665,-2.5,null,-3.4166666666666665,-1.5,null,-3.4166666666666665,-0.5,null,-3.4166666666666665,0.5,null,-3.4166666666666665,1.5,null,-3.4166666666666665,2.5,null,-3.4166666666666665,3.5,null,-3.4166666666666665,4.5,null,-3.25,-4.5,null,-3.25,-3.5,null,-3.25,-2.5,null,-3.25,-1.5,null,-3.25,-0.5,null,-3.25,0.5,null,-3.25,1.5,null,-3.25,2.5,null,-3.25,3.5,null,-3.25,4.5,null,-3.083333333333333,-4.5,null,-3.083333333333333,-3.5,null,-3.083333333333333,-1.5,null,-3.083333333333333,-0.5,null,-3.083333333333333,0.5,null,-3.083333333333333,1.5,null,-3.083333333333333,2.5,null,-3.083333333333333,3.5,null,-3.083333333333333,4.5,null,-2.9166666666666665,-4.5,null,-2.9166666666666665,-3.5,null,-2.9166666666666665,-2.5,null,-2.9166666666666665,-1.5,null,-2.9166666666666665,-0.5,null,-2.9166666666666665,0.5,null,-2.9166666666666665,1.5,null,-2.9166666666666665,2.5,null,-2.9166666666666665,3.5,null,-2.9166666666666665,4.5,null,-2.75,-4.5,null,-2.75,-3.5,null,-2.75,-2.5,null,-2.75,-1.5,null,-2.75,-0.5,null,-2.75,0.5,null,-2.75,1.5,null,-2.75,2.5,null,-2.75,3.5,null,-2.75,4.5,null,-2.583333333333333,-4.5,null,-2.583333333333333,-3.5,null,-2.583333333333333,-2.5,null,-2.583333333333333,-1.5,null,-2.583333333333333,-0.5,null,-2.583333333333333,0.5,null,-2.583333333333333,1.5,null,-2.583333333333333,2.5,null,-2.583333333333333,3.5,null,-2.583333333333333,4.5,null,-2.4166666666666665,-4.5,null,-2.4166666666666665,-3.5,null,-2.4166666666666665,-2.5,null,-2.4166666666666665,-1.5,null,-2.4166666666666665,-0.5,null,-2.4166666666666665,0.5,null,-2.4166666666666665,1.5,null,-2.4166666666666665,2.5,null,-2.4166666666666665,3.5,null,-2.4166666666666665,4.5,null,-2.25,-4.5,null,-2.25,-3.5,null,-2.25,-2.5,null,-2.25,-1.5,null,-2.25,-0.5,null,-2.25,0.5,null,-2.25,1.5,null,-2.25,2.5,null,-2.25,3.5,null,-2.25,4.5,null,-2.083333333333333,-4.5,null,-2.083333333333333,-3.5,null,-2.083333333333333,-2.5,null,-2.083333333333333,-1.5,null,-2.083333333333333,-0.5,null,-2.083333333333333,0.5,null,-2.083333333333333,1.5,null,-2.083333333333333,2.5,null,-2.083333333333333,3.5,null,-2.083333333333333,4.5,null,-1.9166666666666665,-4.5,null,-1.9166666666666665,-3.5,null,-1.9166666666666665,-2.5,null,-1.9166666666666665,-1.5,null,-1.9166666666666665,-0.5,null,-1.9166666666666665,0.5,null,-1.9166666666666665,1.5,null,-1.9166666666666665,2.5,null,-1.9166666666666665,3.5,null,-1.9166666666666665,4.5,null,-1.75,-4.5,null,-1.75,-3.5,null,-1.75,-2.5,null,-1.75,-1.5,null,-1.75,-0.5,null,-1.75,0.5,null,-1.75,1.5,null,-1.75,2.5,null,-1.75,3.5,null,-1.75,4.5,null,-4.5,0,null,-3.5,0,null,-2.5,0,null,-1.5,0,null,-0.5,0,null,0.5,0,null,1.5,0,null,2.5,0,null,3.5,0,null,4.5,0,null],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":[\"rgba(200, 200, 200, 0.8)\",\"rgba(0, 0, 153, 0.8)\",\"rgba(0, 0, 156, 0.8)\",\"rgba(0, 0, 125, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 125, 0.8)\",\"rgba(0, 0, 141, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 156, 0.8)\",\"rgba(0, 0, 123, 0.8)\",\"rgba(0, 0, 126, 0.8)\",\"rgba(0, 0, 122, 0.8)\",\"rgba(0, 0, 141, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 122, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 152, 0.8)\",\"rgba(0, 0, 244, 0.8)\",\"rgba(0, 0, 177, 0.8)\",\"rgba(0, 0, 232, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(145, 0, 0, 0.8)\",\"rgba(0, 0, 242, 0.8)\",\"rgba(0, 0, 146, 0.8)\",\"rgba(0, 0, 184, 0.8)\",\"rgba(0, 0, 151, 0.8)\",\"rgba(0, 0, 190, 0.8)\",\"rgba(0, 0, 218, 0.8)\",\"rgba(0, 0, 195, 0.8)\",\"rgba(0, 0, 185, 0.8)\",\"rgba(0, 0, 213, 0.8)\",\"rgba(0, 0, 172, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(254, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(152, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(243, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 120, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(209, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(229, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 200, 0, 0.8)\"],\"line\":{\"color\":\"white\",\"width\":2},\"showscale\":false,\"size\":[25,15.141708850860596,15.440637022256851,12.433783933520317,11.039394214749336,12.43877574801445,14.01827186346054,10.256638396531343,15.426360964775085,12.289663478732109,12.611714079976082,12.189680337905884,14.04461070895195,10.001341306960967,12.147325575351715,10.064303481485695,10.752310566604137,15.095483660697937,23.958356082439423,17.5149667263031,22.790999710559845,11.350403726100922,14.418544173240662,23.81269782781601,14.48177769780159,18.17104399204254,14.955917596817017,18.747812807559967,21.493416726589203,19.237805008888245,18.233240842819214,20.950550138950348,16.97776049375534,58.420649766922,37.51822531223297,55.9034788608551,24.956540167331696,40.19983887672424,88.52230548858643,52.658395767211914,35.2060341835022,15.110854506492615,62.72621035575867,26.67936682701111,40.716389417648315,55.8254337310791,134.93057250976562,30.17762541770935,139.2228364944458,87.61640548706055,135.94221591949463,79.77859735488892,90.66518306732178,81.87954664230347,23.866998255252838,40.816171169281006,119.3709135055542,73.90686273574829,101.66656732559204,104.09860849380493,69.83919262886047,36.6812938451767,11.953845769166946,103.99428844451904,67.3849105834961,154.23963069915771,110.47150135040283,126.71005249023438,128.6878776550293,103.43323469161987,20.619024634361267,60.52619457244873,49.323909282684326,89.9592113494873,44.550299644470215,91.35966777801514,79.39437627792358,43.13077092170715,78.8076663017273,47.86317467689514,53.115376234054565,51.686474084854126,111.47763013839722,94.02021169662476,45.520068407058716,37.72389352321625,88.59403848648071,37.480356097221375,31.71039342880249,35.04248917102814,113.67608308792114,48.21637511253357,22.51684308052063,100.12411117553711,67.8143322467804,85.47921419143677,152.40472793579102,135.79431533813477,104.19964551925659,136.41472816467285,69.35897707939148,88.07494878768921,236.09538555145264,25]},\"mode\":\"markers\",\"text\":[\"Imagen de entrada\",\"conv1 neurona 0\\u003cbr\\u003eActivación: 0.3428\",\"conv1 neurona 1\\u003cbr\\u003eActivación: 0.3627\",\"conv1 neurona 2\\u003cbr\\u003eActivación: 0.1623\",\"conv1 neurona 3\\u003cbr\\u003eActivación: 0.0693\",\"conv1 neurona 4\\u003cbr\\u003eActivación: 0.1626\",\"conv1 neurona 5\\u003cbr\\u003eActivación: 0.2679\",\"conv1 neurona 6\\u003cbr\\u003eActivación: 0.0171\",\"conv1 neurona 7\\u003cbr\\u003eActivación: 0.3618\",\"conv1 neurona 8\\u003cbr\\u003eActivación: 0.1526\",\"conv1 neurona 9\\u003cbr\\u003eActivación: 0.1741\",\"conv1 neurona 10\\u003cbr\\u003eActivación: 0.1460\",\"conv1 neurona 11\\u003cbr\\u003eActivación: 0.2696\",\"conv1 neurona 12\\u003cbr\\u003eActivación: 0.0001\",\"conv1 neurona 13\\u003cbr\\u003eActivación: 0.1432\",\"conv1 neurona 14\\u003cbr\\u003eActivación: -0.0043\",\"conv1 neurona 15\\u003cbr\\u003eActivación: 0.0502\",\"conv2 neurona 0\\u003cbr\\u003eActivación: 0.3397\",\"conv2 neurona 1\\u003cbr\\u003eActivación: 0.9306\",\"conv2 neurona 2\\u003cbr\\u003eActivación: 0.5010\",\"conv2 neurona 3\\u003cbr\\u003eActivación: 0.8527\",\"conv2 neurona 4\\u003cbr\\u003eActivación: -0.0900\",\"conv2 neurona 5\\u003cbr\\u003eActivación: -0.2946\",\"conv2 neurona 6\\u003cbr\\u003eActivación: 0.9208\",\"conv2 neurona 7\\u003cbr\\u003eActivación: 0.2988\",\"conv2 neurona 8\\u003cbr\\u003eActivación: 0.5447\",\"conv2 neurona 9\\u003cbr\\u003eActivación: 0.3304\",\"conv2 neurona 10\\u003cbr\\u003eActivación: 0.5832\",\"conv2 neurona 11\\u003cbr\\u003eActivación: 0.7662\",\"conv2 neurona 12\\u003cbr\\u003eActivación: 0.6159\",\"conv2 neurona 13\\u003cbr\\u003eActivación: 0.5489\",\"conv2 neurona 14\\u003cbr\\u003eActivación: 0.7300\",\"conv2 neurona 15\\u003cbr\\u003eActivación: 0.4652\",\"fc1 neurona 0\\u003cbr\\u003eActivación: 3.2280\",\"fc1 neurona 1\\u003cbr\\u003eActivación: -1.8345\",\"fc1 neurona 2\\u003cbr\\u003eActivación: 3.0602\",\"fc1 neurona 3\\u003cbr\\u003eActivación: -0.9971\",\"fc1 neurona 4\\u003cbr\\u003eActivación: -2.0133\",\"fc1 neurona 5\\u003cbr\\u003eActivación: 5.2348\",\"fc1 neurona 6\\u003cbr\\u003eActivación: -2.8439\",\"fc1 neurona 7\\u003cbr\\u003eActivación: -1.6804\",\"fc1 neurona 8\\u003cbr\\u003eActivación: -0.3407\",\"fc1 neurona 9\\u003cbr\\u003eActivación: -3.5151\",\"fc1 neurona 10\\u003cbr\\u003eActivación: -1.1120\",\"fc1 neurona 11\\u003cbr\\u003eActivación: -2.0478\",\"fc1 neurona 12\\u003cbr\\u003eActivación: -3.0550\",\"fc1 neurona 13\\u003cbr\\u003eActivación: 8.3287\",\"fc1 neurona 14\\u003cbr\\u003eActivación: -1.3452\",\"fc1 neurona 15\\u003cbr\\u003eActivación: 8.6149\",\"fc1 neurona 16\\u003cbr\\u003eActivación: 5.1744\",\"fc1 neurona 17\\u003cbr\\u003eActivación: 8.3961\",\"fc1 neurona 18\\u003cbr\\u003eActivación: 4.6519\",\"fc1 neurona 19\\u003cbr\\u003eActivación: 5.3777\",\"fc1 neurona 20\\u003cbr\\u003eActivación: 4.7920\",\"fc1 neurona 21\\u003cbr\\u003eActivación: -0.9245\",\"fc1 neurona 22\\u003cbr\\u003eActivación: -2.0544\",\"fc1 neurona 23\\u003cbr\\u003eActivación: 7.2914\",\"fc1 neurona 24\\u003cbr\\u003eActivación: 4.2605\",\"fc1 neurona 25\\u003cbr\\u003eActivación: 6.1111\",\"fc1 neurona 26\\u003cbr\\u003eActivación: 6.2732\",\"fc1 neurona 27\\u003cbr\\u003eActivación: 3.9893\",\"fc1 neurona 28\\u003cbr\\u003eActivación: -1.7788\",\"fc1 neurona 29\\u003cbr\\u003eActivación: 0.1303\",\"fc1 neurona 30\\u003cbr\\u003eActivación: 6.2663\",\"fc1 neurona 31\\u003cbr\\u003eActivación: -3.8257\",\"fc1 neurona 32\\u003cbr\\u003eActivación: 9.6160\",\"fc1 neurona 33\\u003cbr\\u003eActivación: 6.6981\",\"fc1 neurona 34\\u003cbr\\u003eActivación: 7.7807\",\"fc1 neurona 35\\u003cbr\\u003eActivación: 7.9125\",\"fc1 neurona 36\\u003cbr\\u003eActivación: 6.2289\",\"fc1 neurona 37\\u003cbr\\u003eActivación: -0.7079\",\"fc1 neurona 38\\u003cbr\\u003eActivación: -3.3684\",\"fc1 neurona 39\\u003cbr\\u003eActivación: -2.6216\",\"fc1 neurona 40\\u003cbr\\u003eActivación: 5.3306\",\"fc1 neurona 41\\u003cbr\\u003eActivación: -2.3034\",\"fc1 neurona 42\\u003cbr\\u003eActivación: 5.4240\",\"fc1 neurona 43\\u003cbr\\u003eActivación: 4.6263\",\"fc1 neurona 44\\u003cbr\\u003eActivación: 2.2087\",\"fc1 neurona 45\\u003cbr\\u003eActivación: 4.5872\",\"fc1 neurona 46\\u003cbr\\u003eActivación: -2.5242\",\"fc1 neurona 47\\u003cbr\\u003eActivación: -2.8744\",\"fc1 neurona 48\\u003cbr\\u003eActivación: -2.7791\",\"fc1 neurona 49\\u003cbr\\u003eActivación: 6.7652\",\"fc1 neurona 50\\u003cbr\\u003eActivación: 5.6013\",\"fc1 neurona 51\\u003cbr\\u003eActivación: -2.3680\",\"fc1 neurona 52\\u003cbr\\u003eActivación: -1.8483\",\"fc1 neurona 53\\u003cbr\\u003eActivación: 5.2396\",\"fc1 neurona 54\\u003cbr\\u003eActivación: -1.8320\",\"fc1 neurona 55\\u003cbr\\u003eActivación: -1.4474\",\"fc1 neurona 56\\u003cbr\\u003eActivación: 1.6695\",\"fc1 neurona 57\\u003cbr\\u003eActivación: 6.9117\",\"fc1 neurona 58\\u003cbr\\u003eActivación: -2.5478\",\"fc1 neurona 59\\u003cbr\\u003eActivación: -0.8345\",\"fc2 neurona 0\\u003cbr\\u003eActivación: -6.0083\",\"fc2 neurona 1\\u003cbr\\u003eActivación: 3.8543\",\"fc2 neurona 2\\u003cbr\\u003eActivación: -5.0319\",\"fc2 neurona 3\\u003cbr\\u003eActivación: -9.4936\",\"fc2 neurona 4\\u003cbr\\u003eActivación: -8.3863\",\"fc2 neurona 5\\u003cbr\\u003eActivación: -6.2800\",\"fc2 neurona 6\\u003cbr\\u003eActivación: -8.4276\",\"fc2 neurona 7\\u003cbr\\u003eActivación: -3.9573\",\"fc2 neurona 8\\u003cbr\\u003eActivación: -5.2050\",\"fc2 neurona 9\\u003cbr\\u003eActivación: 15.0730\",\"Clase predicha: 9\"],\"x\":[0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5],\"y\":[0,-4.6875,-4.0625,-3.4375,-2.8125,-2.1875,-1.5625,-0.9375,-0.3125,0.3125,0.9375,1.5625,2.1875,2.8125,3.4375,4.0625,4.6875,-4.6875,-4.0625,-3.4375,-2.8125,-2.1875,-1.5625,-0.9375,-0.3125,0.3125,0.9375,1.5625,2.1875,2.8125,3.4375,4.0625,4.6875,-4.916666666666666,-4.75,-4.583333333333333,-4.416666666666666,-4.25,-4.083333333333333,-3.9166666666666665,-3.75,-3.583333333333333,-3.4166666666666665,-3.25,-3.083333333333333,-2.9166666666666665,-2.75,-2.583333333333333,-2.4166666666666665,-2.25,-2.083333333333333,-1.9166666666666665,-1.75,-1.5833333333333333,-1.4166666666666665,-1.25,-1.0833333333333333,-0.9166666666666666,-0.75,-0.5833333333333333,-0.41666666666666663,-0.25,-0.08333333333333333,0.08333333333333333,0.25,0.41666666666666663,0.5833333333333333,0.75,0.9166666666666666,1.0833333333333333,1.25,1.4166666666666665,1.5833333333333333,1.75,1.9166666666666665,2.083333333333333,2.25,2.4166666666666665,2.583333333333333,2.75,2.9166666666666665,3.083333333333333,3.25,3.4166666666666665,3.583333333333333,3.75,3.9166666666666665,4.083333333333333,4.25,4.416666666666666,4.583333333333333,4.75,4.916666666666666,-4.5,-3.5,-2.5,-1.5,-0.5,0.5,1.5,2.5,3.5,4.5,0],\"type\":\"scatter\"}],                        {\"annotations\":[{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"INPUT\\u003cbr\\u003e(1 neuronas)\",\"x\":0,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"CONV1\\u003cbr\\u003e(16 neuronas)\",\"x\":1,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"CONV2\\u003cbr\\u003e(16 neuronas)\",\"x\":2,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"FC1\\u003cbr\\u003e(60 neuronas)\",\"x\":3,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"FC2\\u003cbr\\u003e(10 neuronas)\",\"x\":4,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"OUTPUT\\u003cbr\\u003e(1 neuronas)\",\"x\":5,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"align\":\"left\",\"showarrow\":false,\"text\":\"\\u003cb\\u003eLeyenda:\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eColor de nodo:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• \\u003cspan style='color:blue'\\u003eAzul\\u003c\\u002fspan\\u003e: Activación positiva\\u003cbr\\u003e• \\u003cspan style='color:red'\\u003eRojo\\u003c\\u002fspan\\u003e: Activación negativa\\u003cbr\\u003e• \\u003cspan style='color:gray'\\u003eGris\\u003c\\u002fspan\\u003e: Activación cercana a cero\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eTamaño de nodo:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• Mayor tamaño = Mayor magnitud de activación\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eColor de conexión:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• \\u003cspan style='color:blue'\\u003eAzul\\u003c\\u002fspan\\u003e: Peso positivo\\u003cbr\\u003e• \\u003cspan style='color:red'\\u003eRojo\\u003c\\u002fspan\\u003e: Peso negativo\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eGrosor de conexión:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• Mayor grosor = Mayor magnitud del peso\",\"x\":1.02,\"xref\":\"paper\",\"y\":0.5,\"yref\":\"paper\"},{\"align\":\"left\",\"showarrow\":false,\"text\":\"\\u003cb\\u003e¿Por qué hay activaciones negativas?\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eEn redes neuronales, las activaciones negativas\\u003cbr\\u003eocurren cuando el input a una neurona produce\\u003cbr\\u003eun valor negativo. Esto es común en capas con\\u003cbr\\u003efunciones de activación como ReLU, tanh o\\u003cbr\\u003efunciones lineales. Las activaciones negativas\\u003cbr\\u003epueden indicar inhibición o respuesta contraria\\u003cbr\\u003ea ciertas características de entrada.\",\"x\":1.02,\"xref\":\"paper\",\"y\":0.05,\"yref\":\"paper\"}],\"height\":1000,\"hovermode\":\"closest\",\"margin\":{\"b\":20,\"l\":5,\"r\":5,\"t\":40},\"showlegend\":false,\"title\":{\"font\":{\"size\":16},\"text\":\"Red Neuronal - Imagen 8\\u003cbr\\u003eEtiqueta real: 9, Predicción: 9\"},\"width\":1600,\"xaxis\":{\"showgrid\":false,\"showticklabels\":false,\"zeroline\":false},\"yaxis\":{\"showgrid\":false,\"showticklabels\":false,\"zeroline\":false},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e556b0dd-f79a-4115-9310-f29c1d56a7ad');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = visualizar_red_neuronal_interactiva(per_image_neuron_data, imagen_index=2)\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4A5uDIdTBY6",
        "outputId": "4cefdae6-abb5-45b4-e54a-c0ede3d4517c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1e7d1e81-a992-4741-be97-dcbc35c92094\" class=\"plotly-graph-div\" style=\"height:1000px; width:1600px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1e7d1e81-a992-4741-be97-dcbc35c92094\")) {                    Plotly.newPlot(                        \"1e7d1e81-a992-4741-be97-dcbc35c92094\",                        [{\"hoverinfo\":\"text\",\"line\":{\"width\":0.5},\"marker\":{\"color\":[\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(0, 0, 255, 0.49548879861831663)\",\"rgba(0, 0, 255, 0.478199428319931)\",\"rgba(0, 0, 255, 0.14466232284903527)\",\"rgba(255, 0, 0, 0.177683687210083)\",\"rgba(0, 0, 255, 0.3098765522241592)\",\"rgba(0, 0, 255, 0.3973027229309082)\",\"rgba(255, 0, 0, 0.2523659825325012)\",\"rgba(0, 0, 255, 0.47543243169784544)\",\"rgba(255, 0, 0, 0.15969223380088807)\",\"rgba(0, 0, 255, 0.1734786093235016)\",\"rgba(0, 0, 255, 0.5192788004875183)\",\"rgba(0, 0, 255, 0.5156277775764465)\",\"rgba(255, 0, 0, 0.17516806572675706)\",\"rgba(0, 0, 255, 0.3722387909889221)\",\"rgba(255, 0, 0, 0.17676905393600464)\",\"rgba(0, 0, 255, 0.35481775403022764)\",\"rgba(255, 0, 0, 0.18479847759008408)\",\"rgba(0, 0, 255, 0.11256333477795125)\",\"rgba(0, 0, 255, 0.17643774151802064)\",\"rgba(0, 0, 255, 0.32657302021980283)\",\"rgba(0, 0, 255, 0.44209170937538145)\",\"rgba(0, 0, 255, 0.41036651134490965)\",\"rgba(255, 0, 0, 0.2747374832630157)\",\"rgba(255, 0, 0, 0.2693179607391357)\",\"rgba(255, 0, 0, 0.2957087069749832)\",\"rgba(0, 0, 255, 0.16043727323412896)\",\"rgba(0, 0, 255, 0.10588635914027691)\",\"rgba(255, 0, 0, 0.2954487860202789)\",\"rgba(0, 0, 255, 0.5079583287239074)\",\"rgba(0, 0, 255, 0.2742736846208572)\",\"rgba(255, 0, 0, 0.22870168685913086)\",\"rgba(0, 0, 255, 0.37615121006965635)\",\"rgba(0, 0, 255, 0.2752258390188217)\",\"rgba(255, 0, 0, 0.10932643283158541)\",\"rgba(255, 0, 0, 0.299082151055336)\",\"rgba(0, 0, 255, 0.1544593557715416)\",\"rgba(255, 0, 0, 0.1878574475646019)\",\"rgba(255, 0, 0, 0.10299682095646859)\",\"rgba(0, 0, 255, 0.31407847106456754)\",\"rgba(0, 0, 255, 0.38076297044754026)\",\"rgba(0, 0, 255, 0.17920287549495698)\",\"rgba(0, 0, 255, 0.2862309873104095)\",\"rgba(0, 0, 255, 0.4426393866539001)\",\"rgba(255, 0, 0, 0.21012519896030427)\",\"rgba(0, 0, 255, 0.4459571897983551)\",\"rgba(255, 0, 0, 0.37126166224479673)\",\"rgba(255, 0, 0, 0.10629299692809582)\",\"rgba(0, 0, 255, 0.3153266370296478)\",\"rgba(0, 0, 255, 0.17139802724123002)\",\"rgba(0, 0, 255, 0.1725584849715233)\",\"rgba(0, 0, 255, 0.24965689182281495)\",\"rgba(0, 0, 255, 0.1810677632689476)\",\"rgba(0, 0, 255, 0.1727474093437195)\",\"rgba(255, 0, 0, 0.2172143742442131)\",\"rgba(255, 0, 0, 0.5470112025737762)\",\"rgba(255, 0, 0, 0.35271883606910703)\",\"rgba(255, 0, 0, 0.3498561441898346)\",\"rgba(0, 0, 255, 0.1630682334303856)\",\"rgba(0, 0, 255, 0.23590829074382783)\",\"rgba(0, 0, 255, 0.3403503060340881)\",\"rgba(0, 0, 255, 0.4222856521606445)\",\"rgba(255, 0, 0, 0.21231402307748795)\",\"rgba(0, 0, 255, 0.2133197695016861)\",\"rgba(0, 0, 255, 0.47085214257240293)\",\"rgba(0, 0, 255, 0.48222280144691465)\",\"rgba(0, 0, 255, 0.15390509217977524)\",\"rgba(0, 0, 255, 0.2591764390468597)\",\"rgba(255, 0, 0, 0.1489599294960499)\",\"rgba(0, 0, 255, 0.20857946425676346)\",\"rgba(255, 0, 0, 0.3309042364358902)\",\"rgba(0, 0, 255, 0.45697722434997556)\",\"rgba(255, 0, 0, 0.38950235247612)\",\"rgba(0, 0, 255, 0.3325397729873657)\",\"rgba(0, 0, 255, 0.27854142189025877)\",\"rgba(0, 0, 255, 0.46781996488571165)\",\"rgba(0, 0, 255, 0.23787814974784852)\",\"rgba(255, 0, 0, 0.1643598809838295)\",\"rgba(255, 0, 0, 0.15681707710027695)\",\"rgba(255, 0, 0, 0.2021089032292366)\",\"rgba(0, 0, 255, 0.3239493280649185)\",\"rgba(0, 0, 255, 0.2816739737987518)\",\"rgba(0, 0, 255, 0.41357854604721067)\",\"rgba(255, 0, 0, 0.2777995079755783)\",\"rgba(255, 0, 0, 0.2666818857192993)\",\"rgba(0, 0, 255, 0.42318800687789915)\",\"rgba(0, 0, 255, 0.18088799566030503)\",\"rgba(0, 0, 255, 0.2810082197189331)\",\"rgba(0, 0, 255, 0.28109431862831114)\",\"rgba(0, 0, 255, 0.4472943007946014)\",\"rgba(0, 0, 255, 0.27397598922252653)\",\"rgba(0, 0, 255, 0.24490857720375062)\",\"rgba(0, 0, 255, 0.46724478006362913)\",\"rgba(0, 0, 255, 0.4688847422599792)\",\"rgba(0, 0, 255, 0.13124589286744595)\",\"rgba(255, 0, 0, 0.187206169962883)\",\"rgba(255, 0, 0, 0.1550176203250885)\",\"rgba(0, 0, 255, 0.15971722677350045)\",\"rgba(0, 0, 255, 0.3590250968933105)\",\"rgba(0, 0, 255, 0.48915732502937315)\",\"rgba(0, 0, 255, 0.34438098073005674)\",\"rgba(0, 0, 255, 0.38259721398353574)\",\"rgba(0, 0, 255, 0.3597565948963165)\",\"rgba(255, 0, 0, 0.36958218216896055)\",\"rgba(255, 0, 0, 0.2608051061630249)\",\"rgba(0, 0, 255, 0.36007843017578123)\",\"rgba(255, 0, 0, 0.29213166832923887)\",\"rgba(0, 0, 255, 0.13488203138113022)\",\"rgba(255, 0, 0, 0.3189231365919113)\",\"rgba(255, 0, 0, 0.1583360865712166)\",\"rgba(255, 0, 0, 0.23929422199726105)\",\"rgba(0, 0, 255, 0.4234710395336151)\",\"rgba(255, 0, 0, 0.10799361150711775)\",\"rgba(255, 0, 0, 0.22437988817691804)\",\"rgba(255, 0, 0, 0.13250041976571084)\",\"rgba(0, 0, 255, 0.36733183860778806)\",\"rgba(255, 0, 0, 0.10533227529376746)\",\"rgba(0, 0, 255, 0.23108427822589875)\",\"rgba(0, 0, 255, 0.1269213728606701)\",\"rgba(0, 0, 255, 0.2178092733025551)\",\"rgba(0, 0, 255, 0.4475667834281921)\",\"rgba(0, 0, 255, 0.29243766367435453)\",\"rgba(255, 0, 0, 0.3232967615127563)\",\"rgba(0, 0, 255, 0.48328558206558225)\",\"rgba(0, 0, 255, 0.2952555626630783)\",\"rgba(255, 0, 0, 0.22201957404613495)\",\"rgba(0, 0, 255, 0.291690519452095)\",\"rgba(255, 0, 0, 0.3410115748643875)\",\"rgba(0, 0, 255, 0.23403376936912537)\",\"rgba(255, 0, 0, 0.1590900182723999)\",\"rgba(255, 0, 0, 0.23071313500404358)\",\"rgba(255, 0, 0, 0.16546325236558915)\",\"rgba(0, 0, 255, 0.5442004561424255)\",\"rgba(255, 0, 0, 0.10763260200619698)\",\"rgba(255, 0, 0, 0.3982511222362518)\",\"rgba(0, 0, 255, 0.3256711423397064)\",\"rgba(0, 0, 255, 0.28056963384151457)\",\"rgba(255, 0, 0, 0.3958565354347229)\",\"rgba(0, 0, 255, 0.23396245241165162)\",\"rgba(255, 0, 0, 0.11965508796274663)\",\"rgba(255, 0, 0, 0.36107057929039)\",\"rgba(0, 0, 255, 0.4305442631244659)\",\"rgba(255, 0, 0, 0.16671704351902009)\",\"rgba(0, 0, 255, 0.24037953317165375)\",\"rgba(0, 0, 255, 0.15226198583841324)\",\"rgba(0, 0, 255, 0.1386566072702408)\",\"rgba(255, 0, 0, 0.13595873713493348)\",\"rgba(255, 0, 0, 0.15670680478215218)\",\"rgba(255, 0, 0, 0.1724240615963936)\",\"rgba(255, 0, 0, 0.21617275327444077)\",\"rgba(0, 0, 255, 0.1054623312316835)\",\"rgba(0, 0, 255, 0.17781460136175156)\",\"rgba(0, 0, 255, 0.2550832390785217)\",\"rgba(0, 0, 255, 0.23532580435276032)\",\"rgba(0, 0, 255, 0.1271420180797577)\",\"rgba(0, 0, 255, 0.25483386814594267)\",\"rgba(0, 0, 255, 0.11958501152694226)\",\"rgba(255, 0, 0, 0.17505299150943757)\",\"rgba(255, 0, 0, 0.20065092146396638)\",\"rgba(255, 0, 0, 0.14416592419147492)\",\"rgba(255, 0, 0, 0.19682178497314454)\",\"rgba(255, 0, 0, 0.13787001967430115)\",\"rgba(0, 0, 255, 0.1209137424826622)\",\"rgba(0, 0, 255, 0.11684178784489632)\",\"rgba(0, 0, 255, 0.15851465463638306)\",\"rgba(255, 0, 0, 0.18223507553339005)\",\"rgba(0, 0, 255, 0.11689410023391247)\",\"rgba(0, 0, 255, 0.17545654475688935)\",\"rgba(0, 0, 255, 0.24069390296936036)\",\"rgba(0, 0, 255, 0.15251357331871987)\",\"rgba(0, 0, 255, 0.11536910142749549)\",\"rgba(0, 0, 255, 0.17129435986280442)\",\"rgba(0, 0, 255, 0.10941847078502179)\",\"rgba(255, 0, 0, 0.1767335817217827)\",\"rgba(0, 0, 255, 0.13535399213433266)\",\"rgba(0, 0, 255, 0.15336353927850724)\",\"rgba(0, 0, 255, 0.20378362983465195)\",\"rgba(0, 0, 255, 0.12714175544679165)\",\"rgba(0, 0, 255, 0.16998100876808167)\",\"rgba(0, 0, 255, 0.2746469140052795)\",\"rgba(0, 0, 255, 0.27548803985118864)\",\"rgba(0, 0, 255, 0.16393295526504517)\",\"rgba(255, 0, 0, 0.15806545540690423)\",\"rgba(0, 0, 255, 0.16643111258745194)\",\"rgba(0, 0, 255, 0.2083114892244339)\",\"rgba(255, 0, 0, 0.13128596022725106)\",\"rgba(0, 0, 255, 0.12906168811023236)\",\"rgba(0, 0, 255, 0.20891366004943848)\",\"rgba(255, 0, 0, 0.15957501381635666)\",\"rgba(255, 0, 0, 0.12259227074682713)\",\"rgba(255, 0, 0, 0.14415588453412057)\",\"rgba(0, 0, 255, 0.10972395390272141)\",\"rgba(0, 0, 255, 0.11184686925262213)\",\"rgba(255, 0, 0, 0.15321859195828438)\",\"rgba(0, 0, 255, 0.22908414006233216)\",\"rgba(0, 0, 255, 0.22326332926750184)\",\"rgba(0, 0, 255, 0.19815044701099396)\",\"rgba(255, 0, 0, 0.11162238139659167)\",\"rgba(0, 0, 255, 0.186253322660923)\",\"rgba(255, 0, 0, 0.14448411986231804)\",\"rgba(255, 0, 0, 0.1661178305745125)\",\"rgba(0, 0, 255, 0.20666831582784653)\",\"rgba(255, 0, 0, 0.12844959124922753)\",\"rgba(0, 0, 255, 0.11097518485039473)\",\"rgba(0, 0, 255, 0.1838935896754265)\",\"rgba(0, 0, 255, 0.19256912022829056)\",\"rgba(0, 0, 255, 0.21605315655469895)\",\"rgba(0, 0, 255, 0.2842900365591049)\",\"rgba(255, 0, 0, 0.12351627871394158)\",\"rgba(0, 0, 255, 0.15942964926362038)\",\"rgba(0, 0, 255, 0.19926258772611619)\",\"rgba(255, 0, 0, 0.1916508510708809)\",\"rgba(255, 0, 0, 0.1020223087631166)\",\"rgba(255, 0, 0, 0.12769693918526173)\",\"rgba(0, 0, 255, 0.12617848478257657)\",\"rgba(0, 0, 255, 0.2083066835999489)\",\"rgba(0, 0, 255, 0.2719566285610199)\",\"rgba(0, 0, 255, 0.24234213531017304)\",\"rgba(0, 0, 255, 0.28349374830722807)\",\"rgba(0, 0, 255, 0.1305425487458706)\",\"rgba(0, 0, 255, 0.11249254308640957)\",\"rgba(255, 0, 0, 0.1090659698471427)\",\"rgba(0, 0, 255, 0.1655781701207161)\",\"rgba(255, 0, 0, 0.21959099024534226)\",\"rgba(255, 0, 0, 0.13270576521754265)\",\"rgba(0, 0, 255, 0.1260148234665394)\",\"rgba(255, 0, 0, 0.16755132228136063)\",\"rgba(255, 0, 0, 0.16150597333908082)\",\"rgba(255, 0, 0, 0.19079410880804062)\",\"rgba(0, 0, 255, 0.16053490936756135)\",\"rgba(0, 0, 255, 0.1525706559419632)\",\"rgba(255, 0, 0, 0.16704683601856232)\",\"rgba(255, 0, 0, 0.1014490872854367)\",\"rgba(255, 0, 0, 0.1155973332002759)\",\"rgba(0, 0, 255, 0.12969427965581418)\",\"rgba(255, 0, 0, 0.19690987318754197)\",\"rgba(255, 0, 0, 0.1365249253809452)\",\"rgba(0, 0, 255, 0.16420420110225678)\",\"rgba(255, 0, 0, 0.12144586481153966)\",\"rgba(255, 0, 0, 0.19393659234046937)\",\"rgba(255, 0, 0, 0.10180329836439342)\",\"rgba(0, 0, 255, 0.15597561225295067)\",\"rgba(255, 0, 0, 0.11922614499926568)\",\"rgba(255, 0, 0, 0.1807197540998459)\",\"rgba(0, 0, 255, 0.13591635078191758)\",\"rgba(0, 0, 255, 0.12025427483022214)\",\"rgba(0, 0, 255, 0.10969479233026505)\",\"rgba(0, 0, 255, 0.13583026677370072)\",\"rgba(255, 0, 0, 0.10631299614906312)\",\"rgba(255, 0, 0, 0.14084687680006028)\",\"rgba(255, 0, 0, 0.12094217017292977)\",\"rgba(0, 0, 255, 0.17337053120136262)\",\"rgba(255, 0, 0, 0.12403955534100533)\",\"rgba(0, 0, 255, 0.10931183528155089)\",\"rgba(255, 0, 0, 0.1337904989719391)\",\"rgba(255, 0, 0, 0.14871316999197007)\",\"rgba(255, 0, 0, 0.13775223344564438)\",\"rgba(255, 0, 0, 0.15456002801656724)\",\"rgba(255, 0, 0, 0.15006642267107964)\",\"rgba(255, 0, 0, 0.11680560037493706)\",\"rgba(0, 0, 255, 0.1573251247406006)\",\"rgba(0, 0, 255, 0.15939288064837456)\",\"rgba(0, 0, 255, 0.16052596867084504)\",\"rgba(0, 0, 255, 0.15456433445215226)\",\"rgba(255, 0, 0, 0.12262217178940774)\",\"rgba(0, 0, 255, 0.11019854284822941)\",\"rgba(0, 0, 255, 0.15847012251615525)\",\"rgba(0, 0, 255, 0.16794290244579316)\",\"rgba(0, 0, 255, 0.10782784912735224)\",\"rgba(255, 0, 0, 0.15122115463018418)\",\"rgba(255, 0, 0, 0.15147096887230874)\",\"rgba(255, 0, 0, 0.11417598221451045)\",\"rgba(0, 0, 255, 0.19837329387664795)\",\"rgba(255, 0, 0, 0.12020259201526642)\",\"rgba(0, 0, 255, 0.2163897141814232)\",\"rgba(0, 0, 255, 0.20436511784791947)\",\"rgba(0, 0, 255, 0.21039837449789048)\",\"rgba(0, 0, 255, 0.2790644973516464)\",\"rgba(0, 0, 255, 0.22842857837677003)\",\"rgba(0, 0, 255, 0.25524275600910185)\",\"rgba(0, 0, 255, 0.19176838397979737)\",\"rgba(255, 0, 0, 0.11521802134811879)\",\"rgba(255, 0, 0, 0.24807364344596863)\",\"rgba(255, 0, 0, 0.14195474460721016)\",\"rgba(0, 0, 255, 0.21409225314855576)\",\"rgba(255, 0, 0, 0.1633474662899971)\",\"rgba(255, 0, 0, 0.16295283883810044)\",\"rgba(0, 0, 255, 0.14685645550489426)\",\"rgba(255, 0, 0, 0.14034655913710595)\",\"rgba(0, 0, 255, 0.10358967785723508)\",\"rgba(0, 0, 255, 0.1459873117506504)\",\"rgba(255, 0, 0, 0.12523482404649258)\",\"rgba(0, 0, 255, 0.15648961290717125)\",\"rgba(0, 0, 255, 0.1803705006837845)\",\"rgba(255, 0, 0, 0.22341751158237458)\",\"rgba(0, 0, 255, 0.17330399751663209)\",\"rgba(0, 0, 255, 0.11614801585674286)\",\"rgba(0, 0, 255, 0.13756815567612649)\",\"rgba(0, 0, 255, 0.1353468768298626)\",\"rgba(0, 0, 255, 0.12318404242396355)\",\"rgba(255, 0, 0, 0.1239439569413662)\",\"rgba(255, 0, 0, 0.15918757244944573)\",\"rgba(0, 0, 255, 0.24578624069690705)\",\"rgba(0, 0, 255, 0.15151745304465294)\",\"rgba(255, 0, 0, 0.12972494624555111)\",\"rgba(0, 0, 255, 0.2308891475200653)\",\"rgba(255, 0, 0, 0.10197818316519261)\",\"rgba(255, 0, 0, 0.23131056725978852)\",\"rgba(255, 0, 0, 0.12178245782852173)\",\"rgba(0, 0, 255, 0.14231640323996544)\",\"rgba(255, 0, 0, 0.21269105225801468)\",\"rgba(0, 0, 255, 0.11979728788137436)\",\"rgba(255, 0, 0, 0.11434784382581711)\",\"rgba(0, 0, 255, 0.17585982978343964)\",\"rgba(0, 0, 255, 0.2864547878503799)\",\"rgba(255, 0, 0, 0.19370731562376023)\",\"rgba(255, 0, 0, 0.11561188604682684)\",\"rgba(0, 0, 255, 0.17931382954120637)\",\"rgba(255, 0, 0, 0.18723654597997666)\",\"rgba(0, 0, 255, 0.1310098957270384)\",\"rgba(255, 0, 0, 0.10726854288950563)\",\"rgba(0, 0, 255, 0.12043144963681698)\",\"rgba(255, 0, 0, 0.14788494110107422)\",\"rgba(255, 0, 0, 0.19651190787553788)\",\"rgba(0, 0, 255, 0.10894980225712061)\",\"rgba(255, 0, 0, 0.11024332363158465)\",\"rgba(255, 0, 0, 0.1113961324095726)\",\"rgba(0, 0, 255, 0.19189245104789734)\",\"rgba(0, 0, 255, 0.1650133341550827)\",\"rgba(255, 0, 0, 0.12992463670670987)\",\"rgba(0, 0, 255, 0.20241249203681946)\",\"rgba(0, 0, 255, 0.1984674021601677)\",\"rgba(0, 0, 255, 0.13412004932761193)\",\"rgba(0, 0, 255, 0.16683219224214554)\",\"rgba(255, 0, 0, 0.12224020659923554)\",\"rgba(0, 0, 255, 0.12131464891135693)\",\"rgba(255, 0, 0, 0.12754815481603146)\",\"rgba(255, 0, 0, 0.21309650540351868)\",\"rgba(255, 0, 0, 0.1770358145236969)\",\"rgba(0, 0, 255, 0.11529336255043746)\",\"rgba(0, 0, 255, 0.20691694170236588)\",\"rgba(255, 0, 0, 0.14987320676445962)\",\"rgba(255, 0, 0, 0.1559365823864937)\",\"rgba(255, 0, 0, 0.1450836792588234)\",\"rgba(0, 0, 255, 0.22001517415046692)\",\"rgba(255, 0, 0, 0.20618361085653306)\",\"rgba(0, 0, 255, 0.14627396166324616)\",\"rgba(0, 0, 255, 0.28120065331459043)\",\"rgba(0, 0, 255, 0.11846693083643914)\",\"rgba(255, 0, 0, 0.17231620699167252)\",\"rgba(0, 0, 255, 0.13356506302952767)\",\"rgba(0, 0, 255, 0.10218037003651262)\",\"rgba(0, 0, 255, 0.21720247566699982)\",\"rgba(0, 0, 255, 0.1685992017388344)\",\"rgba(255, 0, 0, 0.14049494117498398)\",\"rgba(0, 0, 255, 0.19640442579984665)\",\"rgba(0, 0, 255, 0.2749678999185562)\",\"rgba(0, 0, 255, 0.10281565403565765)\",\"rgba(0, 0, 255, 0.10951499957591296)\",\"rgba(255, 0, 0, 0.11713052950799466)\",\"rgba(0, 0, 255, 0.13773346543312073)\",\"rgba(255, 0, 0, 0.1716003179550171)\",\"rgba(0, 0, 255, 0.17127328962087632)\",\"rgba(0, 0, 255, 0.12467096224427224)\",\"rgba(0, 0, 255, 0.19485531598329545)\",\"rgba(0, 0, 255, 0.14595022648572922)\",\"rgba(0, 0, 255, 0.2692528277635574)\",\"rgba(0, 0, 255, 0.10690880464389921)\",\"rgba(0, 0, 255, 0.20661641508340836)\",\"rgba(0, 0, 255, 0.15907241627573968)\",\"rgba(255, 0, 0, 0.14439767450094224)\",\"rgba(255, 0, 0, 0.1238613747060299)\",\"rgba(255, 0, 0, 0.19980181604623795)\",\"rgba(255, 0, 0, 0.14902878031134606)\",\"rgba(0, 0, 255, 0.14211461916565896)\",\"rgba(0, 0, 255, 0.256250849366188)\",\"rgba(0, 0, 255, 0.14176847636699677)\",\"rgba(0, 0, 255, 0.262353903055191)\",\"rgba(0, 0, 255, 0.18315620571374894)\",\"rgba(0, 0, 255, 0.12715127542614937)\",\"rgba(255, 0, 0, 0.11525964587926865)\",\"rgba(0, 0, 255, 0.1643814578652382)\",\"rgba(0, 0, 255, 0.19453471750020981)\",\"rgba(0, 0, 255, 0.11423598732799292)\",\"rgba(255, 0, 0, 0.1350361578166485)\",\"rgba(0, 0, 255, 0.24258751273155213)\",\"rgba(0, 0, 255, 0.21224818229675294)\",\"rgba(0, 0, 255, 0.12961379289627076)\",\"rgba(0, 0, 255, 0.2269052028656006)\",\"rgba(0, 0, 255, 0.23853234052658082)\",\"rgba(255, 0, 0, 0.20922084748744965)\",\"rgba(255, 0, 0, 0.22195508182048798)\",\"rgba(255, 0, 0, 0.21937642097473145)\",\"rgba(255, 0, 0, 0.18703076094388962)\",\"rgba(0, 0, 255, 0.19272335469722748)\",\"rgba(255, 0, 0, 0.2073676198720932)\",\"rgba(255, 0, 0, 0.11221138704568148)\",\"rgba(0, 0, 255, 0.20201732069253922)\",\"rgba(0, 0, 255, 0.15784472450613976)\",\"rgba(0, 0, 255, 0.13575390949845315)\",\"rgba(0, 0, 255, 0.17514524459838868)\",\"rgba(0, 0, 255, 0.10388252851553262)\",\"rgba(255, 0, 0, 0.11220337487757207)\",\"rgba(255, 0, 0, 0.20671365261077881)\",\"rgba(0, 0, 255, 0.14328499361872674)\",\"rgba(255, 0, 0, 0.12937498688697815)\",\"rgba(255, 0, 0, 0.1867907926440239)\",\"rgba(0, 0, 255, 0.1518893264234066)\",\"rgba(0, 0, 255, 0.12568084746599198)\",\"rgba(0, 0, 255, 0.10187984190415592)\",\"rgba(0, 0, 255, 0.20666269063949586)\",\"rgba(255, 0, 0, 0.1337924286723137)\",\"rgba(255, 0, 0, 0.12738125808537007)\",\"rgba(0, 0, 255, 0.23599146902561188)\",\"rgba(255, 0, 0, 0.1201748013496399)\",\"rgba(0, 0, 255, 0.1866667553782463)\",\"rgba(0, 0, 255, 0.16635120511054993)\",\"rgba(255, 0, 0, 0.12595003321766854)\",\"rgba(0, 0, 255, 0.11274861209094525)\",\"rgba(255, 0, 0, 0.1086600873619318)\",\"rgba(0, 0, 255, 0.17839718461036683)\",\"rgba(0, 0, 255, 0.14725469276309014)\",\"rgba(255, 0, 0, 0.10861964654177428)\",\"rgba(0, 0, 255, 0.21814913898706437)\",\"rgba(0, 0, 255, 0.1377427414059639)\",\"rgba(0, 0, 255, 0.13983725234866143)\",\"rgba(0, 0, 255, 0.24521875977516175)\",\"rgba(255, 0, 0, 0.11230627484619618)\",\"rgba(255, 0, 0, 0.1379784442484379)\",\"rgba(0, 0, 255, 0.11385879050940276)\",\"rgba(255, 0, 0, 0.1376894883811474)\",\"rgba(0, 0, 255, 0.16396060436964036)\",\"rgba(255, 0, 0, 0.19228187799453736)\",\"rgba(255, 0, 0, 0.16415797024965287)\",\"rgba(0, 0, 255, 0.15122511833906174)\",\"rgba(0, 0, 255, 0.2187347322702408)\",\"rgba(0, 0, 255, 0.15166426301002503)\",\"rgba(0, 0, 255, 0.2489807516336441)\",\"rgba(0, 0, 255, 0.11504863426089287)\",\"rgba(0, 0, 255, 0.1371341183781624)\",\"rgba(0, 0, 255, 0.18508042246103287)\",\"rgba(0, 0, 255, 0.18222902566194535)\",\"rgba(0, 0, 255, 0.22881997227668763)\",\"rgba(0, 0, 255, 0.23717131316661835)\",\"rgba(0, 0, 255, 0.16285112351179123)\",\"rgba(0, 0, 255, 0.11251801289618016)\",\"rgba(255, 0, 0, 0.12101685851812363)\",\"rgba(255, 0, 0, 0.15124419182538987)\",\"rgba(0, 0, 255, 0.1134767796844244)\",\"rgba(0, 0, 255, 0.12203266695141793)\",\"rgba(0, 0, 255, 0.10303762708790601)\",\"rgba(0, 0, 255, 0.2956202834844589)\",\"rgba(0, 0, 255, 0.14800493642687798)\",\"rgba(0, 0, 255, 0.17198162376880646)\",\"rgba(0, 0, 255, 0.12344769425690175)\",\"rgba(0, 0, 255, 0.10664820661768318)\",\"rgba(255, 0, 0, 0.10932208448648453)\",\"rgba(0, 0, 255, 0.11176549028605223)\",\"rgba(255, 0, 0, 0.1951811671257019)\",\"rgba(255, 0, 0, 0.23855357468128205)\",\"rgba(0, 0, 255, 0.19646214544773102)\",\"rgba(0, 0, 255, 0.23631613552570344)\",\"rgba(255, 0, 0, 0.13550074622035027)\",\"rgba(255, 0, 0, 0.13415663540363312)\",\"rgba(0, 0, 255, 0.10178810534998775)\",\"rgba(255, 0, 0, 0.10528963981196285)\",\"rgba(0, 0, 255, 0.11088422257453204)\",\"rgba(255, 0, 0, 0.11812226139009)\",\"rgba(255, 0, 0, 0.12855682745575905)\",\"rgba(255, 0, 0, 0.10194849295075983)\",\"rgba(0, 0, 255, 0.11335254777222872)\",\"rgba(255, 0, 0, 0.10477193295955659)\",\"rgba(0, 0, 255, 0.13525649756193162)\",\"rgba(255, 0, 0, 0.10138341111596674)\",\"rgba(255, 0, 0, 0.11898531429469586)\",\"rgba(255, 0, 0, 0.10669446168467403)\",\"rgba(0, 0, 255, 0.11909543946385384)\",\"rgba(0, 0, 255, 0.11284026727080346)\",\"rgba(0, 0, 255, 0.12494340389966965)\",\"rgba(255, 0, 0, 0.13423922508955002)\",\"rgba(0, 0, 255, 0.10875162612646819)\",\"rgba(0, 0, 255, 0.12378135360777379)\",\"rgba(0, 0, 255, 0.11301876362413169)\",\"rgba(0, 0, 255, 0.1392383858561516)\",\"rgba(255, 0, 0, 0.12818730287253857)\",\"rgba(255, 0, 0, 0.10952464435249568)\",\"rgba(255, 0, 0, 0.1302299577742815)\",\"rgba(0, 0, 255, 0.11377327833324671)\",\"rgba(255, 0, 0, 0.11041976548731328)\",\"rgba(255, 0, 0, 0.13126677870750428)\",\"rgba(0, 0, 255, 0.13708266839385033)\",\"rgba(0, 0, 255, 0.1256254728883505)\",\"rgba(255, 0, 0, 0.12527118287980557)\",\"rgba(255, 0, 0, 0.12645238488912583)\",\"rgba(0, 0, 255, 0.11784278266131878)\",\"rgba(0, 0, 255, 0.1245390459895134)\",\"rgba(255, 0, 0, 0.10199589133262635)\",\"rgba(0, 0, 255, 0.12229139767587185)\",\"rgba(255, 0, 0, 0.12321538887917996)\",\"rgba(255, 0, 0, 0.10967915635555983)\",\"rgba(0, 0, 255, 0.13354786708950997)\",\"rgba(255, 0, 0, 0.13497540578246117)\",\"rgba(255, 0, 0, 0.11177858095616103)\",\"rgba(0, 0, 255, 0.12067967876791955)\",\"rgba(0, 0, 255, 0.1145891271531582)\",\"rgba(255, 0, 0, 0.11227287389338017)\",\"rgba(255, 0, 0, 0.1227322205901146)\",\"rgba(0, 0, 255, 0.11777483895421029)\",\"rgba(255, 0, 0, 0.1210136566311121)\",\"rgba(0, 0, 255, 0.13480204567313195)\",\"rgba(0, 0, 255, 0.10357760721817613)\",\"rgba(255, 0, 0, 0.10157259195111693)\",\"rgba(255, 0, 0, 0.10464161392301322)\",\"rgba(0, 0, 255, 0.12674394473433495)\",\"rgba(255, 0, 0, 0.11254624594002963)\",\"rgba(255, 0, 0, 0.10764271104708314)\",\"rgba(0, 0, 255, 0.12714161574840546)\",\"rgba(255, 0, 0, 0.12564972266554833)\",\"rgba(255, 0, 0, 0.11082544773817063)\",\"rgba(255, 0, 0, 0.10330140697769821)\",\"rgba(255, 0, 0, 0.10202240608632565)\",\"rgba(0, 0, 255, 0.10889413338154555)\",\"rgba(255, 0, 0, 0.11328547578305007)\",\"rgba(255, 0, 0, 0.10853688456118107)\",\"rgba(255, 0, 0, 0.12781956270337105)\",\"rgba(0, 0, 255, 0.11289478410035372)\",\"rgba(0, 0, 255, 0.10882397033274174)\",\"rgba(0, 0, 255, 0.12341452799737454)\",\"rgba(255, 0, 0, 0.13452210128307343)\",\"rgba(0, 0, 255, 0.12166286669671536)\",\"rgba(0, 0, 255, 0.1051689762622118)\",\"rgba(255, 0, 0, 0.10501733785495163)\",\"rgba(0, 0, 255, 0.12685213834047318)\",\"rgba(255, 0, 0, 0.12051553502678872)\",\"rgba(255, 0, 0, 0.12066951431334019)\",\"rgba(255, 0, 0, 0.11311690732836724)\",\"rgba(255, 0, 0, 0.12540358901023865)\",\"rgba(0, 0, 255, 0.11209100801497698)\",\"rgba(0, 0, 255, 0.13927620872855187)\",\"rgba(0, 0, 255, 0.10848551187664271)\",\"rgba(0, 0, 255, 0.10279579334892333)\",\"rgba(255, 0, 0, 0.12267191745340825)\",\"rgba(255, 0, 0, 0.10763601623475552)\",\"rgba(0, 0, 255, 0.11573729328811169)\",\"rgba(255, 0, 0, 0.10125677592586727)\",\"rgba(255, 0, 0, 0.13239775821566582)\",\"rgba(255, 0, 0, 0.13136330842971802)\",\"rgba(255, 0, 0, 0.12779588103294373)\",\"rgba(255, 0, 0, 0.1357319675385952)\",\"rgba(255, 0, 0, 0.10869381800293923)\",\"rgba(255, 0, 0, 0.13561437502503396)\",\"rgba(255, 0, 0, 0.1225213747471571)\",\"rgba(255, 0, 0, 0.12315363846719266)\",\"rgba(0, 0, 255, 0.11590544171631337)\",\"rgba(255, 0, 0, 0.12157687954604626)\",\"rgba(0, 0, 255, 0.10577223906293512)\",\"rgba(255, 0, 0, 0.12614531479775906)\",\"rgba(0, 0, 255, 0.10851957499980927)\",\"rgba(0, 0, 255, 0.12348689734935761)\",\"rgba(0, 0, 255, 0.12644682675600052)\",\"rgba(0, 0, 255, 0.11854973286390305)\",\"rgba(255, 0, 0, 0.10309220398776234)\",\"rgba(0, 0, 255, 0.12290166728198529)\",\"rgba(0, 0, 255, 0.1170432385057211)\",\"rgba(0, 0, 255, 0.12599342912435532)\",\"rgba(255, 0, 0, 0.11203938946127892)\",\"rgba(255, 0, 0, 0.12202796190977097)\",\"rgba(255, 0, 0, 0.12837234921753407)\",\"rgba(255, 0, 0, 0.12956916391849518)\",\"rgba(0, 0, 255, 0.10923163164407015)\",\"rgba(0, 0, 255, 0.11519543025642634)\",\"rgba(0, 0, 255, 0.1216660387814045)\",\"rgba(0, 0, 255, 0.12724638395011426)\",\"rgba(255, 0, 0, 0.10837976671755314)\",\"rgba(0, 0, 255, 0.12760357223451138)\",\"rgba(255, 0, 0, 0.10698248436674476)\",\"rgba(255, 0, 0, 0.12589728683233262)\",\"rgba(255, 0, 0, 0.13548674285411835)\",\"rgba(255, 0, 0, 0.12117929048836232)\",\"rgba(255, 0, 0, 0.12281826175749302)\",\"rgba(0, 0, 255, 0.12560950629413128)\",\"rgba(255, 0, 0, 0.1332603193819523)\",\"rgba(255, 0, 0, 0.1105143990367651)\",\"rgba(0, 0, 255, 0.11511187013238669)\",\"rgba(255, 0, 0, 0.115893142670393)\",\"rgba(0, 0, 255, 0.11972759515047074)\",\"rgba(255, 0, 0, 0.130759422108531)\",\"rgba(255, 0, 0, 0.11499363873153925)\",\"rgba(0, 0, 255, 0.12377713657915593)\",\"rgba(255, 0, 0, 0.1070054205134511)\",\"rgba(255, 0, 0, 0.1233430676162243)\",\"rgba(0, 0, 255, 0.11888049766421319)\",\"rgba(0, 0, 255, 0.13450947627425194)\",\"rgba(255, 0, 0, 0.10393780786544085)\",\"rgba(0, 0, 255, 0.1307334214448929)\",\"rgba(255, 0, 0, 0.1209021419286728)\",\"rgba(255, 0, 0, 0.10316362059675158)\",\"rgba(0, 0, 255, 0.11180319860577584)\",\"rgba(0, 0, 255, 0.13188538178801537)\",\"rgba(0, 0, 255, 0.10275327498093248)\",\"rgba(0, 0, 255, 0.10653820391744376)\",\"rgba(0, 0, 255, 0.10739089958369732)\",\"rgba(0, 0, 255, 0.12424860559403897)\",\"rgba(255, 0, 0, 0.11453035324811936)\",\"rgba(0, 0, 255, 0.10300271180458367)\",\"rgba(0, 0, 255, 0.13130850940942765)\",\"rgba(0, 0, 255, 0.1298285335302353)\",\"rgba(0, 0, 255, 0.11188487280160189)\",\"rgba(0, 0, 255, 0.12178274281322957)\",\"rgba(255, 0, 0, 0.13709698840975762)\",\"rgba(0, 0, 255, 0.11990456692874432)\",\"rgba(0, 0, 255, 0.12315901406109334)\",\"rgba(255, 0, 0, 0.1294719133526087)\",\"rgba(0, 0, 255, 0.12622405998408795)\",\"rgba(0, 0, 255, 0.10833397079259158)\",\"rgba(255, 0, 0, 0.11352474559098483)\",\"rgba(0, 0, 255, 0.10798902846872807)\",\"rgba(0, 0, 255, 0.11840439066290856)\",\"rgba(0, 0, 255, 0.11246601063758135)\",\"rgba(0, 0, 255, 0.13183584660291672)\",\"rgba(255, 0, 0, 0.11004013419151307)\",\"rgba(0, 0, 255, 0.11885845139622689)\",\"rgba(255, 0, 0, 0.12595820650458336)\",\"rgba(0, 0, 255, 0.11622243598103524)\",\"rgba(255, 0, 0, 0.1310254879295826)\",\"rgba(255, 0, 0, 0.11859396696090699)\",\"rgba(255, 0, 0, 0.13564567118883133)\",\"rgba(0, 0, 255, 0.10313020590692759)\",\"rgba(0, 0, 255, 0.12493580430746079)\",\"rgba(0, 0, 255, 0.1075038094073534)\",\"rgba(0, 0, 255, 0.11264271512627602)\",\"rgba(255, 0, 0, 0.12367415465414525)\",\"rgba(255, 0, 0, 0.12898183465003968)\",\"rgba(255, 0, 0, 0.11142000779509545)\",\"rgba(0, 0, 255, 0.13165503963828087)\",\"rgba(0, 0, 255, 0.1350271873176098)\",\"rgba(255, 0, 0, 0.11390573754906655)\",\"rgba(0, 0, 255, 0.1333295352756977)\",\"rgba(0, 0, 255, 0.1265175923705101)\",\"rgba(0, 0, 255, 0.11086704060435296)\",\"rgba(255, 0, 0, 0.11598026417195798)\",\"rgba(255, 0, 0, 0.12957257628440857)\",\"rgba(255, 0, 0, 0.13771797567605973)\",\"rgba(255, 0, 0, 0.11726574636995793)\",\"rgba(255, 0, 0, 0.1128816943615675)\",\"rgba(0, 0, 255, 0.13586895763874055)\",\"rgba(0, 0, 255, 0.11412793342024088)\",\"rgba(0, 0, 255, 0.1373203605413437)\",\"rgba(255, 0, 0, 0.10446724426001311)\",\"rgba(255, 0, 0, 0.12678651548922062)\",\"rgba(255, 0, 0, 0.10116313295438886)\",\"rgba(0, 0, 255, 0.12944259718060494)\",\"rgba(0, 0, 255, 0.12917208522558213)\",\"rgba(0, 0, 255, 0.11909547299146653)\",\"rgba(255, 0, 0, 0.10127910147421063)\",\"rgba(255, 0, 0, 0.13052162006497384)\",\"rgba(0, 0, 255, 0.1019775559194386)\",\"rgba(0, 0, 255, 0.13780749812722207)\",\"rgba(0, 0, 255, 0.14757176339626313)\",\"rgba(255, 0, 0, 0.4003326326608658)\",\"rgba(255, 0, 0, 0.41093588918447493)\",\"rgba(255, 0, 0, 0.3339069217443466)\",\"rgba(255, 0, 0, 0.3273177847266197)\",\"rgba(0, 0, 255, 0.388435135781765)\",\"rgba(255, 0, 0, 0.4264832764863968)\",\"rgba(0, 0, 255, 0.3855177417397499)\",\"rgba(255, 0, 0, 0.4082920357584953)\",\"rgba(255, 0, 0, 0.3389418564736843)\",\"rgba(255, 0, 0, 0.38378174901008605)\"],\"size\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1.18646639585495,1.134598284959793,0.5,0.5,0.6296296566724777,0.8919081687927246,0.5,1.1262972950935364,0.5,0.5,1.257836401462555,1.2468833327293396,0.5,0.8167163729667664,0.5,0.764453262090683,0.5,0.5,0.5,0.6797190606594086,1.0262751281261444,0.931099534034729,0.5242124497890472,0.5079538822174072,0.5871261209249496,0.5,0.5,0.5863463580608368,1.2238749861717224,0.5228210538625717,0.5,0.8284536302089691,0.5256775170564651,0.5,0.597246453166008,0.5,0.5,0.5,0.6422354131937027,0.8422889113426208,0.5,0.5586929619312286,1.0279181599617004,0.5,1.0378715693950653,0.8137849867343903,0.5,0.6459799110889435,0.5,0.5,0.5,0.5,0.5,0.5,1.3410336077213287,0.7581565082073212,0.7495684325695038,0.5,0.5,0.7210509181022644,0.9668569564819336,0.5,0.5,1.1125564277172089,1.146668404340744,0.5,0.5,0.5,0.5,0.6927127093076706,1.0709316730499268,0.86850705742836,0.6976193189620972,0.5356242656707764,1.103459894657135,0.5,0.5,0.5,0.5,0.6718479841947556,0.5450219213962555,0.9407356381416321,0.5333985239267349,0.500045657157898,0.9695640206336975,0.5,0.5430246591567993,0.5432829558849335,1.0418829023838043,0.5219279676675797,0.5,1.1017343401908875,1.1066542267799377,0.5,0.5,0.5,0.5,0.7770752906799316,1.1674719750881195,0.7331429421901703,0.8477916419506073,0.7792697846889496,0.8087465465068817,0.5,0.7802352905273438,0.5763950049877167,0.5,0.656769409775734,0.5,0.5,0.9704131186008453,0.5,0.5,0.5,0.8019955158233643,0.5,0.5,0.5,0.5,1.0427003502845764,0.5773129910230637,0.669890284538269,1.1498567461967468,0.5857666879892349,0.5,0.5750715583562851,0.7230347245931625,0.5,0.5,0.5,0.5,1.3326013684272766,0.5,0.8947533667087555,0.6770134270191193,0.5417089015245438,0.8875696063041687,0.5,0.5,0.78321173787117,0.9916327893733978,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5239407420158386,0.526464119553566,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5528701096773148,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5158698856830597,0.5,0.5504812449216843,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5371934920549393,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5593643635511398,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5436019599437714,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5249036997556686,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5077584832906723,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5868608504533768,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,1,1,1,1,1,1,1,1,1,1]},\"mode\":\"lines\",\"text\":[\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Peso: 0.3955\",\"Peso: 0.3782\",\"Peso: 0.0447\",\"Peso: -0.0777\",\"Peso: 0.2099\",\"Peso: 0.2973\",\"Peso: -0.1524\",\"Peso: 0.3754\",\"Peso: -0.0597\",\"Peso: 0.0735\",\"Peso: 0.4193\",\"Peso: 0.4156\",\"Peso: -0.0752\",\"Peso: 0.2722\",\"Peso: -0.0768\",\"Peso: 0.2548\",\"Peso: -0.0848\",\"Peso: 0.0126\",\"Peso: 0.0764\",\"Peso: 0.2266\",\"Peso: 0.3421\",\"Peso: 0.3104\",\"Peso: -0.1747\",\"Peso: -0.1693\",\"Peso: -0.1957\",\"Peso: 0.0604\",\"Peso: 0.0059\",\"Peso: -0.1954\",\"Peso: 0.4080\",\"Peso: 0.1743\",\"Peso: -0.1287\",\"Peso: 0.2762\",\"Peso: 0.1752\",\"Peso: -0.0093\",\"Peso: -0.1991\",\"Peso: 0.0545\",\"Peso: -0.0879\",\"Peso: -0.0030\",\"Peso: 0.2141\",\"Peso: 0.2808\",\"Peso: 0.0792\",\"Peso: 0.1862\",\"Peso: 0.3426\",\"Peso: -0.1101\",\"Peso: 0.3460\",\"Peso: -0.2713\",\"Peso: -0.0063\",\"Peso: 0.2153\",\"Peso: 0.0714\",\"Peso: 0.0726\",\"Peso: 0.1497\",\"Peso: 0.0811\",\"Peso: 0.0727\",\"Peso: -0.1172\",\"Peso: -0.4470\",\"Peso: -0.2527\",\"Peso: -0.2499\",\"Peso: 0.0631\",\"Peso: 0.1359\",\"Peso: 0.2404\",\"Peso: 0.3223\",\"Peso: -0.1123\",\"Peso: 0.1133\",\"Peso: 0.3709\",\"Peso: 0.3822\",\"Peso: 0.0539\",\"Peso: 0.1592\",\"Peso: -0.0490\",\"Peso: 0.1086\",\"Peso: -0.2309\",\"Peso: 0.3570\",\"Peso: -0.2895\",\"Peso: 0.2325\",\"Peso: 0.1785\",\"Peso: 0.3678\",\"Peso: 0.1379\",\"Peso: -0.0644\",\"Peso: -0.0568\",\"Peso: -0.1021\",\"Peso: 0.2239\",\"Peso: 0.1817\",\"Peso: 0.3136\",\"Peso: -0.1778\",\"Peso: -0.1667\",\"Peso: 0.3232\",\"Peso: 0.0809\",\"Peso: 0.1810\",\"Peso: 0.1811\",\"Peso: 0.3473\",\"Peso: 0.1740\",\"Peso: 0.1449\",\"Peso: 0.3672\",\"Peso: 0.3689\",\"Peso: 0.0312\",\"Peso: -0.0872\",\"Peso: -0.0550\",\"Peso: 0.0597\",\"Peso: 0.2590\",\"Peso: 0.3892\",\"Peso: 0.2444\",\"Peso: 0.2826\",\"Peso: 0.2598\",\"Peso: -0.2696\",\"Peso: -0.1608\",\"Peso: 0.2601\",\"Peso: -0.1921\",\"Peso: 0.0349\",\"Peso: -0.2189\",\"Peso: -0.0583\",\"Peso: -0.1393\",\"Peso: 0.3235\",\"Peso: -0.0080\",\"Peso: -0.1244\",\"Peso: -0.0325\",\"Peso: 0.2673\",\"Peso: -0.0053\",\"Peso: 0.1311\",\"Peso: 0.0269\",\"Peso: 0.1178\",\"Peso: 0.3476\",\"Peso: 0.1924\",\"Peso: -0.2233\",\"Peso: 0.3833\",\"Peso: 0.1953\",\"Peso: -0.1220\",\"Peso: 0.1917\",\"Peso: -0.2410\",\"Peso: 0.1340\",\"Peso: -0.0591\",\"Peso: -0.1307\",\"Peso: -0.0655\",\"Peso: 0.4442\",\"Peso: -0.0076\",\"Peso: -0.2983\",\"Peso: 0.2257\",\"Peso: 0.1806\",\"Peso: -0.2959\",\"Peso: 0.1340\",\"Peso: -0.0197\",\"Peso: -0.2611\",\"Peso: 0.3305\",\"Peso: -0.0667\",\"Peso: 0.1404\",\"Peso: 0.0523\",\"Peso: 0.0387\",\"Peso: -0.0360\",\"Peso: -0.0567\",\"Peso: -0.0724\",\"Peso: -0.1162\",\"Peso: 0.0055\",\"Peso: 0.0778\",\"Peso: 0.1551\",\"Peso: 0.1353\",\"Peso: 0.0271\",\"Peso: 0.1548\",\"Peso: 0.0196\",\"Peso: -0.0751\",\"Peso: -0.1007\",\"Peso: -0.0442\",\"Peso: -0.0968\",\"Peso: -0.0379\",\"Peso: 0.0209\",\"Peso: 0.0168\",\"Peso: 0.0585\",\"Peso: -0.0822\",\"Peso: 0.0169\",\"Peso: 0.0755\",\"Peso: 0.1407\",\"Peso: 0.0525\",\"Peso: 0.0154\",\"Peso: 0.0713\",\"Peso: 0.0094\",\"Peso: -0.0767\",\"Peso: 0.0354\",\"Peso: 0.0534\",\"Peso: 0.1038\",\"Peso: 0.0271\",\"Peso: 0.0700\",\"Peso: 0.1746\",\"Peso: 0.1755\",\"Peso: 0.0639\",\"Peso: -0.0581\",\"Peso: 0.0664\",\"Peso: 0.1083\",\"Peso: -0.0313\",\"Peso: 0.0291\",\"Peso: 0.1089\",\"Peso: -0.0596\",\"Peso: -0.0226\",\"Peso: -0.0442\",\"Peso: 0.0097\",\"Peso: 0.0118\",\"Peso: -0.0532\",\"Peso: 0.1291\",\"Peso: 0.1233\",\"Peso: 0.0982\",\"Peso: -0.0116\",\"Peso: 0.0863\",\"Peso: -0.0445\",\"Peso: -0.0661\",\"Peso: 0.1067\",\"Peso: -0.0284\",\"Peso: 0.0110\",\"Peso: 0.0839\",\"Peso: 0.0926\",\"Peso: 0.1161\",\"Peso: 0.1843\",\"Peso: -0.0235\",\"Peso: 0.0594\",\"Peso: 0.0993\",\"Peso: -0.0917\",\"Peso: -0.0020\",\"Peso: -0.0277\",\"Peso: 0.0262\",\"Peso: 0.1083\",\"Peso: 0.1720\",\"Peso: 0.1423\",\"Peso: 0.1835\",\"Peso: 0.0305\",\"Peso: 0.0125\",\"Peso: -0.0091\",\"Peso: 0.0656\",\"Peso: -0.1196\",\"Peso: -0.0327\",\"Peso: 0.0260\",\"Peso: -0.0676\",\"Peso: -0.0615\",\"Peso: -0.0908\",\"Peso: 0.0605\",\"Peso: 0.0526\",\"Peso: -0.0670\",\"Peso: -0.0014\",\"Peso: -0.0156\",\"Peso: 0.0297\",\"Peso: -0.0969\",\"Peso: -0.0365\",\"Peso: 0.0642\",\"Peso: -0.0214\",\"Peso: -0.0939\",\"Peso: -0.0018\",\"Peso: 0.0560\",\"Peso: -0.0192\",\"Peso: -0.0807\",\"Peso: 0.0359\",\"Peso: 0.0203\",\"Peso: 0.0097\",\"Peso: 0.0358\",\"Peso: -0.0063\",\"Peso: -0.0408\",\"Peso: -0.0209\",\"Peso: 0.0734\",\"Peso: -0.0240\",\"Peso: 0.0093\",\"Peso: -0.0338\",\"Peso: -0.0487\",\"Peso: -0.0378\",\"Peso: -0.0546\",\"Peso: -0.0501\",\"Peso: -0.0168\",\"Peso: 0.0573\",\"Peso: 0.0594\",\"Peso: 0.0605\",\"Peso: 0.0546\",\"Peso: -0.0226\",\"Peso: 0.0102\",\"Peso: 0.0585\",\"Peso: 0.0679\",\"Peso: 0.0078\",\"Peso: -0.0512\",\"Peso: -0.0515\",\"Peso: -0.0142\",\"Peso: 0.0984\",\"Peso: -0.0202\",\"Peso: 0.1164\",\"Peso: 0.1044\",\"Peso: 0.1104\",\"Peso: 0.1791\",\"Peso: 0.1284\",\"Peso: 0.1552\",\"Peso: 0.0918\",\"Peso: -0.0152\",\"Peso: -0.1481\",\"Peso: -0.0420\",\"Peso: 0.1141\",\"Peso: -0.0633\",\"Peso: -0.0630\",\"Peso: 0.0469\",\"Peso: -0.0403\",\"Peso: 0.0036\",\"Peso: 0.0460\",\"Peso: -0.0252\",\"Peso: 0.0565\",\"Peso: 0.0804\",\"Peso: -0.1234\",\"Peso: 0.0733\",\"Peso: 0.0161\",\"Peso: 0.0376\",\"Peso: 0.0353\",\"Peso: 0.0232\",\"Peso: -0.0239\",\"Peso: -0.0592\",\"Peso: 0.1458\",\"Peso: 0.0515\",\"Peso: -0.0297\",\"Peso: 0.1309\",\"Peso: -0.0020\",\"Peso: -0.1313\",\"Peso: -0.0218\",\"Peso: 0.0423\",\"Peso: -0.1127\",\"Peso: 0.0198\",\"Peso: -0.0143\",\"Peso: 0.0759\",\"Peso: 0.1865\",\"Peso: -0.0937\",\"Peso: -0.0156\",\"Peso: 0.0793\",\"Peso: -0.0872\",\"Peso: 0.0310\",\"Peso: -0.0073\",\"Peso: 0.0204\",\"Peso: -0.0479\",\"Peso: -0.0965\",\"Peso: 0.0089\",\"Peso: -0.0102\",\"Peso: -0.0114\",\"Peso: 0.0919\",\"Peso: 0.0650\",\"Peso: -0.0299\",\"Peso: 0.1024\",\"Peso: 0.0985\",\"Peso: 0.0341\",\"Peso: 0.0668\",\"Peso: -0.0222\",\"Peso: 0.0213\",\"Peso: -0.0275\",\"Peso: -0.1131\",\"Peso: -0.0770\",\"Peso: 0.0153\",\"Peso: 0.1069\",\"Peso: -0.0499\",\"Peso: -0.0559\",\"Peso: -0.0451\",\"Peso: 0.1200\",\"Peso: -0.1062\",\"Peso: 0.0463\",\"Peso: 0.1812\",\"Peso: 0.0185\",\"Peso: -0.0723\",\"Peso: 0.0336\",\"Peso: 0.0022\",\"Peso: 0.1172\",\"Peso: 0.0686\",\"Peso: -0.0405\",\"Peso: 0.0964\",\"Peso: 0.1750\",\"Peso: 0.0028\",\"Peso: 0.0095\",\"Peso: -0.0171\",\"Peso: 0.0377\",\"Peso: -0.0716\",\"Peso: 0.0713\",\"Peso: 0.0247\",\"Peso: 0.0949\",\"Peso: 0.0460\",\"Peso: 0.1693\",\"Peso: 0.0069\",\"Peso: 0.1066\",\"Peso: 0.0591\",\"Peso: -0.0444\",\"Peso: -0.0239\",\"Peso: -0.0998\",\"Peso: -0.0490\",\"Peso: 0.0421\",\"Peso: 0.1563\",\"Peso: 0.0418\",\"Peso: 0.1624\",\"Peso: 0.0832\",\"Peso: 0.0272\",\"Peso: -0.0153\",\"Peso: 0.0644\",\"Peso: 0.0945\",\"Peso: 0.0142\",\"Peso: -0.0350\",\"Peso: 0.1426\",\"Peso: 0.1122\",\"Peso: 0.0296\",\"Peso: 0.1269\",\"Peso: 0.1385\",\"Peso: -0.1092\",\"Peso: -0.1220\",\"Peso: -0.1194\",\"Peso: -0.0870\",\"Peso: 0.0927\",\"Peso: -0.1074\",\"Peso: -0.0122\",\"Peso: 0.1020\",\"Peso: 0.0578\",\"Peso: 0.0358\",\"Peso: 0.0751\",\"Peso: 0.0039\",\"Peso: -0.0122\",\"Peso: -0.1067\",\"Peso: 0.0433\",\"Peso: -0.0294\",\"Peso: -0.0868\",\"Peso: 0.0519\",\"Peso: 0.0257\",\"Peso: 0.0019\",\"Peso: 0.1067\",\"Peso: -0.0338\",\"Peso: -0.0274\",\"Peso: 0.1360\",\"Peso: -0.0202\",\"Peso: 0.0867\",\"Peso: 0.0664\",\"Peso: -0.0260\",\"Peso: 0.0127\",\"Peso: -0.0087\",\"Peso: 0.0784\",\"Peso: 0.0473\",\"Peso: -0.0086\",\"Peso: 0.1181\",\"Peso: 0.0377\",\"Peso: 0.0398\",\"Peso: 0.1452\",\"Peso: -0.0123\",\"Peso: -0.0380\",\"Peso: 0.0139\",\"Peso: -0.0377\",\"Peso: 0.0640\",\"Peso: -0.0923\",\"Peso: -0.0642\",\"Peso: 0.0512\",\"Peso: 0.1187\",\"Peso: 0.0517\",\"Peso: 0.1490\",\"Peso: 0.0150\",\"Peso: 0.0371\",\"Peso: 0.0851\",\"Peso: 0.0822\",\"Peso: 0.1288\",\"Peso: 0.1372\",\"Peso: 0.0629\",\"Peso: 0.0125\",\"Peso: -0.0210\",\"Peso: -0.0512\",\"Peso: 0.0135\",\"Peso: 0.0220\",\"Peso: 0.0030\",\"Peso: 0.1956\",\"Peso: 0.0480\",\"Peso: 0.0720\",\"Peso: 0.0234\",\"Peso: 0.0066\",\"Peso: -0.0093\",\"Peso: 0.0118\",\"Peso: -0.0952\",\"Peso: -0.1386\",\"Peso: 0.0965\",\"Peso: 0.1363\",\"Peso: -0.0355\",\"Peso: -0.0342\",\"Peso: 0.0018\",\"Peso: -0.0053\",\"Peso: 0.0109\",\"Peso: -0.0181\",\"Peso: -0.0286\",\"Peso: -0.0019\",\"Peso: 0.0134\",\"Peso: -0.0048\",\"Peso: 0.0353\",\"Peso: -0.0014\",\"Peso: -0.0190\",\"Peso: -0.0067\",\"Peso: 0.0191\",\"Peso: 0.0128\",\"Peso: 0.0249\",\"Peso: -0.0342\",\"Peso: 0.0088\",\"Peso: 0.0238\",\"Peso: 0.0130\",\"Peso: 0.0392\",\"Peso: -0.0282\",\"Peso: -0.0095\",\"Peso: -0.0302\",\"Peso: 0.0138\",\"Peso: -0.0104\",\"Peso: -0.0313\",\"Peso: 0.0371\",\"Peso: 0.0256\",\"Peso: -0.0253\",\"Peso: -0.0265\",\"Peso: 0.0178\",\"Peso: 0.0245\",\"Peso: -0.0020\",\"Peso: 0.0223\",\"Peso: -0.0232\",\"Peso: -0.0097\",\"Peso: 0.0335\",\"Peso: -0.0350\",\"Peso: -0.0118\",\"Peso: 0.0207\",\"Peso: 0.0146\",\"Peso: -0.0123\",\"Peso: -0.0227\",\"Peso: 0.0178\",\"Peso: -0.0210\",\"Peso: 0.0348\",\"Peso: 0.0036\",\"Peso: -0.0016\",\"Peso: -0.0046\",\"Peso: 0.0267\",\"Peso: -0.0125\",\"Peso: -0.0076\",\"Peso: 0.0271\",\"Peso: -0.0256\",\"Peso: -0.0108\",\"Peso: -0.0033\",\"Peso: -0.0020\",\"Peso: 0.0089\",\"Peso: -0.0133\",\"Peso: -0.0085\",\"Peso: -0.0278\",\"Peso: 0.0129\",\"Peso: 0.0088\",\"Peso: 0.0234\",\"Peso: -0.0345\",\"Peso: 0.0217\",\"Peso: 0.0052\",\"Peso: -0.0050\",\"Peso: 0.0269\",\"Peso: -0.0205\",\"Peso: -0.0207\",\"Peso: -0.0131\",\"Peso: -0.0254\",\"Peso: 0.0121\",\"Peso: 0.0393\",\"Peso: 0.0085\",\"Peso: 0.0028\",\"Peso: -0.0227\",\"Peso: -0.0076\",\"Peso: 0.0157\",\"Peso: -0.0013\",\"Peso: -0.0324\",\"Peso: -0.0314\",\"Peso: -0.0278\",\"Peso: -0.0357\",\"Peso: -0.0087\",\"Peso: -0.0356\",\"Peso: -0.0225\",\"Peso: -0.0232\",\"Peso: 0.0159\",\"Peso: -0.0216\",\"Peso: 0.0058\",\"Peso: -0.0261\",\"Peso: 0.0085\",\"Peso: 0.0235\",\"Peso: 0.0264\",\"Peso: 0.0185\",\"Peso: -0.0031\",\"Peso: 0.0229\",\"Peso: 0.0170\",\"Peso: 0.0260\",\"Peso: -0.0120\",\"Peso: -0.0220\",\"Peso: -0.0284\",\"Peso: -0.0296\",\"Peso: 0.0092\",\"Peso: 0.0152\",\"Peso: 0.0217\",\"Peso: 0.0272\",\"Peso: -0.0084\",\"Peso: 0.0276\",\"Peso: -0.0070\",\"Peso: -0.0259\",\"Peso: -0.0355\",\"Peso: -0.0212\",\"Peso: -0.0228\",\"Peso: 0.0256\",\"Peso: -0.0333\",\"Peso: -0.0105\",\"Peso: 0.0151\",\"Peso: -0.0159\",\"Peso: 0.0197\",\"Peso: -0.0308\",\"Peso: -0.0150\",\"Peso: 0.0238\",\"Peso: -0.0070\",\"Peso: -0.0233\",\"Peso: 0.0189\",\"Peso: 0.0345\",\"Peso: -0.0039\",\"Peso: 0.0307\",\"Peso: -0.0209\",\"Peso: -0.0032\",\"Peso: 0.0118\",\"Peso: 0.0319\",\"Peso: 0.0028\",\"Peso: 0.0065\",\"Peso: 0.0074\",\"Peso: 0.0242\",\"Peso: -0.0145\",\"Peso: 0.0030\",\"Peso: 0.0313\",\"Peso: 0.0298\",\"Peso: 0.0119\",\"Peso: 0.0218\",\"Peso: -0.0371\",\"Peso: 0.0199\",\"Peso: 0.0232\",\"Peso: -0.0295\",\"Peso: 0.0262\",\"Peso: 0.0083\",\"Peso: -0.0135\",\"Peso: 0.0080\",\"Peso: 0.0184\",\"Peso: 0.0125\",\"Peso: 0.0318\",\"Peso: -0.0100\",\"Peso: 0.0189\",\"Peso: -0.0260\",\"Peso: 0.0162\",\"Peso: -0.0310\",\"Peso: -0.0186\",\"Peso: -0.0356\",\"Peso: 0.0031\",\"Peso: 0.0249\",\"Peso: 0.0075\",\"Peso: 0.0126\",\"Peso: -0.0237\",\"Peso: -0.0290\",\"Peso: -0.0114\",\"Peso: 0.0317\",\"Peso: 0.0350\",\"Peso: -0.0139\",\"Peso: 0.0333\",\"Peso: 0.0265\",\"Peso: 0.0109\",\"Peso: -0.0160\",\"Peso: -0.0296\",\"Peso: -0.0377\",\"Peso: -0.0173\",\"Peso: -0.0129\",\"Peso: 0.0359\",\"Peso: 0.0141\",\"Peso: 0.0373\",\"Peso: -0.0045\",\"Peso: -0.0268\",\"Peso: -0.0012\",\"Peso: 0.0294\",\"Peso: 0.0292\",\"Peso: 0.0191\",\"Peso: -0.0013\",\"Peso: -0.0305\",\"Peso: 0.0020\",\"Peso: 0.0378\",\"Peso: 0.0476\",\"Peso: -0.1003\",\"Peso: -0.1109\",\"Peso: -0.0339\",\"Peso: -0.0273\",\"Peso: 0.0884\",\"Peso: -0.1265\",\"Peso: 0.0855\",\"Peso: -0.1083\",\"Peso: -0.0389\",\"Peso: -0.0838\"],\"x\":[0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null],\"y\":[0,-4.6875,null,0,-4.0625,null,0,-3.4375,null,0,-2.8125,null,0,-2.1875,null,0,-1.5625,null,0,-0.9375,null,0,-0.3125,null,0,0.3125,null,0,0.9375,null,0,1.5625,null,0,2.1875,null,0,2.8125,null,0,3.4375,null,0,4.0625,null,0,4.6875,null,-4.6875,-4.6875,null,-4.6875,-4.0625,null,-4.6875,-3.4375,null,-4.6875,-2.8125,null,-4.6875,-2.1875,null,-4.6875,-1.5625,null,-4.6875,-0.9375,null,-4.6875,-0.3125,null,-4.6875,0.3125,null,-4.0625,-4.6875,null,-4.0625,-4.0625,null,-4.0625,-3.4375,null,-4.0625,-2.8125,null,-4.0625,-2.1875,null,-4.0625,-1.5625,null,-4.0625,-0.9375,null,-4.0625,-0.3125,null,-4.0625,0.3125,null,-3.4375,-4.6875,null,-3.4375,-4.0625,null,-3.4375,-3.4375,null,-3.4375,-2.8125,null,-3.4375,-2.1875,null,-3.4375,-1.5625,null,-3.4375,-0.9375,null,-3.4375,-0.3125,null,-3.4375,0.3125,null,-2.8125,-4.6875,null,-2.8125,-4.0625,null,-2.8125,-3.4375,null,-2.8125,-2.8125,null,-2.8125,-2.1875,null,-2.8125,-1.5625,null,-2.8125,-0.9375,null,-2.8125,-0.3125,null,-2.8125,0.3125,null,-2.1875,-4.6875,null,-2.1875,-4.0625,null,-2.1875,-3.4375,null,-2.1875,-2.8125,null,-2.1875,-2.1875,null,-2.1875,-1.5625,null,-2.1875,-0.9375,null,-2.1875,-0.3125,null,-2.1875,0.3125,null,-1.5625,-4.6875,null,-1.5625,-4.0625,null,-1.5625,-3.4375,null,-1.5625,-2.8125,null,-1.5625,-2.1875,null,-1.5625,-1.5625,null,-1.5625,-0.9375,null,-1.5625,-0.3125,null,-1.5625,0.3125,null,-0.9375,-4.6875,null,-0.9375,-4.0625,null,-0.9375,-3.4375,null,-0.9375,-2.8125,null,-0.9375,-2.1875,null,-0.9375,-1.5625,null,-0.9375,-0.9375,null,-0.9375,-0.3125,null,-0.9375,0.3125,null,-0.3125,-4.6875,null,-0.3125,-4.0625,null,-0.3125,-3.4375,null,-0.3125,-2.8125,null,-0.3125,-2.1875,null,-0.3125,-1.5625,null,-0.3125,-0.9375,null,-0.3125,-0.3125,null,-0.3125,0.3125,null,0.3125,-4.6875,null,0.3125,-4.0625,null,0.3125,-3.4375,null,0.3125,-2.8125,null,0.3125,-2.1875,null,0.3125,-1.5625,null,0.3125,-0.9375,null,0.3125,-0.3125,null,0.3125,0.3125,null,0.9375,-4.6875,null,0.9375,-4.0625,null,0.9375,-3.4375,null,0.9375,-2.8125,null,0.9375,-2.1875,null,0.9375,-1.5625,null,0.9375,-0.9375,null,0.9375,-0.3125,null,0.9375,0.3125,null,1.5625,-4.6875,null,1.5625,-4.0625,null,1.5625,-3.4375,null,1.5625,-2.8125,null,1.5625,-2.1875,null,1.5625,-0.9375,null,1.5625,-0.3125,null,1.5625,0.3125,null,2.1875,-4.6875,null,2.1875,-4.0625,null,2.1875,-3.4375,null,2.1875,-2.8125,null,2.1875,-2.1875,null,2.1875,-1.5625,null,2.1875,-0.9375,null,2.1875,-0.3125,null,2.1875,0.3125,null,2.8125,-4.6875,null,2.8125,-4.0625,null,2.8125,-3.4375,null,2.8125,-2.8125,null,2.8125,-2.1875,null,2.8125,-1.5625,null,2.8125,-0.9375,null,2.8125,-0.3125,null,2.8125,0.3125,null,3.4375,-4.6875,null,3.4375,-4.0625,null,3.4375,-3.4375,null,3.4375,-2.8125,null,3.4375,-2.1875,null,3.4375,-1.5625,null,3.4375,-0.9375,null,3.4375,-0.3125,null,3.4375,0.3125,null,4.0625,-4.6875,null,4.0625,-4.0625,null,4.0625,-3.4375,null,4.0625,-2.8125,null,4.0625,-2.1875,null,4.0625,-1.5625,null,4.0625,-0.9375,null,4.0625,-0.3125,null,4.0625,0.3125,null,4.6875,-4.6875,null,4.6875,-4.0625,null,4.6875,-3.4375,null,4.6875,-2.8125,null,4.6875,-2.1875,null,4.6875,-1.5625,null,4.6875,-0.9375,null,4.6875,-0.3125,null,4.6875,0.3125,null,-4.6875,-4.916666666666666,null,-4.6875,-4.75,null,-4.6875,-4.583333333333333,null,-4.6875,-4.416666666666666,null,-4.6875,-4.25,null,-4.6875,-4.083333333333333,null,-4.6875,-3.9166666666666665,null,-4.6875,-3.75,null,-4.6875,-3.583333333333333,null,-4.6875,-3.4166666666666665,null,-4.6875,-3.25,null,-4.6875,-3.083333333333333,null,-4.6875,-2.9166666666666665,null,-4.6875,-2.75,null,-4.6875,-2.583333333333333,null,-4.6875,-2.4166666666666665,null,-4.6875,-2.25,null,-4.6875,-2.083333333333333,null,-4.6875,-1.9166666666666665,null,-4.6875,-1.75,null,-4.0625,-4.916666666666666,null,-4.0625,-4.75,null,-4.0625,-4.583333333333333,null,-4.0625,-4.416666666666666,null,-4.0625,-4.25,null,-4.0625,-4.083333333333333,null,-4.0625,-3.9166666666666665,null,-4.0625,-3.75,null,-4.0625,-3.4166666666666665,null,-4.0625,-3.25,null,-4.0625,-3.083333333333333,null,-4.0625,-2.9166666666666665,null,-4.0625,-2.75,null,-4.0625,-2.583333333333333,null,-4.0625,-2.4166666666666665,null,-4.0625,-2.25,null,-4.0625,-2.083333333333333,null,-4.0625,-1.9166666666666665,null,-4.0625,-1.75,null,-3.4375,-4.916666666666666,null,-3.4375,-4.75,null,-3.4375,-4.583333333333333,null,-3.4375,-4.416666666666666,null,-3.4375,-4.25,null,-3.4375,-4.083333333333333,null,-3.4375,-3.9166666666666665,null,-3.4375,-3.75,null,-3.4375,-3.583333333333333,null,-3.4375,-3.4166666666666665,null,-3.4375,-3.25,null,-3.4375,-3.083333333333333,null,-3.4375,-2.9166666666666665,null,-3.4375,-2.75,null,-3.4375,-2.583333333333333,null,-3.4375,-2.4166666666666665,null,-3.4375,-2.25,null,-3.4375,-1.9166666666666665,null,-3.4375,-1.75,null,-2.8125,-4.916666666666666,null,-2.8125,-4.75,null,-2.8125,-4.583333333333333,null,-2.8125,-4.416666666666666,null,-2.8125,-4.25,null,-2.8125,-4.083333333333333,null,-2.8125,-3.9166666666666665,null,-2.8125,-3.75,null,-2.8125,-3.583333333333333,null,-2.8125,-3.4166666666666665,null,-2.8125,-3.25,null,-2.8125,-3.083333333333333,null,-2.8125,-2.9166666666666665,null,-2.8125,-2.75,null,-2.8125,-2.583333333333333,null,-2.8125,-2.4166666666666665,null,-2.8125,-2.25,null,-2.8125,-2.083333333333333,null,-2.8125,-1.9166666666666665,null,-2.8125,-1.75,null,-2.1875,-4.916666666666666,null,-2.1875,-4.75,null,-2.1875,-4.583333333333333,null,-2.1875,-4.416666666666666,null,-2.1875,-4.25,null,-2.1875,-4.083333333333333,null,-2.1875,-3.9166666666666665,null,-2.1875,-3.75,null,-2.1875,-3.583333333333333,null,-2.1875,-3.4166666666666665,null,-2.1875,-3.25,null,-2.1875,-3.083333333333333,null,-2.1875,-2.9166666666666665,null,-2.1875,-2.75,null,-2.1875,-2.583333333333333,null,-2.1875,-2.4166666666666665,null,-2.1875,-2.25,null,-2.1875,-2.083333333333333,null,-2.1875,-1.9166666666666665,null,-2.1875,-1.75,null,-1.5625,-4.916666666666666,null,-1.5625,-4.75,null,-1.5625,-4.583333333333333,null,-1.5625,-4.416666666666666,null,-1.5625,-4.25,null,-1.5625,-4.083333333333333,null,-1.5625,-3.9166666666666665,null,-1.5625,-3.75,null,-1.5625,-3.583333333333333,null,-1.5625,-3.4166666666666665,null,-1.5625,-3.25,null,-1.5625,-3.083333333333333,null,-1.5625,-2.9166666666666665,null,-1.5625,-2.75,null,-1.5625,-2.583333333333333,null,-1.5625,-2.4166666666666665,null,-1.5625,-2.25,null,-1.5625,-2.083333333333333,null,-1.5625,-1.9166666666666665,null,-1.5625,-1.75,null,-0.9375,-4.916666666666666,null,-0.9375,-4.75,null,-0.9375,-4.583333333333333,null,-0.9375,-4.416666666666666,null,-0.9375,-4.25,null,-0.9375,-4.083333333333333,null,-0.9375,-3.9166666666666665,null,-0.9375,-3.75,null,-0.9375,-3.583333333333333,null,-0.9375,-3.4166666666666665,null,-0.9375,-3.25,null,-0.9375,-3.083333333333333,null,-0.9375,-2.9166666666666665,null,-0.9375,-2.75,null,-0.9375,-2.583333333333333,null,-0.9375,-2.4166666666666665,null,-0.9375,-2.25,null,-0.9375,-2.083333333333333,null,-0.9375,-1.9166666666666665,null,-0.9375,-1.75,null,-0.3125,-4.916666666666666,null,-0.3125,-4.75,null,-0.3125,-4.583333333333333,null,-0.3125,-4.416666666666666,null,-0.3125,-4.25,null,-0.3125,-4.083333333333333,null,-0.3125,-3.9166666666666665,null,-0.3125,-3.75,null,-0.3125,-3.583333333333333,null,-0.3125,-3.4166666666666665,null,-0.3125,-3.25,null,-0.3125,-3.083333333333333,null,-0.3125,-2.9166666666666665,null,-0.3125,-2.75,null,-0.3125,-2.583333333333333,null,-0.3125,-2.4166666666666665,null,-0.3125,-2.25,null,-0.3125,-2.083333333333333,null,-0.3125,-1.9166666666666665,null,-0.3125,-1.75,null,0.3125,-4.916666666666666,null,0.3125,-4.75,null,0.3125,-4.583333333333333,null,0.3125,-4.416666666666666,null,0.3125,-4.25,null,0.3125,-4.083333333333333,null,0.3125,-3.9166666666666665,null,0.3125,-3.75,null,0.3125,-3.583333333333333,null,0.3125,-3.4166666666666665,null,0.3125,-3.25,null,0.3125,-3.083333333333333,null,0.3125,-2.9166666666666665,null,0.3125,-2.75,null,0.3125,-2.583333333333333,null,0.3125,-2.4166666666666665,null,0.3125,-2.25,null,0.3125,-2.083333333333333,null,0.3125,-1.9166666666666665,null,0.3125,-1.75,null,0.9375,-4.916666666666666,null,0.9375,-4.75,null,0.9375,-4.583333333333333,null,0.9375,-4.416666666666666,null,0.9375,-4.25,null,0.9375,-4.083333333333333,null,0.9375,-3.9166666666666665,null,0.9375,-3.75,null,0.9375,-3.583333333333333,null,0.9375,-3.4166666666666665,null,0.9375,-3.25,null,0.9375,-3.083333333333333,null,0.9375,-2.9166666666666665,null,0.9375,-2.75,null,0.9375,-2.583333333333333,null,0.9375,-2.4166666666666665,null,0.9375,-2.25,null,0.9375,-2.083333333333333,null,0.9375,-1.9166666666666665,null,0.9375,-1.75,null,1.5625,-4.916666666666666,null,1.5625,-4.75,null,1.5625,-4.583333333333333,null,1.5625,-4.416666666666666,null,1.5625,-4.25,null,1.5625,-4.083333333333333,null,1.5625,-3.9166666666666665,null,1.5625,-3.75,null,1.5625,-3.583333333333333,null,1.5625,-3.4166666666666665,null,1.5625,-3.25,null,1.5625,-3.083333333333333,null,1.5625,-2.9166666666666665,null,1.5625,-2.75,null,1.5625,-2.583333333333333,null,1.5625,-2.4166666666666665,null,1.5625,-2.25,null,1.5625,-2.083333333333333,null,1.5625,-1.9166666666666665,null,1.5625,-1.75,null,2.1875,-4.916666666666666,null,2.1875,-4.75,null,2.1875,-4.583333333333333,null,2.1875,-4.416666666666666,null,2.1875,-4.25,null,2.1875,-4.083333333333333,null,2.1875,-3.9166666666666665,null,2.1875,-3.75,null,2.1875,-3.583333333333333,null,2.1875,-3.4166666666666665,null,2.1875,-3.25,null,2.1875,-3.083333333333333,null,2.1875,-2.9166666666666665,null,2.1875,-2.75,null,2.1875,-2.583333333333333,null,2.1875,-2.4166666666666665,null,2.1875,-2.25,null,2.1875,-2.083333333333333,null,2.1875,-1.9166666666666665,null,2.1875,-1.75,null,2.8125,-4.916666666666666,null,2.8125,-4.75,null,2.8125,-4.583333333333333,null,2.8125,-4.416666666666666,null,2.8125,-4.25,null,2.8125,-4.083333333333333,null,2.8125,-3.9166666666666665,null,2.8125,-3.75,null,2.8125,-3.583333333333333,null,2.8125,-3.4166666666666665,null,2.8125,-3.25,null,2.8125,-3.083333333333333,null,2.8125,-2.9166666666666665,null,2.8125,-2.75,null,2.8125,-2.583333333333333,null,2.8125,-2.4166666666666665,null,2.8125,-2.25,null,2.8125,-2.083333333333333,null,2.8125,-1.9166666666666665,null,2.8125,-1.75,null,3.4375,-4.916666666666666,null,3.4375,-4.75,null,3.4375,-4.583333333333333,null,3.4375,-4.416666666666666,null,3.4375,-4.25,null,3.4375,-4.083333333333333,null,3.4375,-3.9166666666666665,null,3.4375,-3.75,null,3.4375,-3.583333333333333,null,3.4375,-3.4166666666666665,null,3.4375,-3.25,null,3.4375,-3.083333333333333,null,3.4375,-2.9166666666666665,null,3.4375,-2.75,null,3.4375,-2.583333333333333,null,3.4375,-2.4166666666666665,null,3.4375,-2.25,null,3.4375,-2.083333333333333,null,3.4375,-1.9166666666666665,null,3.4375,-1.75,null,4.0625,-4.916666666666666,null,4.0625,-4.75,null,4.0625,-4.583333333333333,null,4.0625,-4.416666666666666,null,4.0625,-4.25,null,4.0625,-4.083333333333333,null,4.0625,-3.9166666666666665,null,4.0625,-3.75,null,4.0625,-3.583333333333333,null,4.0625,-3.4166666666666665,null,4.0625,-3.25,null,4.0625,-3.083333333333333,null,4.0625,-2.9166666666666665,null,4.0625,-2.75,null,4.0625,-2.583333333333333,null,4.0625,-2.4166666666666665,null,4.0625,-2.25,null,4.0625,-2.083333333333333,null,4.0625,-1.9166666666666665,null,4.0625,-1.75,null,4.6875,-4.916666666666666,null,4.6875,-4.75,null,4.6875,-4.583333333333333,null,4.6875,-4.416666666666666,null,4.6875,-4.25,null,4.6875,-4.083333333333333,null,4.6875,-3.9166666666666665,null,4.6875,-3.75,null,4.6875,-3.583333333333333,null,4.6875,-3.4166666666666665,null,4.6875,-3.25,null,4.6875,-3.083333333333333,null,4.6875,-2.9166666666666665,null,4.6875,-2.75,null,4.6875,-2.583333333333333,null,4.6875,-2.4166666666666665,null,4.6875,-2.25,null,4.6875,-2.083333333333333,null,4.6875,-1.9166666666666665,null,4.6875,-1.75,null,-4.916666666666666,-4.5,null,-4.916666666666666,-3.5,null,-4.916666666666666,-2.5,null,-4.916666666666666,-1.5,null,-4.916666666666666,-0.5,null,-4.916666666666666,0.5,null,-4.916666666666666,1.5,null,-4.916666666666666,2.5,null,-4.916666666666666,3.5,null,-4.916666666666666,4.5,null,-4.75,-4.5,null,-4.75,-3.5,null,-4.75,-2.5,null,-4.75,-1.5,null,-4.75,-0.5,null,-4.75,0.5,null,-4.75,1.5,null,-4.75,2.5,null,-4.75,3.5,null,-4.75,4.5,null,-4.583333333333333,-4.5,null,-4.583333333333333,-3.5,null,-4.583333333333333,-2.5,null,-4.583333333333333,-1.5,null,-4.583333333333333,-0.5,null,-4.583333333333333,0.5,null,-4.583333333333333,1.5,null,-4.583333333333333,2.5,null,-4.583333333333333,3.5,null,-4.583333333333333,4.5,null,-4.416666666666666,-4.5,null,-4.416666666666666,-3.5,null,-4.416666666666666,-2.5,null,-4.416666666666666,-1.5,null,-4.416666666666666,-0.5,null,-4.416666666666666,0.5,null,-4.416666666666666,1.5,null,-4.416666666666666,2.5,null,-4.416666666666666,3.5,null,-4.416666666666666,4.5,null,-4.25,-4.5,null,-4.25,-3.5,null,-4.25,-2.5,null,-4.25,-1.5,null,-4.25,-0.5,null,-4.25,0.5,null,-4.25,1.5,null,-4.25,2.5,null,-4.25,3.5,null,-4.25,4.5,null,-4.083333333333333,-4.5,null,-4.083333333333333,-3.5,null,-4.083333333333333,-2.5,null,-4.083333333333333,-1.5,null,-4.083333333333333,-0.5,null,-4.083333333333333,1.5,null,-4.083333333333333,2.5,null,-4.083333333333333,3.5,null,-4.083333333333333,4.5,null,-3.9166666666666665,-4.5,null,-3.9166666666666665,-3.5,null,-3.9166666666666665,-2.5,null,-3.9166666666666665,-1.5,null,-3.9166666666666665,-0.5,null,-3.9166666666666665,0.5,null,-3.9166666666666665,1.5,null,-3.9166666666666665,2.5,null,-3.9166666666666665,3.5,null,-3.9166666666666665,4.5,null,-3.75,-4.5,null,-3.75,-3.5,null,-3.75,-2.5,null,-3.75,-1.5,null,-3.75,-0.5,null,-3.75,0.5,null,-3.75,1.5,null,-3.75,2.5,null,-3.75,3.5,null,-3.75,4.5,null,-3.583333333333333,-4.5,null,-3.583333333333333,-2.5,null,-3.583333333333333,-1.5,null,-3.583333333333333,-0.5,null,-3.583333333333333,0.5,null,-3.583333333333333,1.5,null,-3.583333333333333,2.5,null,-3.583333333333333,3.5,null,-3.583333333333333,4.5,null,-3.4166666666666665,-4.5,null,-3.4166666666666665,-3.5,null,-3.4166666666666665,-2.5,null,-3.4166666666666665,-1.5,null,-3.4166666666666665,-0.5,null,-3.4166666666666665,0.5,null,-3.4166666666666665,1.5,null,-3.4166666666666665,2.5,null,-3.4166666666666665,3.5,null,-3.4166666666666665,4.5,null,-3.25,-4.5,null,-3.25,-3.5,null,-3.25,-2.5,null,-3.25,-1.5,null,-3.25,-0.5,null,-3.25,0.5,null,-3.25,1.5,null,-3.25,2.5,null,-3.25,3.5,null,-3.25,4.5,null,-3.083333333333333,-4.5,null,-3.083333333333333,-3.5,null,-3.083333333333333,-1.5,null,-3.083333333333333,-0.5,null,-3.083333333333333,0.5,null,-3.083333333333333,1.5,null,-3.083333333333333,2.5,null,-3.083333333333333,3.5,null,-3.083333333333333,4.5,null,-2.9166666666666665,-4.5,null,-2.9166666666666665,-3.5,null,-2.9166666666666665,-2.5,null,-2.9166666666666665,-1.5,null,-2.9166666666666665,-0.5,null,-2.9166666666666665,0.5,null,-2.9166666666666665,1.5,null,-2.9166666666666665,2.5,null,-2.9166666666666665,3.5,null,-2.9166666666666665,4.5,null,-2.75,-4.5,null,-2.75,-3.5,null,-2.75,-2.5,null,-2.75,-1.5,null,-2.75,-0.5,null,-2.75,0.5,null,-2.75,1.5,null,-2.75,2.5,null,-2.75,3.5,null,-2.75,4.5,null,-2.583333333333333,-4.5,null,-2.583333333333333,-3.5,null,-2.583333333333333,-2.5,null,-2.583333333333333,-1.5,null,-2.583333333333333,-0.5,null,-2.583333333333333,0.5,null,-2.583333333333333,1.5,null,-2.583333333333333,2.5,null,-2.583333333333333,3.5,null,-2.583333333333333,4.5,null,-2.4166666666666665,-4.5,null,-2.4166666666666665,-3.5,null,-2.4166666666666665,-2.5,null,-2.4166666666666665,-1.5,null,-2.4166666666666665,-0.5,null,-2.4166666666666665,0.5,null,-2.4166666666666665,1.5,null,-2.4166666666666665,2.5,null,-2.4166666666666665,3.5,null,-2.4166666666666665,4.5,null,-2.25,-4.5,null,-2.25,-3.5,null,-2.25,-2.5,null,-2.25,-1.5,null,-2.25,-0.5,null,-2.25,0.5,null,-2.25,1.5,null,-2.25,2.5,null,-2.25,3.5,null,-2.25,4.5,null,-2.083333333333333,-4.5,null,-2.083333333333333,-3.5,null,-2.083333333333333,-2.5,null,-2.083333333333333,-1.5,null,-2.083333333333333,-0.5,null,-2.083333333333333,0.5,null,-2.083333333333333,1.5,null,-2.083333333333333,2.5,null,-2.083333333333333,3.5,null,-2.083333333333333,4.5,null,-1.9166666666666665,-4.5,null,-1.9166666666666665,-3.5,null,-1.9166666666666665,-2.5,null,-1.9166666666666665,-1.5,null,-1.9166666666666665,-0.5,null,-1.9166666666666665,0.5,null,-1.9166666666666665,1.5,null,-1.9166666666666665,2.5,null,-1.9166666666666665,3.5,null,-1.9166666666666665,4.5,null,-1.75,-4.5,null,-1.75,-3.5,null,-1.75,-2.5,null,-1.75,-1.5,null,-1.75,-0.5,null,-1.75,0.5,null,-1.75,1.5,null,-1.75,2.5,null,-1.75,3.5,null,-1.75,4.5,null,-4.5,0,null,-3.5,0,null,-2.5,0,null,-1.5,0,null,-0.5,0,null,0.5,0,null,1.5,0,null,2.5,0,null,3.5,0,null,4.5,0,null],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":[\"rgba(200, 200, 200, 0.8)\",\"rgba(0, 0, 135, 0.8)\",\"rgba(0, 0, 141, 0.8)\",\"rgba(0, 0, 118, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 138, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 145, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 130, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 134, 0.8)\",\"rgba(0, 0, 196, 0.8)\",\"rgba(0, 0, 145, 0.8)\",\"rgba(0, 0, 192, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(140, 0, 0, 0.8)\",\"rgba(0, 0, 190, 0.8)\",\"rgba(0, 0, 139, 0.8)\",\"rgba(0, 0, 160, 0.8)\",\"rgba(0, 0, 140, 0.8)\",\"rgba(0, 0, 162, 0.8)\",\"rgba(0, 0, 169, 0.8)\",\"rgba(0, 0, 169, 0.8)\",\"rgba(0, 0, 161, 0.8)\",\"rgba(0, 0, 169, 0.8)\",\"rgba(0, 0, 152, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(248, 0, 0, 0.8)\",\"rgba(233, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(133, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(194, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(237, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(118, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 162, 0.8)\",\"rgba(152, 0, 0, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(237, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(200, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(130, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(115, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 225, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(253, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(142, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 200, 0, 0.8)\"],\"line\":{\"color\":\"white\",\"width\":2},\"showscale\":false,\"size\":[25,13.481226786971092,13.996795862913132,11.836531087756157,10.408129170536995,10.948276668787003,13.716790452599525,10.51316637545824,14.403729438781738,10.952176935970783,11.12216416746378,10.857959315180779,12.975018844008446,10.01467738125939,10.90503565967083,10.059404824860394,10.29437112621963,13.350001946091652,19.3698188662529,14.422628730535507,18.9272677898407,10.417863801121712,13.874528259038925,18.76609742641449,13.825993239879608,15.897938013076782,13.896732032299042,16.017732620239258,16.7177252471447,16.693968027830124,15.958532691001892,16.68259233236313,15.06351351737976,91.24603509902954,31.75684928894043,24.32791978120804,22.89430409669876,32.03659474849701,13.23577918112278,38.18967163562775,31.817013025283813,71.26364946365356,42.319971323013306,19.163663387298584,28.007807731628418,39.24209535121918,77.7083969116211,23.309484124183655,112.0319390296936,11.78077332675457,52.76727557182312,16.089935898780823,15.048837810754776,10.56079626083374,23.29754114151001,26.4222252368927,97.2475528717041,106.12813234329224,26.260611414909363,101.63453817367554,90.08702754974365,29.935988783836365,70.68957805633545,27.413553595542908,47.36618638038635,71.91424608230591,97.71352767944336,112.96403169631958,68.71238946914673,94.86903429031372,19.706633687019348,38.82133901119232,36.71002924442291,12.918466255068779,31.476136445999146,11.000128574669361,11.504696682095528,90.6305718421936,99.8825478553772,35.78999578952789,41.27722144126892,33.12110126018524,91.74829483032227,22.16468870639801,29.28146183490753,24.897451102733612,99.96728420257568,29.243757128715515,25.272002816200256,79.33772087097168,31.26403272151947,35.250285267829895,14.119987040758133,83.94490480422974,156.9392204284668,58.49777579307556,96.68999671936035,68.22182059288025,66.2983763217926,70.6119441986084,31.116870641708374,95.87212324142456,65.24980068206787,25]},\"mode\":\"markers\",\"text\":[\"Imagen de entrada\",\"conv1 neurona 0\\u003cbr\\u003eActivación: 0.2321\",\"conv1 neurona 1\\u003cbr\\u003eActivación: 0.2665\",\"conv1 neurona 2\\u003cbr\\u003eActivación: 0.1224\",\"conv1 neurona 3\\u003cbr\\u003eActivación: 0.0272\",\"conv1 neurona 4\\u003cbr\\u003eActivación: 0.0632\",\"conv1 neurona 5\\u003cbr\\u003eActivación: 0.2478\",\"conv1 neurona 6\\u003cbr\\u003eActivación: 0.0342\",\"conv1 neurona 7\\u003cbr\\u003eActivación: 0.2936\",\"conv1 neurona 8\\u003cbr\\u003eActivación: 0.0635\",\"conv1 neurona 9\\u003cbr\\u003eActivación: 0.0748\",\"conv1 neurona 10\\u003cbr\\u003eActivación: 0.0572\",\"conv1 neurona 11\\u003cbr\\u003eActivación: 0.1983\",\"conv1 neurona 12\\u003cbr\\u003eActivación: 0.0010\",\"conv1 neurona 13\\u003cbr\\u003eActivación: 0.0603\",\"conv1 neurona 14\\u003cbr\\u003eActivación: -0.0040\",\"conv1 neurona 15\\u003cbr\\u003eActivación: 0.0196\",\"conv2 neurona 0\\u003cbr\\u003eActivación: 0.2233\",\"conv2 neurona 1\\u003cbr\\u003eActivación: 0.6247\",\"conv2 neurona 2\\u003cbr\\u003eActivación: 0.2948\",\"conv2 neurona 3\\u003cbr\\u003eActivación: 0.5952\",\"conv2 neurona 4\\u003cbr\\u003eActivación: -0.0279\",\"conv2 neurona 5\\u003cbr\\u003eActivación: -0.2583\",\"conv2 neurona 6\\u003cbr\\u003eActivación: 0.5844\",\"conv2 neurona 7\\u003cbr\\u003eActivación: 0.2551\",\"conv2 neurona 8\\u003cbr\\u003eActivación: 0.3932\",\"conv2 neurona 9\\u003cbr\\u003eActivación: 0.2598\",\"conv2 neurona 10\\u003cbr\\u003eActivación: 0.4012\",\"conv2 neurona 11\\u003cbr\\u003eActivación: 0.4478\",\"conv2 neurona 12\\u003cbr\\u003eActivación: 0.4463\",\"conv2 neurona 13\\u003cbr\\u003eActivación: 0.3972\",\"conv2 neurona 14\\u003cbr\\u003eActivación: 0.4455\",\"conv2 neurona 15\\u003cbr\\u003eActivación: 0.3376\",\"fc1 neurona 0\\u003cbr\\u003eActivación: 5.4164\",\"fc1 neurona 1\\u003cbr\\u003eActivación: -1.4505\",\"fc1 neurona 2\\u003cbr\\u003eActivación: -0.9552\",\"fc1 neurona 3\\u003cbr\\u003eActivación: -0.8596\",\"fc1 neurona 4\\u003cbr\\u003eActivación: -1.4691\",\"fc1 neurona 5\\u003cbr\\u003eActivación: -0.2157\",\"fc1 neurona 6\\u003cbr\\u003eActivación: -1.8793\",\"fc1 neurona 7\\u003cbr\\u003eActivación: -1.4545\",\"fc1 neurona 8\\u003cbr\\u003eActivación: 4.0842\",\"fc1 neurona 9\\u003cbr\\u003eActivación: -2.1547\",\"fc1 neurona 10\\u003cbr\\u003eActivación: -0.6109\",\"fc1 neurona 11\\u003cbr\\u003eActivación: -1.2005\",\"fc1 neurona 12\\u003cbr\\u003eActivación: -1.9495\",\"fc1 neurona 13\\u003cbr\\u003eActivación: 4.5139\",\"fc1 neurona 14\\u003cbr\\u003eActivación: -0.8873\",\"fc1 neurona 15\\u003cbr\\u003eActivación: 6.8021\",\"fc1 neurona 16\\u003cbr\\u003eActivación: -0.1187\",\"fc1 neurona 17\\u003cbr\\u003eActivación: 2.8512\",\"fc1 neurona 18\\u003cbr\\u003eActivación: 0.4060\",\"fc1 neurona 19\\u003cbr\\u003eActivación: -0.3366\",\"fc1 neurona 20\\u003cbr\\u003eActivación: 0.0374\",\"fc1 neurona 21\\u003cbr\\u003eActivación: -0.8865\",\"fc1 neurona 22\\u003cbr\\u003eActivación: -1.0948\",\"fc1 neurona 23\\u003cbr\\u003eActivación: 5.8165\",\"fc1 neurona 24\\u003cbr\\u003eActivación: 6.4085\",\"fc1 neurona 25\\u003cbr\\u003eActivación: 1.0840\",\"fc1 neurona 26\\u003cbr\\u003eActivación: 6.1090\",\"fc1 neurona 27\\u003cbr\\u003eActivación: 5.3391\",\"fc1 neurona 28\\u003cbr\\u003eActivación: -1.3291\",\"fc1 neurona 29\\u003cbr\\u003eActivación: 4.0460\",\"fc1 neurona 30\\u003cbr\\u003eActivación: 1.1609\",\"fc1 neurona 31\\u003cbr\\u003eActivación: -2.4911\",\"fc1 neurona 32\\u003cbr\\u003eActivación: 4.1276\",\"fc1 neurona 33\\u003cbr\\u003eActivación: 5.8476\",\"fc1 neurona 34\\u003cbr\\u003eActivación: 6.8643\",\"fc1 neurona 35\\u003cbr\\u003eActivación: 3.9142\",\"fc1 neurona 36\\u003cbr\\u003eActivación: 5.6579\",\"fc1 neurona 37\\u003cbr\\u003eActivación: -0.6471\",\"fc1 neurona 38\\u003cbr\\u003eActivación: -1.9214\",\"fc1 neurona 39\\u003cbr\\u003eActivación: -1.7807\",\"fc1 neurona 40\\u003cbr\\u003eActivación: -0.1946\",\"fc1 neurona 41\\u003cbr\\u003eActivación: -1.4317\",\"fc1 neurona 42\\u003cbr\\u003eActivación: -0.0667\",\"fc1 neurona 43\\u003cbr\\u003eActivación: -0.1003\",\"fc1 neurona 44\\u003cbr\\u003eActivación: 5.3754\",\"fc1 neurona 45\\u003cbr\\u003eActivación: 5.9922\",\"fc1 neurona 46\\u003cbr\\u003eActivación: -1.7193\",\"fc1 neurona 47\\u003cbr\\u003eActivación: -2.0851\",\"fc1 neurona 48\\u003cbr\\u003eActivación: -1.5414\",\"fc1 neurona 49\\u003cbr\\u003eActivación: 5.4499\",\"fc1 neurona 50\\u003cbr\\u003eActivación: 0.8110\",\"fc1 neurona 51\\u003cbr\\u003eActivación: -1.2854\",\"fc1 neurona 52\\u003cbr\\u003eActivación: -0.9932\",\"fc1 neurona 53\\u003cbr\\u003eActivación: 5.9978\",\"fc1 neurona 54\\u003cbr\\u003eActivación: -1.2829\",\"fc1 neurona 55\\u003cbr\\u003eActivación: -1.0181\",\"fc1 neurona 56\\u003cbr\\u003eActivación: 4.6225\",\"fc1 neurona 57\\u003cbr\\u003eActivación: 1.4176\",\"fc1 neurona 58\\u003cbr\\u003eActivación: -1.6834\",\"fc1 neurona 59\\u003cbr\\u003eActivación: -0.2747\",\"fc2 neurona 0\\u003cbr\\u003eActivación: -4.9297\",\"fc2 neurona 1\\u003cbr\\u003eActivación: 9.7959\",\"fc2 neurona 2\\u003cbr\\u003eActivación: -3.2332\",\"fc2 neurona 3\\u003cbr\\u003eActivación: -5.7793\",\"fc2 neurona 4\\u003cbr\\u003eActivación: -3.8815\",\"fc2 neurona 5\\u003cbr\\u003eActivación: -3.7532\",\"fc2 neurona 6\\u003cbr\\u003eActivación: -4.0408\",\"fc2 neurona 7\\u003cbr\\u003eActivación: -1.4078\",\"fc2 neurona 8\\u003cbr\\u003eActivación: -5.7248\",\"fc2 neurona 9\\u003cbr\\u003eActivación: 3.6833\",\"Clase predicha: 1\"],\"x\":[0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5],\"y\":[0,-4.6875,-4.0625,-3.4375,-2.8125,-2.1875,-1.5625,-0.9375,-0.3125,0.3125,0.9375,1.5625,2.1875,2.8125,3.4375,4.0625,4.6875,-4.6875,-4.0625,-3.4375,-2.8125,-2.1875,-1.5625,-0.9375,-0.3125,0.3125,0.9375,1.5625,2.1875,2.8125,3.4375,4.0625,4.6875,-4.916666666666666,-4.75,-4.583333333333333,-4.416666666666666,-4.25,-4.083333333333333,-3.9166666666666665,-3.75,-3.583333333333333,-3.4166666666666665,-3.25,-3.083333333333333,-2.9166666666666665,-2.75,-2.583333333333333,-2.4166666666666665,-2.25,-2.083333333333333,-1.9166666666666665,-1.75,-1.5833333333333333,-1.4166666666666665,-1.25,-1.0833333333333333,-0.9166666666666666,-0.75,-0.5833333333333333,-0.41666666666666663,-0.25,-0.08333333333333333,0.08333333333333333,0.25,0.41666666666666663,0.5833333333333333,0.75,0.9166666666666666,1.0833333333333333,1.25,1.4166666666666665,1.5833333333333333,1.75,1.9166666666666665,2.083333333333333,2.25,2.4166666666666665,2.583333333333333,2.75,2.9166666666666665,3.083333333333333,3.25,3.4166666666666665,3.583333333333333,3.75,3.9166666666666665,4.083333333333333,4.25,4.416666666666666,4.583333333333333,4.75,4.916666666666666,-4.5,-3.5,-2.5,-1.5,-0.5,0.5,1.5,2.5,3.5,4.5,0],\"type\":\"scatter\"}],                        {\"annotations\":[{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"INPUT\\u003cbr\\u003e(1 neuronas)\",\"x\":0,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"CONV1\\u003cbr\\u003e(16 neuronas)\",\"x\":1,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"CONV2\\u003cbr\\u003e(16 neuronas)\",\"x\":2,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"FC1\\u003cbr\\u003e(60 neuronas)\",\"x\":3,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"FC2\\u003cbr\\u003e(10 neuronas)\",\"x\":4,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"OUTPUT\\u003cbr\\u003e(1 neuronas)\",\"x\":5,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"align\":\"left\",\"showarrow\":false,\"text\":\"\\u003cb\\u003eLeyenda:\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eColor de nodo:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• \\u003cspan style='color:blue'\\u003eAzul\\u003c\\u002fspan\\u003e: Activación positiva\\u003cbr\\u003e• \\u003cspan style='color:red'\\u003eRojo\\u003c\\u002fspan\\u003e: Activación negativa\\u003cbr\\u003e• \\u003cspan style='color:gray'\\u003eGris\\u003c\\u002fspan\\u003e: Activación cercana a cero\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eTamaño de nodo:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• Mayor tamaño = Mayor magnitud de activación\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eColor de conexión:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• \\u003cspan style='color:blue'\\u003eAzul\\u003c\\u002fspan\\u003e: Peso positivo\\u003cbr\\u003e• \\u003cspan style='color:red'\\u003eRojo\\u003c\\u002fspan\\u003e: Peso negativo\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eGrosor de conexión:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• Mayor grosor = Mayor magnitud del peso\",\"x\":1.02,\"xref\":\"paper\",\"y\":0.5,\"yref\":\"paper\"},{\"align\":\"left\",\"showarrow\":false,\"text\":\"\\u003cb\\u003e¿Por qué hay activaciones negativas?\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eEn redes neuronales, las activaciones negativas\\u003cbr\\u003eocurren cuando el input a una neurona produce\\u003cbr\\u003eun valor negativo. Esto es común en capas con\\u003cbr\\u003efunciones de activación como ReLU, tanh o\\u003cbr\\u003efunciones lineales. Las activaciones negativas\\u003cbr\\u003epueden indicar inhibición o respuesta contraria\\u003cbr\\u003ea ciertas características de entrada.\",\"x\":1.02,\"xref\":\"paper\",\"y\":0.05,\"yref\":\"paper\"}],\"height\":1000,\"hovermode\":\"closest\",\"margin\":{\"b\":20,\"l\":5,\"r\":5,\"t\":40},\"showlegend\":false,\"title\":{\"font\":{\"size\":16},\"text\":\"Red Neuronal - Imagen 2\\u003cbr\\u003eEtiqueta real: 1, Predicción: 1\"},\"width\":1600,\"xaxis\":{\"showgrid\":false,\"showticklabels\":false,\"zeroline\":false},\"yaxis\":{\"showgrid\":false,\"showticklabels\":false,\"zeroline\":false},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1e7d1e81-a992-4741-be97-dcbc35c92094');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = visualizar_red_neuronal_interactiva(per_image_neuron_data, imagen_index=1)\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRs3GzR-TAjD",
        "outputId": "63398652-c529-4ebc-89a6-8c35a398dc1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"6ea0539e-efa4-42c5-aa4b-932a29a8d0ac\" class=\"plotly-graph-div\" style=\"height:1000px; width:1600px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6ea0539e-efa4-42c5-aa4b-932a29a8d0ac\")) {                    Plotly.newPlot(                        \"6ea0539e-efa4-42c5-aa4b-932a29a8d0ac\",                        [{\"hoverinfo\":\"text\",\"line\":{\"width\":0.5},\"marker\":{\"color\":[\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(100, 100, 100, 0.3)\",\"rgba(0, 0, 255, 0.49548879861831663)\",\"rgba(0, 0, 255, 0.478199428319931)\",\"rgba(0, 0, 255, 0.14466232284903527)\",\"rgba(255, 0, 0, 0.177683687210083)\",\"rgba(0, 0, 255, 0.3098765522241592)\",\"rgba(0, 0, 255, 0.3973027229309082)\",\"rgba(255, 0, 0, 0.2523659825325012)\",\"rgba(0, 0, 255, 0.47543243169784544)\",\"rgba(255, 0, 0, 0.15969223380088807)\",\"rgba(0, 0, 255, 0.1734786093235016)\",\"rgba(0, 0, 255, 0.5192788004875183)\",\"rgba(0, 0, 255, 0.5156277775764465)\",\"rgba(255, 0, 0, 0.17516806572675706)\",\"rgba(0, 0, 255, 0.3722387909889221)\",\"rgba(255, 0, 0, 0.17676905393600464)\",\"rgba(0, 0, 255, 0.35481775403022764)\",\"rgba(255, 0, 0, 0.18479847759008408)\",\"rgba(0, 0, 255, 0.11256333477795125)\",\"rgba(0, 0, 255, 0.17643774151802064)\",\"rgba(0, 0, 255, 0.32657302021980283)\",\"rgba(0, 0, 255, 0.44209170937538145)\",\"rgba(0, 0, 255, 0.41036651134490965)\",\"rgba(255, 0, 0, 0.2747374832630157)\",\"rgba(255, 0, 0, 0.2693179607391357)\",\"rgba(255, 0, 0, 0.2957087069749832)\",\"rgba(0, 0, 255, 0.16043727323412896)\",\"rgba(0, 0, 255, 0.10588635914027691)\",\"rgba(255, 0, 0, 0.2954487860202789)\",\"rgba(0, 0, 255, 0.5079583287239074)\",\"rgba(0, 0, 255, 0.2742736846208572)\",\"rgba(255, 0, 0, 0.22870168685913086)\",\"rgba(0, 0, 255, 0.37615121006965635)\",\"rgba(0, 0, 255, 0.2752258390188217)\",\"rgba(255, 0, 0, 0.10932643283158541)\",\"rgba(255, 0, 0, 0.299082151055336)\",\"rgba(0, 0, 255, 0.1544593557715416)\",\"rgba(255, 0, 0, 0.1878574475646019)\",\"rgba(255, 0, 0, 0.10299682095646859)\",\"rgba(0, 0, 255, 0.31407847106456754)\",\"rgba(0, 0, 255, 0.38076297044754026)\",\"rgba(0, 0, 255, 0.17920287549495698)\",\"rgba(0, 0, 255, 0.2862309873104095)\",\"rgba(0, 0, 255, 0.4426393866539001)\",\"rgba(255, 0, 0, 0.21012519896030427)\",\"rgba(0, 0, 255, 0.4459571897983551)\",\"rgba(255, 0, 0, 0.37126166224479673)\",\"rgba(255, 0, 0, 0.10629299692809582)\",\"rgba(0, 0, 255, 0.3153266370296478)\",\"rgba(0, 0, 255, 0.17139802724123002)\",\"rgba(0, 0, 255, 0.1725584849715233)\",\"rgba(0, 0, 255, 0.24965689182281495)\",\"rgba(0, 0, 255, 0.1810677632689476)\",\"rgba(0, 0, 255, 0.1727474093437195)\",\"rgba(255, 0, 0, 0.2172143742442131)\",\"rgba(255, 0, 0, 0.5470112025737762)\",\"rgba(255, 0, 0, 0.35271883606910703)\",\"rgba(255, 0, 0, 0.3498561441898346)\",\"rgba(0, 0, 255, 0.1630682334303856)\",\"rgba(0, 0, 255, 0.23590829074382783)\",\"rgba(0, 0, 255, 0.3403503060340881)\",\"rgba(0, 0, 255, 0.4222856521606445)\",\"rgba(255, 0, 0, 0.21231402307748795)\",\"rgba(0, 0, 255, 0.2133197695016861)\",\"rgba(0, 0, 255, 0.47085214257240293)\",\"rgba(0, 0, 255, 0.48222280144691465)\",\"rgba(0, 0, 255, 0.15390509217977524)\",\"rgba(0, 0, 255, 0.2591764390468597)\",\"rgba(255, 0, 0, 0.1489599294960499)\",\"rgba(0, 0, 255, 0.20857946425676346)\",\"rgba(255, 0, 0, 0.3309042364358902)\",\"rgba(0, 0, 255, 0.45697722434997556)\",\"rgba(255, 0, 0, 0.38950235247612)\",\"rgba(0, 0, 255, 0.3325397729873657)\",\"rgba(0, 0, 255, 0.27854142189025877)\",\"rgba(0, 0, 255, 0.46781996488571165)\",\"rgba(0, 0, 255, 0.23787814974784852)\",\"rgba(255, 0, 0, 0.1643598809838295)\",\"rgba(255, 0, 0, 0.15681707710027695)\",\"rgba(255, 0, 0, 0.2021089032292366)\",\"rgba(0, 0, 255, 0.3239493280649185)\",\"rgba(0, 0, 255, 0.2816739737987518)\",\"rgba(0, 0, 255, 0.41357854604721067)\",\"rgba(255, 0, 0, 0.2777995079755783)\",\"rgba(255, 0, 0, 0.2666818857192993)\",\"rgba(0, 0, 255, 0.42318800687789915)\",\"rgba(0, 0, 255, 0.18088799566030503)\",\"rgba(0, 0, 255, 0.2810082197189331)\",\"rgba(0, 0, 255, 0.28109431862831114)\",\"rgba(0, 0, 255, 0.4472943007946014)\",\"rgba(0, 0, 255, 0.27397598922252653)\",\"rgba(0, 0, 255, 0.24490857720375062)\",\"rgba(0, 0, 255, 0.46724478006362913)\",\"rgba(0, 0, 255, 0.4688847422599792)\",\"rgba(0, 0, 255, 0.13124589286744595)\",\"rgba(255, 0, 0, 0.187206169962883)\",\"rgba(255, 0, 0, 0.1550176203250885)\",\"rgba(0, 0, 255, 0.15971722677350045)\",\"rgba(0, 0, 255, 0.3590250968933105)\",\"rgba(0, 0, 255, 0.48915732502937315)\",\"rgba(0, 0, 255, 0.34438098073005674)\",\"rgba(0, 0, 255, 0.38259721398353574)\",\"rgba(0, 0, 255, 0.3597565948963165)\",\"rgba(255, 0, 0, 0.36958218216896055)\",\"rgba(255, 0, 0, 0.2608051061630249)\",\"rgba(0, 0, 255, 0.36007843017578123)\",\"rgba(255, 0, 0, 0.29213166832923887)\",\"rgba(0, 0, 255, 0.13488203138113022)\",\"rgba(255, 0, 0, 0.3189231365919113)\",\"rgba(255, 0, 0, 0.1583360865712166)\",\"rgba(255, 0, 0, 0.23929422199726105)\",\"rgba(0, 0, 255, 0.4234710395336151)\",\"rgba(255, 0, 0, 0.10799361150711775)\",\"rgba(255, 0, 0, 0.22437988817691804)\",\"rgba(255, 0, 0, 0.13250041976571084)\",\"rgba(0, 0, 255, 0.36733183860778806)\",\"rgba(255, 0, 0, 0.10533227529376746)\",\"rgba(0, 0, 255, 0.23108427822589875)\",\"rgba(0, 0, 255, 0.1269213728606701)\",\"rgba(0, 0, 255, 0.2178092733025551)\",\"rgba(0, 0, 255, 0.4475667834281921)\",\"rgba(0, 0, 255, 0.29243766367435453)\",\"rgba(255, 0, 0, 0.3232967615127563)\",\"rgba(0, 0, 255, 0.48328558206558225)\",\"rgba(0, 0, 255, 0.2952555626630783)\",\"rgba(255, 0, 0, 0.22201957404613495)\",\"rgba(0, 0, 255, 0.291690519452095)\",\"rgba(255, 0, 0, 0.3410115748643875)\",\"rgba(0, 0, 255, 0.23403376936912537)\",\"rgba(255, 0, 0, 0.1590900182723999)\",\"rgba(255, 0, 0, 0.23071313500404358)\",\"rgba(255, 0, 0, 0.16546325236558915)\",\"rgba(0, 0, 255, 0.5442004561424255)\",\"rgba(255, 0, 0, 0.10763260200619698)\",\"rgba(255, 0, 0, 0.3982511222362518)\",\"rgba(0, 0, 255, 0.3256711423397064)\",\"rgba(0, 0, 255, 0.28056963384151457)\",\"rgba(255, 0, 0, 0.3958565354347229)\",\"rgba(0, 0, 255, 0.23396245241165162)\",\"rgba(255, 0, 0, 0.11965508796274663)\",\"rgba(255, 0, 0, 0.36107057929039)\",\"rgba(0, 0, 255, 0.4305442631244659)\",\"rgba(255, 0, 0, 0.16671704351902009)\",\"rgba(0, 0, 255, 0.24037953317165375)\",\"rgba(0, 0, 255, 0.15226198583841324)\",\"rgba(0, 0, 255, 0.1386566072702408)\",\"rgba(255, 0, 0, 0.13595873713493348)\",\"rgba(255, 0, 0, 0.15670680478215218)\",\"rgba(255, 0, 0, 0.1724240615963936)\",\"rgba(255, 0, 0, 0.21617275327444077)\",\"rgba(0, 0, 255, 0.1054623312316835)\",\"rgba(0, 0, 255, 0.17781460136175156)\",\"rgba(0, 0, 255, 0.2550832390785217)\",\"rgba(0, 0, 255, 0.23532580435276032)\",\"rgba(0, 0, 255, 0.1271420180797577)\",\"rgba(0, 0, 255, 0.25483386814594267)\",\"rgba(0, 0, 255, 0.11958501152694226)\",\"rgba(255, 0, 0, 0.17505299150943757)\",\"rgba(255, 0, 0, 0.20065092146396638)\",\"rgba(255, 0, 0, 0.14416592419147492)\",\"rgba(255, 0, 0, 0.19682178497314454)\",\"rgba(255, 0, 0, 0.13787001967430115)\",\"rgba(0, 0, 255, 0.1209137424826622)\",\"rgba(0, 0, 255, 0.11684178784489632)\",\"rgba(0, 0, 255, 0.15851465463638306)\",\"rgba(255, 0, 0, 0.18223507553339005)\",\"rgba(0, 0, 255, 0.11689410023391247)\",\"rgba(0, 0, 255, 0.17545654475688935)\",\"rgba(0, 0, 255, 0.24069390296936036)\",\"rgba(0, 0, 255, 0.15251357331871987)\",\"rgba(0, 0, 255, 0.11536910142749549)\",\"rgba(0, 0, 255, 0.17129435986280442)\",\"rgba(0, 0, 255, 0.10941847078502179)\",\"rgba(255, 0, 0, 0.1767335817217827)\",\"rgba(0, 0, 255, 0.13535399213433266)\",\"rgba(0, 0, 255, 0.15336353927850724)\",\"rgba(0, 0, 255, 0.20378362983465195)\",\"rgba(0, 0, 255, 0.12714175544679165)\",\"rgba(0, 0, 255, 0.16998100876808167)\",\"rgba(0, 0, 255, 0.2746469140052795)\",\"rgba(0, 0, 255, 0.27548803985118864)\",\"rgba(0, 0, 255, 0.16393295526504517)\",\"rgba(255, 0, 0, 0.15806545540690423)\",\"rgba(0, 0, 255, 0.16643111258745194)\",\"rgba(0, 0, 255, 0.2083114892244339)\",\"rgba(255, 0, 0, 0.13128596022725106)\",\"rgba(0, 0, 255, 0.12906168811023236)\",\"rgba(0, 0, 255, 0.20891366004943848)\",\"rgba(255, 0, 0, 0.15957501381635666)\",\"rgba(255, 0, 0, 0.12259227074682713)\",\"rgba(255, 0, 0, 0.14415588453412057)\",\"rgba(0, 0, 255, 0.10972395390272141)\",\"rgba(0, 0, 255, 0.11184686925262213)\",\"rgba(255, 0, 0, 0.15321859195828438)\",\"rgba(0, 0, 255, 0.22908414006233216)\",\"rgba(0, 0, 255, 0.22326332926750184)\",\"rgba(0, 0, 255, 0.19815044701099396)\",\"rgba(255, 0, 0, 0.11162238139659167)\",\"rgba(0, 0, 255, 0.186253322660923)\",\"rgba(255, 0, 0, 0.14448411986231804)\",\"rgba(255, 0, 0, 0.1661178305745125)\",\"rgba(0, 0, 255, 0.20666831582784653)\",\"rgba(255, 0, 0, 0.12844959124922753)\",\"rgba(0, 0, 255, 0.11097518485039473)\",\"rgba(0, 0, 255, 0.1838935896754265)\",\"rgba(0, 0, 255, 0.19256912022829056)\",\"rgba(0, 0, 255, 0.21605315655469895)\",\"rgba(0, 0, 255, 0.2842900365591049)\",\"rgba(255, 0, 0, 0.12351627871394158)\",\"rgba(0, 0, 255, 0.15942964926362038)\",\"rgba(0, 0, 255, 0.19926258772611619)\",\"rgba(255, 0, 0, 0.1916508510708809)\",\"rgba(255, 0, 0, 0.1020223087631166)\",\"rgba(255, 0, 0, 0.12769693918526173)\",\"rgba(0, 0, 255, 0.12617848478257657)\",\"rgba(0, 0, 255, 0.2083066835999489)\",\"rgba(0, 0, 255, 0.2719566285610199)\",\"rgba(0, 0, 255, 0.24234213531017304)\",\"rgba(0, 0, 255, 0.28349374830722807)\",\"rgba(0, 0, 255, 0.1305425487458706)\",\"rgba(0, 0, 255, 0.11249254308640957)\",\"rgba(255, 0, 0, 0.1090659698471427)\",\"rgba(0, 0, 255, 0.1655781701207161)\",\"rgba(255, 0, 0, 0.21959099024534226)\",\"rgba(255, 0, 0, 0.13270576521754265)\",\"rgba(0, 0, 255, 0.1260148234665394)\",\"rgba(255, 0, 0, 0.16755132228136063)\",\"rgba(255, 0, 0, 0.16150597333908082)\",\"rgba(255, 0, 0, 0.19079410880804062)\",\"rgba(0, 0, 255, 0.16053490936756135)\",\"rgba(0, 0, 255, 0.1525706559419632)\",\"rgba(255, 0, 0, 0.16704683601856232)\",\"rgba(255, 0, 0, 0.1014490872854367)\",\"rgba(255, 0, 0, 0.1155973332002759)\",\"rgba(0, 0, 255, 0.12969427965581418)\",\"rgba(255, 0, 0, 0.19690987318754197)\",\"rgba(255, 0, 0, 0.1365249253809452)\",\"rgba(0, 0, 255, 0.16420420110225678)\",\"rgba(255, 0, 0, 0.12144586481153966)\",\"rgba(255, 0, 0, 0.19393659234046937)\",\"rgba(255, 0, 0, 0.10180329836439342)\",\"rgba(0, 0, 255, 0.15597561225295067)\",\"rgba(255, 0, 0, 0.11922614499926568)\",\"rgba(255, 0, 0, 0.1807197540998459)\",\"rgba(0, 0, 255, 0.13591635078191758)\",\"rgba(0, 0, 255, 0.12025427483022214)\",\"rgba(0, 0, 255, 0.10969479233026505)\",\"rgba(0, 0, 255, 0.13583026677370072)\",\"rgba(255, 0, 0, 0.10631299614906312)\",\"rgba(255, 0, 0, 0.14084687680006028)\",\"rgba(255, 0, 0, 0.12094217017292977)\",\"rgba(0, 0, 255, 0.17337053120136262)\",\"rgba(255, 0, 0, 0.12403955534100533)\",\"rgba(0, 0, 255, 0.10931183528155089)\",\"rgba(255, 0, 0, 0.1337904989719391)\",\"rgba(255, 0, 0, 0.14871316999197007)\",\"rgba(255, 0, 0, 0.13775223344564438)\",\"rgba(255, 0, 0, 0.15456002801656724)\",\"rgba(255, 0, 0, 0.15006642267107964)\",\"rgba(255, 0, 0, 0.11680560037493706)\",\"rgba(0, 0, 255, 0.1573251247406006)\",\"rgba(0, 0, 255, 0.15939288064837456)\",\"rgba(0, 0, 255, 0.16052596867084504)\",\"rgba(0, 0, 255, 0.15456433445215226)\",\"rgba(255, 0, 0, 0.12262217178940774)\",\"rgba(0, 0, 255, 0.11019854284822941)\",\"rgba(0, 0, 255, 0.15847012251615525)\",\"rgba(0, 0, 255, 0.16794290244579316)\",\"rgba(0, 0, 255, 0.10782784912735224)\",\"rgba(255, 0, 0, 0.15122115463018418)\",\"rgba(255, 0, 0, 0.15147096887230874)\",\"rgba(255, 0, 0, 0.11417598221451045)\",\"rgba(0, 0, 255, 0.19837329387664795)\",\"rgba(255, 0, 0, 0.12020259201526642)\",\"rgba(0, 0, 255, 0.2163897141814232)\",\"rgba(0, 0, 255, 0.20436511784791947)\",\"rgba(0, 0, 255, 0.21039837449789048)\",\"rgba(0, 0, 255, 0.2790644973516464)\",\"rgba(0, 0, 255, 0.22842857837677003)\",\"rgba(0, 0, 255, 0.25524275600910185)\",\"rgba(0, 0, 255, 0.19176838397979737)\",\"rgba(255, 0, 0, 0.11521802134811879)\",\"rgba(255, 0, 0, 0.24807364344596863)\",\"rgba(255, 0, 0, 0.14195474460721016)\",\"rgba(0, 0, 255, 0.21409225314855576)\",\"rgba(255, 0, 0, 0.1633474662899971)\",\"rgba(255, 0, 0, 0.16295283883810044)\",\"rgba(0, 0, 255, 0.14685645550489426)\",\"rgba(255, 0, 0, 0.14034655913710595)\",\"rgba(0, 0, 255, 0.10358967785723508)\",\"rgba(0, 0, 255, 0.1459873117506504)\",\"rgba(255, 0, 0, 0.12523482404649258)\",\"rgba(0, 0, 255, 0.15648961290717125)\",\"rgba(0, 0, 255, 0.1803705006837845)\",\"rgba(255, 0, 0, 0.22341751158237458)\",\"rgba(0, 0, 255, 0.17330399751663209)\",\"rgba(0, 0, 255, 0.11614801585674286)\",\"rgba(0, 0, 255, 0.13756815567612649)\",\"rgba(0, 0, 255, 0.1353468768298626)\",\"rgba(0, 0, 255, 0.12318404242396355)\",\"rgba(255, 0, 0, 0.1239439569413662)\",\"rgba(255, 0, 0, 0.15918757244944573)\",\"rgba(0, 0, 255, 0.24578624069690705)\",\"rgba(0, 0, 255, 0.15151745304465294)\",\"rgba(255, 0, 0, 0.12972494624555111)\",\"rgba(0, 0, 255, 0.2308891475200653)\",\"rgba(255, 0, 0, 0.10197818316519261)\",\"rgba(255, 0, 0, 0.23131056725978852)\",\"rgba(255, 0, 0, 0.12178245782852173)\",\"rgba(0, 0, 255, 0.14231640323996544)\",\"rgba(255, 0, 0, 0.21269105225801468)\",\"rgba(0, 0, 255, 0.11979728788137436)\",\"rgba(255, 0, 0, 0.11434784382581711)\",\"rgba(0, 0, 255, 0.17585982978343964)\",\"rgba(0, 0, 255, 0.2864547878503799)\",\"rgba(255, 0, 0, 0.19370731562376023)\",\"rgba(255, 0, 0, 0.11561188604682684)\",\"rgba(0, 0, 255, 0.17931382954120637)\",\"rgba(255, 0, 0, 0.18723654597997666)\",\"rgba(0, 0, 255, 0.1310098957270384)\",\"rgba(255, 0, 0, 0.10726854288950563)\",\"rgba(0, 0, 255, 0.12043144963681698)\",\"rgba(255, 0, 0, 0.14788494110107422)\",\"rgba(255, 0, 0, 0.19651190787553788)\",\"rgba(0, 0, 255, 0.10894980225712061)\",\"rgba(255, 0, 0, 0.11024332363158465)\",\"rgba(255, 0, 0, 0.1113961324095726)\",\"rgba(0, 0, 255, 0.19189245104789734)\",\"rgba(0, 0, 255, 0.1650133341550827)\",\"rgba(255, 0, 0, 0.12992463670670987)\",\"rgba(0, 0, 255, 0.20241249203681946)\",\"rgba(0, 0, 255, 0.1984674021601677)\",\"rgba(0, 0, 255, 0.13412004932761193)\",\"rgba(0, 0, 255, 0.16683219224214554)\",\"rgba(255, 0, 0, 0.12224020659923554)\",\"rgba(0, 0, 255, 0.12131464891135693)\",\"rgba(255, 0, 0, 0.12754815481603146)\",\"rgba(255, 0, 0, 0.21309650540351868)\",\"rgba(255, 0, 0, 0.1770358145236969)\",\"rgba(0, 0, 255, 0.11529336255043746)\",\"rgba(0, 0, 255, 0.20691694170236588)\",\"rgba(255, 0, 0, 0.14987320676445962)\",\"rgba(255, 0, 0, 0.1559365823864937)\",\"rgba(255, 0, 0, 0.1450836792588234)\",\"rgba(0, 0, 255, 0.22001517415046692)\",\"rgba(255, 0, 0, 0.20618361085653306)\",\"rgba(0, 0, 255, 0.14627396166324616)\",\"rgba(0, 0, 255, 0.28120065331459043)\",\"rgba(0, 0, 255, 0.11846693083643914)\",\"rgba(255, 0, 0, 0.17231620699167252)\",\"rgba(0, 0, 255, 0.13356506302952767)\",\"rgba(0, 0, 255, 0.10218037003651262)\",\"rgba(0, 0, 255, 0.21720247566699982)\",\"rgba(0, 0, 255, 0.1685992017388344)\",\"rgba(255, 0, 0, 0.14049494117498398)\",\"rgba(0, 0, 255, 0.19640442579984665)\",\"rgba(0, 0, 255, 0.2749678999185562)\",\"rgba(0, 0, 255, 0.10281565403565765)\",\"rgba(0, 0, 255, 0.10951499957591296)\",\"rgba(255, 0, 0, 0.11713052950799466)\",\"rgba(0, 0, 255, 0.13773346543312073)\",\"rgba(255, 0, 0, 0.1716003179550171)\",\"rgba(0, 0, 255, 0.17127328962087632)\",\"rgba(0, 0, 255, 0.12467096224427224)\",\"rgba(0, 0, 255, 0.19485531598329545)\",\"rgba(0, 0, 255, 0.14595022648572922)\",\"rgba(0, 0, 255, 0.2692528277635574)\",\"rgba(0, 0, 255, 0.10690880464389921)\",\"rgba(0, 0, 255, 0.20661641508340836)\",\"rgba(0, 0, 255, 0.15907241627573968)\",\"rgba(255, 0, 0, 0.14439767450094224)\",\"rgba(255, 0, 0, 0.1238613747060299)\",\"rgba(255, 0, 0, 0.19980181604623795)\",\"rgba(255, 0, 0, 0.14902878031134606)\",\"rgba(0, 0, 255, 0.14211461916565896)\",\"rgba(0, 0, 255, 0.256250849366188)\",\"rgba(0, 0, 255, 0.14176847636699677)\",\"rgba(0, 0, 255, 0.262353903055191)\",\"rgba(0, 0, 255, 0.18315620571374894)\",\"rgba(0, 0, 255, 0.12715127542614937)\",\"rgba(255, 0, 0, 0.11525964587926865)\",\"rgba(0, 0, 255, 0.1643814578652382)\",\"rgba(0, 0, 255, 0.19453471750020981)\",\"rgba(0, 0, 255, 0.11423598732799292)\",\"rgba(255, 0, 0, 0.1350361578166485)\",\"rgba(0, 0, 255, 0.24258751273155213)\",\"rgba(0, 0, 255, 0.21224818229675294)\",\"rgba(0, 0, 255, 0.12961379289627076)\",\"rgba(0, 0, 255, 0.2269052028656006)\",\"rgba(0, 0, 255, 0.23853234052658082)\",\"rgba(255, 0, 0, 0.20922084748744965)\",\"rgba(255, 0, 0, 0.22195508182048798)\",\"rgba(255, 0, 0, 0.21937642097473145)\",\"rgba(255, 0, 0, 0.18703076094388962)\",\"rgba(0, 0, 255, 0.19272335469722748)\",\"rgba(255, 0, 0, 0.2073676198720932)\",\"rgba(255, 0, 0, 0.11221138704568148)\",\"rgba(0, 0, 255, 0.20201732069253922)\",\"rgba(0, 0, 255, 0.15784472450613976)\",\"rgba(0, 0, 255, 0.13575390949845315)\",\"rgba(0, 0, 255, 0.17514524459838868)\",\"rgba(0, 0, 255, 0.10388252851553262)\",\"rgba(255, 0, 0, 0.11220337487757207)\",\"rgba(255, 0, 0, 0.20671365261077881)\",\"rgba(0, 0, 255, 0.14328499361872674)\",\"rgba(255, 0, 0, 0.12937498688697815)\",\"rgba(255, 0, 0, 0.1867907926440239)\",\"rgba(0, 0, 255, 0.1518893264234066)\",\"rgba(0, 0, 255, 0.12568084746599198)\",\"rgba(0, 0, 255, 0.10187984190415592)\",\"rgba(0, 0, 255, 0.20666269063949586)\",\"rgba(255, 0, 0, 0.1337924286723137)\",\"rgba(255, 0, 0, 0.12738125808537007)\",\"rgba(0, 0, 255, 0.23599146902561188)\",\"rgba(255, 0, 0, 0.1201748013496399)\",\"rgba(0, 0, 255, 0.1866667553782463)\",\"rgba(0, 0, 255, 0.16635120511054993)\",\"rgba(255, 0, 0, 0.12595003321766854)\",\"rgba(0, 0, 255, 0.11274861209094525)\",\"rgba(255, 0, 0, 0.1086600873619318)\",\"rgba(0, 0, 255, 0.17839718461036683)\",\"rgba(0, 0, 255, 0.14725469276309014)\",\"rgba(255, 0, 0, 0.10861964654177428)\",\"rgba(0, 0, 255, 0.21814913898706437)\",\"rgba(0, 0, 255, 0.1377427414059639)\",\"rgba(0, 0, 255, 0.13983725234866143)\",\"rgba(0, 0, 255, 0.24521875977516175)\",\"rgba(255, 0, 0, 0.11230627484619618)\",\"rgba(255, 0, 0, 0.1379784442484379)\",\"rgba(0, 0, 255, 0.11385879050940276)\",\"rgba(255, 0, 0, 0.1376894883811474)\",\"rgba(0, 0, 255, 0.16396060436964036)\",\"rgba(255, 0, 0, 0.19228187799453736)\",\"rgba(255, 0, 0, 0.16415797024965287)\",\"rgba(0, 0, 255, 0.15122511833906174)\",\"rgba(0, 0, 255, 0.2187347322702408)\",\"rgba(0, 0, 255, 0.15166426301002503)\",\"rgba(0, 0, 255, 0.2489807516336441)\",\"rgba(0, 0, 255, 0.11504863426089287)\",\"rgba(0, 0, 255, 0.1371341183781624)\",\"rgba(0, 0, 255, 0.18508042246103287)\",\"rgba(0, 0, 255, 0.18222902566194535)\",\"rgba(0, 0, 255, 0.22881997227668763)\",\"rgba(0, 0, 255, 0.23717131316661835)\",\"rgba(0, 0, 255, 0.16285112351179123)\",\"rgba(0, 0, 255, 0.11251801289618016)\",\"rgba(255, 0, 0, 0.12101685851812363)\",\"rgba(255, 0, 0, 0.15124419182538987)\",\"rgba(0, 0, 255, 0.1134767796844244)\",\"rgba(0, 0, 255, 0.12203266695141793)\",\"rgba(0, 0, 255, 0.10303762708790601)\",\"rgba(0, 0, 255, 0.2956202834844589)\",\"rgba(0, 0, 255, 0.14800493642687798)\",\"rgba(0, 0, 255, 0.17198162376880646)\",\"rgba(0, 0, 255, 0.12344769425690175)\",\"rgba(0, 0, 255, 0.10664820661768318)\",\"rgba(255, 0, 0, 0.10932208448648453)\",\"rgba(0, 0, 255, 0.11176549028605223)\",\"rgba(255, 0, 0, 0.1951811671257019)\",\"rgba(255, 0, 0, 0.23855357468128205)\",\"rgba(0, 0, 255, 0.19646214544773102)\",\"rgba(0, 0, 255, 0.23631613552570344)\",\"rgba(255, 0, 0, 0.13550074622035027)\",\"rgba(255, 0, 0, 0.13415663540363312)\",\"rgba(0, 0, 255, 0.10178810534998775)\",\"rgba(255, 0, 0, 0.10528963981196285)\",\"rgba(0, 0, 255, 0.11088422257453204)\",\"rgba(255, 0, 0, 0.11812226139009)\",\"rgba(255, 0, 0, 0.12855682745575905)\",\"rgba(255, 0, 0, 0.10194849295075983)\",\"rgba(0, 0, 255, 0.11335254777222872)\",\"rgba(255, 0, 0, 0.10477193295955659)\",\"rgba(0, 0, 255, 0.13525649756193162)\",\"rgba(255, 0, 0, 0.10138341111596674)\",\"rgba(255, 0, 0, 0.11898531429469586)\",\"rgba(255, 0, 0, 0.10669446168467403)\",\"rgba(0, 0, 255, 0.11909543946385384)\",\"rgba(0, 0, 255, 0.11284026727080346)\",\"rgba(0, 0, 255, 0.12494340389966965)\",\"rgba(255, 0, 0, 0.13423922508955002)\",\"rgba(0, 0, 255, 0.10875162612646819)\",\"rgba(0, 0, 255, 0.12378135360777379)\",\"rgba(0, 0, 255, 0.11301876362413169)\",\"rgba(0, 0, 255, 0.1392383858561516)\",\"rgba(255, 0, 0, 0.12818730287253857)\",\"rgba(255, 0, 0, 0.10952464435249568)\",\"rgba(255, 0, 0, 0.1302299577742815)\",\"rgba(0, 0, 255, 0.11377327833324671)\",\"rgba(255, 0, 0, 0.11041976548731328)\",\"rgba(255, 0, 0, 0.13126677870750428)\",\"rgba(0, 0, 255, 0.13708266839385033)\",\"rgba(0, 0, 255, 0.1256254728883505)\",\"rgba(255, 0, 0, 0.12527118287980557)\",\"rgba(255, 0, 0, 0.12645238488912583)\",\"rgba(0, 0, 255, 0.11784278266131878)\",\"rgba(0, 0, 255, 0.1245390459895134)\",\"rgba(255, 0, 0, 0.10199589133262635)\",\"rgba(0, 0, 255, 0.12229139767587185)\",\"rgba(255, 0, 0, 0.12321538887917996)\",\"rgba(255, 0, 0, 0.10967915635555983)\",\"rgba(0, 0, 255, 0.13354786708950997)\",\"rgba(255, 0, 0, 0.13497540578246117)\",\"rgba(255, 0, 0, 0.11177858095616103)\",\"rgba(0, 0, 255, 0.12067967876791955)\",\"rgba(0, 0, 255, 0.1145891271531582)\",\"rgba(255, 0, 0, 0.11227287389338017)\",\"rgba(255, 0, 0, 0.1227322205901146)\",\"rgba(0, 0, 255, 0.11777483895421029)\",\"rgba(255, 0, 0, 0.1210136566311121)\",\"rgba(0, 0, 255, 0.13480204567313195)\",\"rgba(0, 0, 255, 0.10357760721817613)\",\"rgba(255, 0, 0, 0.10157259195111693)\",\"rgba(255, 0, 0, 0.10464161392301322)\",\"rgba(0, 0, 255, 0.12674394473433495)\",\"rgba(255, 0, 0, 0.11254624594002963)\",\"rgba(255, 0, 0, 0.10764271104708314)\",\"rgba(0, 0, 255, 0.12714161574840546)\",\"rgba(255, 0, 0, 0.12564972266554833)\",\"rgba(255, 0, 0, 0.11082544773817063)\",\"rgba(255, 0, 0, 0.10330140697769821)\",\"rgba(255, 0, 0, 0.10202240608632565)\",\"rgba(0, 0, 255, 0.10889413338154555)\",\"rgba(255, 0, 0, 0.11328547578305007)\",\"rgba(255, 0, 0, 0.10853688456118107)\",\"rgba(255, 0, 0, 0.12781956270337105)\",\"rgba(0, 0, 255, 0.11289478410035372)\",\"rgba(0, 0, 255, 0.10882397033274174)\",\"rgba(0, 0, 255, 0.12341452799737454)\",\"rgba(255, 0, 0, 0.13452210128307343)\",\"rgba(0, 0, 255, 0.12166286669671536)\",\"rgba(0, 0, 255, 0.1051689762622118)\",\"rgba(255, 0, 0, 0.10501733785495163)\",\"rgba(0, 0, 255, 0.12685213834047318)\",\"rgba(255, 0, 0, 0.12051553502678872)\",\"rgba(255, 0, 0, 0.12066951431334019)\",\"rgba(255, 0, 0, 0.11311690732836724)\",\"rgba(255, 0, 0, 0.12540358901023865)\",\"rgba(0, 0, 255, 0.11209100801497698)\",\"rgba(0, 0, 255, 0.13927620872855187)\",\"rgba(0, 0, 255, 0.10848551187664271)\",\"rgba(0, 0, 255, 0.10279579334892333)\",\"rgba(255, 0, 0, 0.12267191745340825)\",\"rgba(255, 0, 0, 0.10763601623475552)\",\"rgba(0, 0, 255, 0.11573729328811169)\",\"rgba(255, 0, 0, 0.10125677592586727)\",\"rgba(255, 0, 0, 0.13239775821566582)\",\"rgba(255, 0, 0, 0.13136330842971802)\",\"rgba(255, 0, 0, 0.12779588103294373)\",\"rgba(255, 0, 0, 0.1357319675385952)\",\"rgba(255, 0, 0, 0.10869381800293923)\",\"rgba(255, 0, 0, 0.13561437502503396)\",\"rgba(255, 0, 0, 0.1225213747471571)\",\"rgba(255, 0, 0, 0.12315363846719266)\",\"rgba(0, 0, 255, 0.11590544171631337)\",\"rgba(255, 0, 0, 0.12157687954604626)\",\"rgba(0, 0, 255, 0.10577223906293512)\",\"rgba(255, 0, 0, 0.12614531479775906)\",\"rgba(0, 0, 255, 0.10851957499980927)\",\"rgba(0, 0, 255, 0.12348689734935761)\",\"rgba(0, 0, 255, 0.12644682675600052)\",\"rgba(0, 0, 255, 0.11854973286390305)\",\"rgba(255, 0, 0, 0.10309220398776234)\",\"rgba(0, 0, 255, 0.12290166728198529)\",\"rgba(0, 0, 255, 0.1170432385057211)\",\"rgba(0, 0, 255, 0.12599342912435532)\",\"rgba(255, 0, 0, 0.11203938946127892)\",\"rgba(255, 0, 0, 0.12202796190977097)\",\"rgba(255, 0, 0, 0.12837234921753407)\",\"rgba(255, 0, 0, 0.12956916391849518)\",\"rgba(0, 0, 255, 0.10923163164407015)\",\"rgba(0, 0, 255, 0.11519543025642634)\",\"rgba(0, 0, 255, 0.1216660387814045)\",\"rgba(0, 0, 255, 0.12724638395011426)\",\"rgba(255, 0, 0, 0.10837976671755314)\",\"rgba(0, 0, 255, 0.12760357223451138)\",\"rgba(255, 0, 0, 0.10698248436674476)\",\"rgba(255, 0, 0, 0.12589728683233262)\",\"rgba(255, 0, 0, 0.13548674285411835)\",\"rgba(255, 0, 0, 0.12117929048836232)\",\"rgba(255, 0, 0, 0.12281826175749302)\",\"rgba(0, 0, 255, 0.12560950629413128)\",\"rgba(255, 0, 0, 0.1332603193819523)\",\"rgba(255, 0, 0, 0.1105143990367651)\",\"rgba(0, 0, 255, 0.11511187013238669)\",\"rgba(255, 0, 0, 0.115893142670393)\",\"rgba(0, 0, 255, 0.11972759515047074)\",\"rgba(255, 0, 0, 0.130759422108531)\",\"rgba(255, 0, 0, 0.11499363873153925)\",\"rgba(0, 0, 255, 0.12377713657915593)\",\"rgba(255, 0, 0, 0.1070054205134511)\",\"rgba(255, 0, 0, 0.1233430676162243)\",\"rgba(0, 0, 255, 0.11888049766421319)\",\"rgba(0, 0, 255, 0.13450947627425194)\",\"rgba(255, 0, 0, 0.10393780786544085)\",\"rgba(0, 0, 255, 0.1307334214448929)\",\"rgba(255, 0, 0, 0.1209021419286728)\",\"rgba(255, 0, 0, 0.10316362059675158)\",\"rgba(0, 0, 255, 0.11180319860577584)\",\"rgba(0, 0, 255, 0.13188538178801537)\",\"rgba(0, 0, 255, 0.10275327498093248)\",\"rgba(0, 0, 255, 0.10653820391744376)\",\"rgba(0, 0, 255, 0.10739089958369732)\",\"rgba(0, 0, 255, 0.12424860559403897)\",\"rgba(255, 0, 0, 0.11453035324811936)\",\"rgba(0, 0, 255, 0.10300271180458367)\",\"rgba(0, 0, 255, 0.13130850940942765)\",\"rgba(0, 0, 255, 0.1298285335302353)\",\"rgba(0, 0, 255, 0.11188487280160189)\",\"rgba(0, 0, 255, 0.12178274281322957)\",\"rgba(255, 0, 0, 0.13709698840975762)\",\"rgba(0, 0, 255, 0.11990456692874432)\",\"rgba(0, 0, 255, 0.12315901406109334)\",\"rgba(255, 0, 0, 0.1294719133526087)\",\"rgba(0, 0, 255, 0.12622405998408795)\",\"rgba(0, 0, 255, 0.10833397079259158)\",\"rgba(255, 0, 0, 0.11352474559098483)\",\"rgba(0, 0, 255, 0.10798902846872807)\",\"rgba(0, 0, 255, 0.11840439066290856)\",\"rgba(0, 0, 255, 0.11246601063758135)\",\"rgba(0, 0, 255, 0.13183584660291672)\",\"rgba(255, 0, 0, 0.11004013419151307)\",\"rgba(0, 0, 255, 0.11885845139622689)\",\"rgba(255, 0, 0, 0.12595820650458336)\",\"rgba(0, 0, 255, 0.11622243598103524)\",\"rgba(255, 0, 0, 0.1310254879295826)\",\"rgba(255, 0, 0, 0.11859396696090699)\",\"rgba(255, 0, 0, 0.13564567118883133)\",\"rgba(0, 0, 255, 0.10313020590692759)\",\"rgba(0, 0, 255, 0.12493580430746079)\",\"rgba(0, 0, 255, 0.1075038094073534)\",\"rgba(0, 0, 255, 0.11264271512627602)\",\"rgba(255, 0, 0, 0.12367415465414525)\",\"rgba(255, 0, 0, 0.12898183465003968)\",\"rgba(255, 0, 0, 0.11142000779509545)\",\"rgba(0, 0, 255, 0.13165503963828087)\",\"rgba(0, 0, 255, 0.1350271873176098)\",\"rgba(255, 0, 0, 0.11390573754906655)\",\"rgba(0, 0, 255, 0.1333295352756977)\",\"rgba(0, 0, 255, 0.1265175923705101)\",\"rgba(0, 0, 255, 0.11086704060435296)\",\"rgba(255, 0, 0, 0.11598026417195798)\",\"rgba(255, 0, 0, 0.12957257628440857)\",\"rgba(255, 0, 0, 0.13771797567605973)\",\"rgba(255, 0, 0, 0.11726574636995793)\",\"rgba(255, 0, 0, 0.1128816943615675)\",\"rgba(0, 0, 255, 0.13586895763874055)\",\"rgba(0, 0, 255, 0.11412793342024088)\",\"rgba(0, 0, 255, 0.1373203605413437)\",\"rgba(255, 0, 0, 0.10446724426001311)\",\"rgba(255, 0, 0, 0.12678651548922062)\",\"rgba(255, 0, 0, 0.10116313295438886)\",\"rgba(0, 0, 255, 0.12944259718060494)\",\"rgba(0, 0, 255, 0.12917208522558213)\",\"rgba(0, 0, 255, 0.11909547299146653)\",\"rgba(255, 0, 0, 0.10127910147421063)\",\"rgba(255, 0, 0, 0.13052162006497384)\",\"rgba(0, 0, 255, 0.1019775559194386)\",\"rgba(0, 0, 255, 0.13780749812722207)\",\"rgba(0, 0, 255, 0.14757176339626313)\",\"rgba(0, 0, 255, 0.37348280400037764)\",\"rgba(0, 0, 255, 0.36263965368270873)\",\"rgba(255, 0, 0, 0.33358931466937064)\",\"rgba(0, 0, 255, 0.3411991812288761)\",\"rgba(255, 0, 0, 0.3570509172976017)\",\"rgba(255, 0, 0, 0.369317989051342)\",\"rgba(0, 0, 255, 0.30326083065010606)\",\"rgba(0, 0, 255, 0.3178831312805414)\",\"rgba(0, 0, 255, 0.3597972050309181)\",\"rgba(255, 0, 0, 0.32757588028907775)\"],\"size\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1.18646639585495,1.134598284959793,0.5,0.5,0.6296296566724777,0.8919081687927246,0.5,1.1262972950935364,0.5,0.5,1.257836401462555,1.2468833327293396,0.5,0.8167163729667664,0.5,0.764453262090683,0.5,0.5,0.5,0.6797190606594086,1.0262751281261444,0.931099534034729,0.5242124497890472,0.5079538822174072,0.5871261209249496,0.5,0.5,0.5863463580608368,1.2238749861717224,0.5228210538625717,0.5,0.8284536302089691,0.5256775170564651,0.5,0.597246453166008,0.5,0.5,0.5,0.6422354131937027,0.8422889113426208,0.5,0.5586929619312286,1.0279181599617004,0.5,1.0378715693950653,0.8137849867343903,0.5,0.6459799110889435,0.5,0.5,0.5,0.5,0.5,0.5,1.3410336077213287,0.7581565082073212,0.7495684325695038,0.5,0.5,0.7210509181022644,0.9668569564819336,0.5,0.5,1.1125564277172089,1.146668404340744,0.5,0.5,0.5,0.5,0.6927127093076706,1.0709316730499268,0.86850705742836,0.6976193189620972,0.5356242656707764,1.103459894657135,0.5,0.5,0.5,0.5,0.6718479841947556,0.5450219213962555,0.9407356381416321,0.5333985239267349,0.500045657157898,0.9695640206336975,0.5,0.5430246591567993,0.5432829558849335,1.0418829023838043,0.5219279676675797,0.5,1.1017343401908875,1.1066542267799377,0.5,0.5,0.5,0.5,0.7770752906799316,1.1674719750881195,0.7331429421901703,0.8477916419506073,0.7792697846889496,0.8087465465068817,0.5,0.7802352905273438,0.5763950049877167,0.5,0.656769409775734,0.5,0.5,0.9704131186008453,0.5,0.5,0.5,0.8019955158233643,0.5,0.5,0.5,0.5,1.0427003502845764,0.5773129910230637,0.669890284538269,1.1498567461967468,0.5857666879892349,0.5,0.5750715583562851,0.7230347245931625,0.5,0.5,0.5,0.5,1.3326013684272766,0.5,0.8947533667087555,0.6770134270191193,0.5417089015245438,0.8875696063041687,0.5,0.5,0.78321173787117,0.9916327893733978,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5239407420158386,0.526464119553566,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5528701096773148,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5158698856830597,0.5,0.5504812449216843,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5371934920549393,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5593643635511398,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5436019599437714,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5249036997556686,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5077584832906723,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5868608504533768,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,1,1,1,1,1,1,1,1,1,1]},\"mode\":\"lines\",\"text\":[\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Conexión entrada\",\"Peso: 0.3955\",\"Peso: 0.3782\",\"Peso: 0.0447\",\"Peso: -0.0777\",\"Peso: 0.2099\",\"Peso: 0.2973\",\"Peso: -0.1524\",\"Peso: 0.3754\",\"Peso: -0.0597\",\"Peso: 0.0735\",\"Peso: 0.4193\",\"Peso: 0.4156\",\"Peso: -0.0752\",\"Peso: 0.2722\",\"Peso: -0.0768\",\"Peso: 0.2548\",\"Peso: -0.0848\",\"Peso: 0.0126\",\"Peso: 0.0764\",\"Peso: 0.2266\",\"Peso: 0.3421\",\"Peso: 0.3104\",\"Peso: -0.1747\",\"Peso: -0.1693\",\"Peso: -0.1957\",\"Peso: 0.0604\",\"Peso: 0.0059\",\"Peso: -0.1954\",\"Peso: 0.4080\",\"Peso: 0.1743\",\"Peso: -0.1287\",\"Peso: 0.2762\",\"Peso: 0.1752\",\"Peso: -0.0093\",\"Peso: -0.1991\",\"Peso: 0.0545\",\"Peso: -0.0879\",\"Peso: -0.0030\",\"Peso: 0.2141\",\"Peso: 0.2808\",\"Peso: 0.0792\",\"Peso: 0.1862\",\"Peso: 0.3426\",\"Peso: -0.1101\",\"Peso: 0.3460\",\"Peso: -0.2713\",\"Peso: -0.0063\",\"Peso: 0.2153\",\"Peso: 0.0714\",\"Peso: 0.0726\",\"Peso: 0.1497\",\"Peso: 0.0811\",\"Peso: 0.0727\",\"Peso: -0.1172\",\"Peso: -0.4470\",\"Peso: -0.2527\",\"Peso: -0.2499\",\"Peso: 0.0631\",\"Peso: 0.1359\",\"Peso: 0.2404\",\"Peso: 0.3223\",\"Peso: -0.1123\",\"Peso: 0.1133\",\"Peso: 0.3709\",\"Peso: 0.3822\",\"Peso: 0.0539\",\"Peso: 0.1592\",\"Peso: -0.0490\",\"Peso: 0.1086\",\"Peso: -0.2309\",\"Peso: 0.3570\",\"Peso: -0.2895\",\"Peso: 0.2325\",\"Peso: 0.1785\",\"Peso: 0.3678\",\"Peso: 0.1379\",\"Peso: -0.0644\",\"Peso: -0.0568\",\"Peso: -0.1021\",\"Peso: 0.2239\",\"Peso: 0.1817\",\"Peso: 0.3136\",\"Peso: -0.1778\",\"Peso: -0.1667\",\"Peso: 0.3232\",\"Peso: 0.0809\",\"Peso: 0.1810\",\"Peso: 0.1811\",\"Peso: 0.3473\",\"Peso: 0.1740\",\"Peso: 0.1449\",\"Peso: 0.3672\",\"Peso: 0.3689\",\"Peso: 0.0312\",\"Peso: -0.0872\",\"Peso: -0.0550\",\"Peso: 0.0597\",\"Peso: 0.2590\",\"Peso: 0.3892\",\"Peso: 0.2444\",\"Peso: 0.2826\",\"Peso: 0.2598\",\"Peso: -0.2696\",\"Peso: -0.1608\",\"Peso: 0.2601\",\"Peso: -0.1921\",\"Peso: 0.0349\",\"Peso: -0.2189\",\"Peso: -0.0583\",\"Peso: -0.1393\",\"Peso: 0.3235\",\"Peso: -0.0080\",\"Peso: -0.1244\",\"Peso: -0.0325\",\"Peso: 0.2673\",\"Peso: -0.0053\",\"Peso: 0.1311\",\"Peso: 0.0269\",\"Peso: 0.1178\",\"Peso: 0.3476\",\"Peso: 0.1924\",\"Peso: -0.2233\",\"Peso: 0.3833\",\"Peso: 0.1953\",\"Peso: -0.1220\",\"Peso: 0.1917\",\"Peso: -0.2410\",\"Peso: 0.1340\",\"Peso: -0.0591\",\"Peso: -0.1307\",\"Peso: -0.0655\",\"Peso: 0.4442\",\"Peso: -0.0076\",\"Peso: -0.2983\",\"Peso: 0.2257\",\"Peso: 0.1806\",\"Peso: -0.2959\",\"Peso: 0.1340\",\"Peso: -0.0197\",\"Peso: -0.2611\",\"Peso: 0.3305\",\"Peso: -0.0667\",\"Peso: 0.1404\",\"Peso: 0.0523\",\"Peso: 0.0387\",\"Peso: -0.0360\",\"Peso: -0.0567\",\"Peso: -0.0724\",\"Peso: -0.1162\",\"Peso: 0.0055\",\"Peso: 0.0778\",\"Peso: 0.1551\",\"Peso: 0.1353\",\"Peso: 0.0271\",\"Peso: 0.1548\",\"Peso: 0.0196\",\"Peso: -0.0751\",\"Peso: -0.1007\",\"Peso: -0.0442\",\"Peso: -0.0968\",\"Peso: -0.0379\",\"Peso: 0.0209\",\"Peso: 0.0168\",\"Peso: 0.0585\",\"Peso: -0.0822\",\"Peso: 0.0169\",\"Peso: 0.0755\",\"Peso: 0.1407\",\"Peso: 0.0525\",\"Peso: 0.0154\",\"Peso: 0.0713\",\"Peso: 0.0094\",\"Peso: -0.0767\",\"Peso: 0.0354\",\"Peso: 0.0534\",\"Peso: 0.1038\",\"Peso: 0.0271\",\"Peso: 0.0700\",\"Peso: 0.1746\",\"Peso: 0.1755\",\"Peso: 0.0639\",\"Peso: -0.0581\",\"Peso: 0.0664\",\"Peso: 0.1083\",\"Peso: -0.0313\",\"Peso: 0.0291\",\"Peso: 0.1089\",\"Peso: -0.0596\",\"Peso: -0.0226\",\"Peso: -0.0442\",\"Peso: 0.0097\",\"Peso: 0.0118\",\"Peso: -0.0532\",\"Peso: 0.1291\",\"Peso: 0.1233\",\"Peso: 0.0982\",\"Peso: -0.0116\",\"Peso: 0.0863\",\"Peso: -0.0445\",\"Peso: -0.0661\",\"Peso: 0.1067\",\"Peso: -0.0284\",\"Peso: 0.0110\",\"Peso: 0.0839\",\"Peso: 0.0926\",\"Peso: 0.1161\",\"Peso: 0.1843\",\"Peso: -0.0235\",\"Peso: 0.0594\",\"Peso: 0.0993\",\"Peso: -0.0917\",\"Peso: -0.0020\",\"Peso: -0.0277\",\"Peso: 0.0262\",\"Peso: 0.1083\",\"Peso: 0.1720\",\"Peso: 0.1423\",\"Peso: 0.1835\",\"Peso: 0.0305\",\"Peso: 0.0125\",\"Peso: -0.0091\",\"Peso: 0.0656\",\"Peso: -0.1196\",\"Peso: -0.0327\",\"Peso: 0.0260\",\"Peso: -0.0676\",\"Peso: -0.0615\",\"Peso: -0.0908\",\"Peso: 0.0605\",\"Peso: 0.0526\",\"Peso: -0.0670\",\"Peso: -0.0014\",\"Peso: -0.0156\",\"Peso: 0.0297\",\"Peso: -0.0969\",\"Peso: -0.0365\",\"Peso: 0.0642\",\"Peso: -0.0214\",\"Peso: -0.0939\",\"Peso: -0.0018\",\"Peso: 0.0560\",\"Peso: -0.0192\",\"Peso: -0.0807\",\"Peso: 0.0359\",\"Peso: 0.0203\",\"Peso: 0.0097\",\"Peso: 0.0358\",\"Peso: -0.0063\",\"Peso: -0.0408\",\"Peso: -0.0209\",\"Peso: 0.0734\",\"Peso: -0.0240\",\"Peso: 0.0093\",\"Peso: -0.0338\",\"Peso: -0.0487\",\"Peso: -0.0378\",\"Peso: -0.0546\",\"Peso: -0.0501\",\"Peso: -0.0168\",\"Peso: 0.0573\",\"Peso: 0.0594\",\"Peso: 0.0605\",\"Peso: 0.0546\",\"Peso: -0.0226\",\"Peso: 0.0102\",\"Peso: 0.0585\",\"Peso: 0.0679\",\"Peso: 0.0078\",\"Peso: -0.0512\",\"Peso: -0.0515\",\"Peso: -0.0142\",\"Peso: 0.0984\",\"Peso: -0.0202\",\"Peso: 0.1164\",\"Peso: 0.1044\",\"Peso: 0.1104\",\"Peso: 0.1791\",\"Peso: 0.1284\",\"Peso: 0.1552\",\"Peso: 0.0918\",\"Peso: -0.0152\",\"Peso: -0.1481\",\"Peso: -0.0420\",\"Peso: 0.1141\",\"Peso: -0.0633\",\"Peso: -0.0630\",\"Peso: 0.0469\",\"Peso: -0.0403\",\"Peso: 0.0036\",\"Peso: 0.0460\",\"Peso: -0.0252\",\"Peso: 0.0565\",\"Peso: 0.0804\",\"Peso: -0.1234\",\"Peso: 0.0733\",\"Peso: 0.0161\",\"Peso: 0.0376\",\"Peso: 0.0353\",\"Peso: 0.0232\",\"Peso: -0.0239\",\"Peso: -0.0592\",\"Peso: 0.1458\",\"Peso: 0.0515\",\"Peso: -0.0297\",\"Peso: 0.1309\",\"Peso: -0.0020\",\"Peso: -0.1313\",\"Peso: -0.0218\",\"Peso: 0.0423\",\"Peso: -0.1127\",\"Peso: 0.0198\",\"Peso: -0.0143\",\"Peso: 0.0759\",\"Peso: 0.1865\",\"Peso: -0.0937\",\"Peso: -0.0156\",\"Peso: 0.0793\",\"Peso: -0.0872\",\"Peso: 0.0310\",\"Peso: -0.0073\",\"Peso: 0.0204\",\"Peso: -0.0479\",\"Peso: -0.0965\",\"Peso: 0.0089\",\"Peso: -0.0102\",\"Peso: -0.0114\",\"Peso: 0.0919\",\"Peso: 0.0650\",\"Peso: -0.0299\",\"Peso: 0.1024\",\"Peso: 0.0985\",\"Peso: 0.0341\",\"Peso: 0.0668\",\"Peso: -0.0222\",\"Peso: 0.0213\",\"Peso: -0.0275\",\"Peso: -0.1131\",\"Peso: -0.0770\",\"Peso: 0.0153\",\"Peso: 0.1069\",\"Peso: -0.0499\",\"Peso: -0.0559\",\"Peso: -0.0451\",\"Peso: 0.1200\",\"Peso: -0.1062\",\"Peso: 0.0463\",\"Peso: 0.1812\",\"Peso: 0.0185\",\"Peso: -0.0723\",\"Peso: 0.0336\",\"Peso: 0.0022\",\"Peso: 0.1172\",\"Peso: 0.0686\",\"Peso: -0.0405\",\"Peso: 0.0964\",\"Peso: 0.1750\",\"Peso: 0.0028\",\"Peso: 0.0095\",\"Peso: -0.0171\",\"Peso: 0.0377\",\"Peso: -0.0716\",\"Peso: 0.0713\",\"Peso: 0.0247\",\"Peso: 0.0949\",\"Peso: 0.0460\",\"Peso: 0.1693\",\"Peso: 0.0069\",\"Peso: 0.1066\",\"Peso: 0.0591\",\"Peso: -0.0444\",\"Peso: -0.0239\",\"Peso: -0.0998\",\"Peso: -0.0490\",\"Peso: 0.0421\",\"Peso: 0.1563\",\"Peso: 0.0418\",\"Peso: 0.1624\",\"Peso: 0.0832\",\"Peso: 0.0272\",\"Peso: -0.0153\",\"Peso: 0.0644\",\"Peso: 0.0945\",\"Peso: 0.0142\",\"Peso: -0.0350\",\"Peso: 0.1426\",\"Peso: 0.1122\",\"Peso: 0.0296\",\"Peso: 0.1269\",\"Peso: 0.1385\",\"Peso: -0.1092\",\"Peso: -0.1220\",\"Peso: -0.1194\",\"Peso: -0.0870\",\"Peso: 0.0927\",\"Peso: -0.1074\",\"Peso: -0.0122\",\"Peso: 0.1020\",\"Peso: 0.0578\",\"Peso: 0.0358\",\"Peso: 0.0751\",\"Peso: 0.0039\",\"Peso: -0.0122\",\"Peso: -0.1067\",\"Peso: 0.0433\",\"Peso: -0.0294\",\"Peso: -0.0868\",\"Peso: 0.0519\",\"Peso: 0.0257\",\"Peso: 0.0019\",\"Peso: 0.1067\",\"Peso: -0.0338\",\"Peso: -0.0274\",\"Peso: 0.1360\",\"Peso: -0.0202\",\"Peso: 0.0867\",\"Peso: 0.0664\",\"Peso: -0.0260\",\"Peso: 0.0127\",\"Peso: -0.0087\",\"Peso: 0.0784\",\"Peso: 0.0473\",\"Peso: -0.0086\",\"Peso: 0.1181\",\"Peso: 0.0377\",\"Peso: 0.0398\",\"Peso: 0.1452\",\"Peso: -0.0123\",\"Peso: -0.0380\",\"Peso: 0.0139\",\"Peso: -0.0377\",\"Peso: 0.0640\",\"Peso: -0.0923\",\"Peso: -0.0642\",\"Peso: 0.0512\",\"Peso: 0.1187\",\"Peso: 0.0517\",\"Peso: 0.1490\",\"Peso: 0.0150\",\"Peso: 0.0371\",\"Peso: 0.0851\",\"Peso: 0.0822\",\"Peso: 0.1288\",\"Peso: 0.1372\",\"Peso: 0.0629\",\"Peso: 0.0125\",\"Peso: -0.0210\",\"Peso: -0.0512\",\"Peso: 0.0135\",\"Peso: 0.0220\",\"Peso: 0.0030\",\"Peso: 0.1956\",\"Peso: 0.0480\",\"Peso: 0.0720\",\"Peso: 0.0234\",\"Peso: 0.0066\",\"Peso: -0.0093\",\"Peso: 0.0118\",\"Peso: -0.0952\",\"Peso: -0.1386\",\"Peso: 0.0965\",\"Peso: 0.1363\",\"Peso: -0.0355\",\"Peso: -0.0342\",\"Peso: 0.0018\",\"Peso: -0.0053\",\"Peso: 0.0109\",\"Peso: -0.0181\",\"Peso: -0.0286\",\"Peso: -0.0019\",\"Peso: 0.0134\",\"Peso: -0.0048\",\"Peso: 0.0353\",\"Peso: -0.0014\",\"Peso: -0.0190\",\"Peso: -0.0067\",\"Peso: 0.0191\",\"Peso: 0.0128\",\"Peso: 0.0249\",\"Peso: -0.0342\",\"Peso: 0.0088\",\"Peso: 0.0238\",\"Peso: 0.0130\",\"Peso: 0.0392\",\"Peso: -0.0282\",\"Peso: -0.0095\",\"Peso: -0.0302\",\"Peso: 0.0138\",\"Peso: -0.0104\",\"Peso: -0.0313\",\"Peso: 0.0371\",\"Peso: 0.0256\",\"Peso: -0.0253\",\"Peso: -0.0265\",\"Peso: 0.0178\",\"Peso: 0.0245\",\"Peso: -0.0020\",\"Peso: 0.0223\",\"Peso: -0.0232\",\"Peso: -0.0097\",\"Peso: 0.0335\",\"Peso: -0.0350\",\"Peso: -0.0118\",\"Peso: 0.0207\",\"Peso: 0.0146\",\"Peso: -0.0123\",\"Peso: -0.0227\",\"Peso: 0.0178\",\"Peso: -0.0210\",\"Peso: 0.0348\",\"Peso: 0.0036\",\"Peso: -0.0016\",\"Peso: -0.0046\",\"Peso: 0.0267\",\"Peso: -0.0125\",\"Peso: -0.0076\",\"Peso: 0.0271\",\"Peso: -0.0256\",\"Peso: -0.0108\",\"Peso: -0.0033\",\"Peso: -0.0020\",\"Peso: 0.0089\",\"Peso: -0.0133\",\"Peso: -0.0085\",\"Peso: -0.0278\",\"Peso: 0.0129\",\"Peso: 0.0088\",\"Peso: 0.0234\",\"Peso: -0.0345\",\"Peso: 0.0217\",\"Peso: 0.0052\",\"Peso: -0.0050\",\"Peso: 0.0269\",\"Peso: -0.0205\",\"Peso: -0.0207\",\"Peso: -0.0131\",\"Peso: -0.0254\",\"Peso: 0.0121\",\"Peso: 0.0393\",\"Peso: 0.0085\",\"Peso: 0.0028\",\"Peso: -0.0227\",\"Peso: -0.0076\",\"Peso: 0.0157\",\"Peso: -0.0013\",\"Peso: -0.0324\",\"Peso: -0.0314\",\"Peso: -0.0278\",\"Peso: -0.0357\",\"Peso: -0.0087\",\"Peso: -0.0356\",\"Peso: -0.0225\",\"Peso: -0.0232\",\"Peso: 0.0159\",\"Peso: -0.0216\",\"Peso: 0.0058\",\"Peso: -0.0261\",\"Peso: 0.0085\",\"Peso: 0.0235\",\"Peso: 0.0264\",\"Peso: 0.0185\",\"Peso: -0.0031\",\"Peso: 0.0229\",\"Peso: 0.0170\",\"Peso: 0.0260\",\"Peso: -0.0120\",\"Peso: -0.0220\",\"Peso: -0.0284\",\"Peso: -0.0296\",\"Peso: 0.0092\",\"Peso: 0.0152\",\"Peso: 0.0217\",\"Peso: 0.0272\",\"Peso: -0.0084\",\"Peso: 0.0276\",\"Peso: -0.0070\",\"Peso: -0.0259\",\"Peso: -0.0355\",\"Peso: -0.0212\",\"Peso: -0.0228\",\"Peso: 0.0256\",\"Peso: -0.0333\",\"Peso: -0.0105\",\"Peso: 0.0151\",\"Peso: -0.0159\",\"Peso: 0.0197\",\"Peso: -0.0308\",\"Peso: -0.0150\",\"Peso: 0.0238\",\"Peso: -0.0070\",\"Peso: -0.0233\",\"Peso: 0.0189\",\"Peso: 0.0345\",\"Peso: -0.0039\",\"Peso: 0.0307\",\"Peso: -0.0209\",\"Peso: -0.0032\",\"Peso: 0.0118\",\"Peso: 0.0319\",\"Peso: 0.0028\",\"Peso: 0.0065\",\"Peso: 0.0074\",\"Peso: 0.0242\",\"Peso: -0.0145\",\"Peso: 0.0030\",\"Peso: 0.0313\",\"Peso: 0.0298\",\"Peso: 0.0119\",\"Peso: 0.0218\",\"Peso: -0.0371\",\"Peso: 0.0199\",\"Peso: 0.0232\",\"Peso: -0.0295\",\"Peso: 0.0262\",\"Peso: 0.0083\",\"Peso: -0.0135\",\"Peso: 0.0080\",\"Peso: 0.0184\",\"Peso: 0.0125\",\"Peso: 0.0318\",\"Peso: -0.0100\",\"Peso: 0.0189\",\"Peso: -0.0260\",\"Peso: 0.0162\",\"Peso: -0.0310\",\"Peso: -0.0186\",\"Peso: -0.0356\",\"Peso: 0.0031\",\"Peso: 0.0249\",\"Peso: 0.0075\",\"Peso: 0.0126\",\"Peso: -0.0237\",\"Peso: -0.0290\",\"Peso: -0.0114\",\"Peso: 0.0317\",\"Peso: 0.0350\",\"Peso: -0.0139\",\"Peso: 0.0333\",\"Peso: 0.0265\",\"Peso: 0.0109\",\"Peso: -0.0160\",\"Peso: -0.0296\",\"Peso: -0.0377\",\"Peso: -0.0173\",\"Peso: -0.0129\",\"Peso: 0.0359\",\"Peso: 0.0141\",\"Peso: 0.0373\",\"Peso: -0.0045\",\"Peso: -0.0268\",\"Peso: -0.0012\",\"Peso: 0.0294\",\"Peso: 0.0292\",\"Peso: 0.0191\",\"Peso: -0.0013\",\"Peso: -0.0305\",\"Peso: 0.0020\",\"Peso: 0.0378\",\"Peso: 0.0476\",\"Peso: 0.0735\",\"Peso: 0.0626\",\"Peso: -0.0336\",\"Peso: 0.0412\",\"Peso: -0.0571\",\"Peso: -0.0693\",\"Peso: 0.0033\",\"Peso: 0.0179\",\"Peso: 0.0598\",\"Peso: -0.0276\"],\"x\":[0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,0,1,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,1,2,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,2,3,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,3,4,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null,4,5,null],\"y\":[0,-4.6875,null,0,-4.0625,null,0,-3.4375,null,0,-2.8125,null,0,-2.1875,null,0,-1.5625,null,0,-0.9375,null,0,-0.3125,null,0,0.3125,null,0,0.9375,null,0,1.5625,null,0,2.1875,null,0,2.8125,null,0,3.4375,null,0,4.0625,null,0,4.6875,null,-4.6875,-4.6875,null,-4.6875,-4.0625,null,-4.6875,-3.4375,null,-4.6875,-2.8125,null,-4.6875,-2.1875,null,-4.6875,-1.5625,null,-4.6875,-0.9375,null,-4.6875,-0.3125,null,-4.6875,0.3125,null,-4.0625,-4.6875,null,-4.0625,-4.0625,null,-4.0625,-3.4375,null,-4.0625,-2.8125,null,-4.0625,-2.1875,null,-4.0625,-1.5625,null,-4.0625,-0.9375,null,-4.0625,-0.3125,null,-4.0625,0.3125,null,-3.4375,-4.6875,null,-3.4375,-4.0625,null,-3.4375,-3.4375,null,-3.4375,-2.8125,null,-3.4375,-2.1875,null,-3.4375,-1.5625,null,-3.4375,-0.9375,null,-3.4375,-0.3125,null,-3.4375,0.3125,null,-2.8125,-4.6875,null,-2.8125,-4.0625,null,-2.8125,-3.4375,null,-2.8125,-2.8125,null,-2.8125,-2.1875,null,-2.8125,-1.5625,null,-2.8125,-0.9375,null,-2.8125,-0.3125,null,-2.8125,0.3125,null,-2.1875,-4.6875,null,-2.1875,-4.0625,null,-2.1875,-3.4375,null,-2.1875,-2.8125,null,-2.1875,-2.1875,null,-2.1875,-1.5625,null,-2.1875,-0.9375,null,-2.1875,-0.3125,null,-2.1875,0.3125,null,-1.5625,-4.6875,null,-1.5625,-4.0625,null,-1.5625,-3.4375,null,-1.5625,-2.8125,null,-1.5625,-2.1875,null,-1.5625,-1.5625,null,-1.5625,-0.9375,null,-1.5625,-0.3125,null,-1.5625,0.3125,null,-0.9375,-4.6875,null,-0.9375,-4.0625,null,-0.9375,-3.4375,null,-0.9375,-2.8125,null,-0.9375,-2.1875,null,-0.9375,-1.5625,null,-0.9375,-0.9375,null,-0.9375,-0.3125,null,-0.9375,0.3125,null,-0.3125,-4.6875,null,-0.3125,-4.0625,null,-0.3125,-3.4375,null,-0.3125,-2.8125,null,-0.3125,-2.1875,null,-0.3125,-1.5625,null,-0.3125,-0.9375,null,-0.3125,-0.3125,null,-0.3125,0.3125,null,0.3125,-4.6875,null,0.3125,-4.0625,null,0.3125,-3.4375,null,0.3125,-2.8125,null,0.3125,-2.1875,null,0.3125,-1.5625,null,0.3125,-0.9375,null,0.3125,-0.3125,null,0.3125,0.3125,null,0.9375,-4.6875,null,0.9375,-4.0625,null,0.9375,-3.4375,null,0.9375,-2.8125,null,0.9375,-2.1875,null,0.9375,-1.5625,null,0.9375,-0.9375,null,0.9375,-0.3125,null,0.9375,0.3125,null,1.5625,-4.6875,null,1.5625,-4.0625,null,1.5625,-3.4375,null,1.5625,-2.8125,null,1.5625,-2.1875,null,1.5625,-0.9375,null,1.5625,-0.3125,null,1.5625,0.3125,null,2.1875,-4.6875,null,2.1875,-4.0625,null,2.1875,-3.4375,null,2.1875,-2.8125,null,2.1875,-2.1875,null,2.1875,-1.5625,null,2.1875,-0.9375,null,2.1875,-0.3125,null,2.1875,0.3125,null,2.8125,-4.6875,null,2.8125,-4.0625,null,2.8125,-3.4375,null,2.8125,-2.8125,null,2.8125,-2.1875,null,2.8125,-1.5625,null,2.8125,-0.9375,null,2.8125,-0.3125,null,2.8125,0.3125,null,3.4375,-4.6875,null,3.4375,-4.0625,null,3.4375,-3.4375,null,3.4375,-2.8125,null,3.4375,-2.1875,null,3.4375,-1.5625,null,3.4375,-0.9375,null,3.4375,-0.3125,null,3.4375,0.3125,null,4.0625,-4.6875,null,4.0625,-4.0625,null,4.0625,-3.4375,null,4.0625,-2.8125,null,4.0625,-2.1875,null,4.0625,-1.5625,null,4.0625,-0.9375,null,4.0625,-0.3125,null,4.0625,0.3125,null,4.6875,-4.6875,null,4.6875,-4.0625,null,4.6875,-3.4375,null,4.6875,-2.8125,null,4.6875,-2.1875,null,4.6875,-1.5625,null,4.6875,-0.9375,null,4.6875,-0.3125,null,4.6875,0.3125,null,-4.6875,-4.916666666666666,null,-4.6875,-4.75,null,-4.6875,-4.583333333333333,null,-4.6875,-4.416666666666666,null,-4.6875,-4.25,null,-4.6875,-4.083333333333333,null,-4.6875,-3.9166666666666665,null,-4.6875,-3.75,null,-4.6875,-3.583333333333333,null,-4.6875,-3.4166666666666665,null,-4.6875,-3.25,null,-4.6875,-3.083333333333333,null,-4.6875,-2.9166666666666665,null,-4.6875,-2.75,null,-4.6875,-2.583333333333333,null,-4.6875,-2.4166666666666665,null,-4.6875,-2.25,null,-4.6875,-2.083333333333333,null,-4.6875,-1.9166666666666665,null,-4.6875,-1.75,null,-4.0625,-4.916666666666666,null,-4.0625,-4.75,null,-4.0625,-4.583333333333333,null,-4.0625,-4.416666666666666,null,-4.0625,-4.25,null,-4.0625,-4.083333333333333,null,-4.0625,-3.9166666666666665,null,-4.0625,-3.75,null,-4.0625,-3.4166666666666665,null,-4.0625,-3.25,null,-4.0625,-3.083333333333333,null,-4.0625,-2.9166666666666665,null,-4.0625,-2.75,null,-4.0625,-2.583333333333333,null,-4.0625,-2.4166666666666665,null,-4.0625,-2.25,null,-4.0625,-2.083333333333333,null,-4.0625,-1.9166666666666665,null,-4.0625,-1.75,null,-3.4375,-4.916666666666666,null,-3.4375,-4.75,null,-3.4375,-4.583333333333333,null,-3.4375,-4.416666666666666,null,-3.4375,-4.25,null,-3.4375,-4.083333333333333,null,-3.4375,-3.9166666666666665,null,-3.4375,-3.75,null,-3.4375,-3.583333333333333,null,-3.4375,-3.4166666666666665,null,-3.4375,-3.25,null,-3.4375,-3.083333333333333,null,-3.4375,-2.9166666666666665,null,-3.4375,-2.75,null,-3.4375,-2.583333333333333,null,-3.4375,-2.4166666666666665,null,-3.4375,-2.25,null,-3.4375,-1.9166666666666665,null,-3.4375,-1.75,null,-2.8125,-4.916666666666666,null,-2.8125,-4.75,null,-2.8125,-4.583333333333333,null,-2.8125,-4.416666666666666,null,-2.8125,-4.25,null,-2.8125,-4.083333333333333,null,-2.8125,-3.9166666666666665,null,-2.8125,-3.75,null,-2.8125,-3.583333333333333,null,-2.8125,-3.4166666666666665,null,-2.8125,-3.25,null,-2.8125,-3.083333333333333,null,-2.8125,-2.9166666666666665,null,-2.8125,-2.75,null,-2.8125,-2.583333333333333,null,-2.8125,-2.4166666666666665,null,-2.8125,-2.25,null,-2.8125,-2.083333333333333,null,-2.8125,-1.9166666666666665,null,-2.8125,-1.75,null,-2.1875,-4.916666666666666,null,-2.1875,-4.75,null,-2.1875,-4.583333333333333,null,-2.1875,-4.416666666666666,null,-2.1875,-4.25,null,-2.1875,-4.083333333333333,null,-2.1875,-3.9166666666666665,null,-2.1875,-3.75,null,-2.1875,-3.583333333333333,null,-2.1875,-3.4166666666666665,null,-2.1875,-3.25,null,-2.1875,-3.083333333333333,null,-2.1875,-2.9166666666666665,null,-2.1875,-2.75,null,-2.1875,-2.583333333333333,null,-2.1875,-2.4166666666666665,null,-2.1875,-2.25,null,-2.1875,-2.083333333333333,null,-2.1875,-1.9166666666666665,null,-2.1875,-1.75,null,-1.5625,-4.916666666666666,null,-1.5625,-4.75,null,-1.5625,-4.583333333333333,null,-1.5625,-4.416666666666666,null,-1.5625,-4.25,null,-1.5625,-4.083333333333333,null,-1.5625,-3.9166666666666665,null,-1.5625,-3.75,null,-1.5625,-3.583333333333333,null,-1.5625,-3.4166666666666665,null,-1.5625,-3.25,null,-1.5625,-3.083333333333333,null,-1.5625,-2.9166666666666665,null,-1.5625,-2.75,null,-1.5625,-2.583333333333333,null,-1.5625,-2.4166666666666665,null,-1.5625,-2.25,null,-1.5625,-2.083333333333333,null,-1.5625,-1.9166666666666665,null,-1.5625,-1.75,null,-0.9375,-4.916666666666666,null,-0.9375,-4.75,null,-0.9375,-4.583333333333333,null,-0.9375,-4.416666666666666,null,-0.9375,-4.25,null,-0.9375,-4.083333333333333,null,-0.9375,-3.9166666666666665,null,-0.9375,-3.75,null,-0.9375,-3.583333333333333,null,-0.9375,-3.4166666666666665,null,-0.9375,-3.25,null,-0.9375,-3.083333333333333,null,-0.9375,-2.9166666666666665,null,-0.9375,-2.75,null,-0.9375,-2.583333333333333,null,-0.9375,-2.4166666666666665,null,-0.9375,-2.25,null,-0.9375,-2.083333333333333,null,-0.9375,-1.9166666666666665,null,-0.9375,-1.75,null,-0.3125,-4.916666666666666,null,-0.3125,-4.75,null,-0.3125,-4.583333333333333,null,-0.3125,-4.416666666666666,null,-0.3125,-4.25,null,-0.3125,-4.083333333333333,null,-0.3125,-3.9166666666666665,null,-0.3125,-3.75,null,-0.3125,-3.583333333333333,null,-0.3125,-3.4166666666666665,null,-0.3125,-3.25,null,-0.3125,-3.083333333333333,null,-0.3125,-2.9166666666666665,null,-0.3125,-2.75,null,-0.3125,-2.583333333333333,null,-0.3125,-2.4166666666666665,null,-0.3125,-2.25,null,-0.3125,-2.083333333333333,null,-0.3125,-1.9166666666666665,null,-0.3125,-1.75,null,0.3125,-4.916666666666666,null,0.3125,-4.75,null,0.3125,-4.583333333333333,null,0.3125,-4.416666666666666,null,0.3125,-4.25,null,0.3125,-4.083333333333333,null,0.3125,-3.9166666666666665,null,0.3125,-3.75,null,0.3125,-3.583333333333333,null,0.3125,-3.4166666666666665,null,0.3125,-3.25,null,0.3125,-3.083333333333333,null,0.3125,-2.9166666666666665,null,0.3125,-2.75,null,0.3125,-2.583333333333333,null,0.3125,-2.4166666666666665,null,0.3125,-2.25,null,0.3125,-2.083333333333333,null,0.3125,-1.9166666666666665,null,0.3125,-1.75,null,0.9375,-4.916666666666666,null,0.9375,-4.75,null,0.9375,-4.583333333333333,null,0.9375,-4.416666666666666,null,0.9375,-4.25,null,0.9375,-4.083333333333333,null,0.9375,-3.9166666666666665,null,0.9375,-3.75,null,0.9375,-3.583333333333333,null,0.9375,-3.4166666666666665,null,0.9375,-3.25,null,0.9375,-3.083333333333333,null,0.9375,-2.9166666666666665,null,0.9375,-2.75,null,0.9375,-2.583333333333333,null,0.9375,-2.4166666666666665,null,0.9375,-2.25,null,0.9375,-2.083333333333333,null,0.9375,-1.9166666666666665,null,0.9375,-1.75,null,1.5625,-4.916666666666666,null,1.5625,-4.75,null,1.5625,-4.583333333333333,null,1.5625,-4.416666666666666,null,1.5625,-4.25,null,1.5625,-4.083333333333333,null,1.5625,-3.9166666666666665,null,1.5625,-3.75,null,1.5625,-3.583333333333333,null,1.5625,-3.4166666666666665,null,1.5625,-3.25,null,1.5625,-3.083333333333333,null,1.5625,-2.9166666666666665,null,1.5625,-2.75,null,1.5625,-2.583333333333333,null,1.5625,-2.4166666666666665,null,1.5625,-2.25,null,1.5625,-2.083333333333333,null,1.5625,-1.9166666666666665,null,1.5625,-1.75,null,2.1875,-4.916666666666666,null,2.1875,-4.75,null,2.1875,-4.583333333333333,null,2.1875,-4.416666666666666,null,2.1875,-4.25,null,2.1875,-4.083333333333333,null,2.1875,-3.9166666666666665,null,2.1875,-3.75,null,2.1875,-3.583333333333333,null,2.1875,-3.4166666666666665,null,2.1875,-3.25,null,2.1875,-3.083333333333333,null,2.1875,-2.9166666666666665,null,2.1875,-2.75,null,2.1875,-2.583333333333333,null,2.1875,-2.4166666666666665,null,2.1875,-2.25,null,2.1875,-2.083333333333333,null,2.1875,-1.9166666666666665,null,2.1875,-1.75,null,2.8125,-4.916666666666666,null,2.8125,-4.75,null,2.8125,-4.583333333333333,null,2.8125,-4.416666666666666,null,2.8125,-4.25,null,2.8125,-4.083333333333333,null,2.8125,-3.9166666666666665,null,2.8125,-3.75,null,2.8125,-3.583333333333333,null,2.8125,-3.4166666666666665,null,2.8125,-3.25,null,2.8125,-3.083333333333333,null,2.8125,-2.9166666666666665,null,2.8125,-2.75,null,2.8125,-2.583333333333333,null,2.8125,-2.4166666666666665,null,2.8125,-2.25,null,2.8125,-2.083333333333333,null,2.8125,-1.9166666666666665,null,2.8125,-1.75,null,3.4375,-4.916666666666666,null,3.4375,-4.75,null,3.4375,-4.583333333333333,null,3.4375,-4.416666666666666,null,3.4375,-4.25,null,3.4375,-4.083333333333333,null,3.4375,-3.9166666666666665,null,3.4375,-3.75,null,3.4375,-3.583333333333333,null,3.4375,-3.4166666666666665,null,3.4375,-3.25,null,3.4375,-3.083333333333333,null,3.4375,-2.9166666666666665,null,3.4375,-2.75,null,3.4375,-2.583333333333333,null,3.4375,-2.4166666666666665,null,3.4375,-2.25,null,3.4375,-2.083333333333333,null,3.4375,-1.9166666666666665,null,3.4375,-1.75,null,4.0625,-4.916666666666666,null,4.0625,-4.75,null,4.0625,-4.583333333333333,null,4.0625,-4.416666666666666,null,4.0625,-4.25,null,4.0625,-4.083333333333333,null,4.0625,-3.9166666666666665,null,4.0625,-3.75,null,4.0625,-3.583333333333333,null,4.0625,-3.4166666666666665,null,4.0625,-3.25,null,4.0625,-3.083333333333333,null,4.0625,-2.9166666666666665,null,4.0625,-2.75,null,4.0625,-2.583333333333333,null,4.0625,-2.4166666666666665,null,4.0625,-2.25,null,4.0625,-2.083333333333333,null,4.0625,-1.9166666666666665,null,4.0625,-1.75,null,4.6875,-4.916666666666666,null,4.6875,-4.75,null,4.6875,-4.583333333333333,null,4.6875,-4.416666666666666,null,4.6875,-4.25,null,4.6875,-4.083333333333333,null,4.6875,-3.9166666666666665,null,4.6875,-3.75,null,4.6875,-3.583333333333333,null,4.6875,-3.4166666666666665,null,4.6875,-3.25,null,4.6875,-3.083333333333333,null,4.6875,-2.9166666666666665,null,4.6875,-2.75,null,4.6875,-2.583333333333333,null,4.6875,-2.4166666666666665,null,4.6875,-2.25,null,4.6875,-2.083333333333333,null,4.6875,-1.9166666666666665,null,4.6875,-1.75,null,-4.916666666666666,-4.5,null,-4.916666666666666,-3.5,null,-4.916666666666666,-2.5,null,-4.916666666666666,-1.5,null,-4.916666666666666,-0.5,null,-4.916666666666666,0.5,null,-4.916666666666666,1.5,null,-4.916666666666666,2.5,null,-4.916666666666666,3.5,null,-4.916666666666666,4.5,null,-4.75,-4.5,null,-4.75,-3.5,null,-4.75,-2.5,null,-4.75,-1.5,null,-4.75,-0.5,null,-4.75,0.5,null,-4.75,1.5,null,-4.75,2.5,null,-4.75,3.5,null,-4.75,4.5,null,-4.583333333333333,-4.5,null,-4.583333333333333,-3.5,null,-4.583333333333333,-2.5,null,-4.583333333333333,-1.5,null,-4.583333333333333,-0.5,null,-4.583333333333333,0.5,null,-4.583333333333333,1.5,null,-4.583333333333333,2.5,null,-4.583333333333333,3.5,null,-4.583333333333333,4.5,null,-4.416666666666666,-4.5,null,-4.416666666666666,-3.5,null,-4.416666666666666,-2.5,null,-4.416666666666666,-1.5,null,-4.416666666666666,-0.5,null,-4.416666666666666,0.5,null,-4.416666666666666,1.5,null,-4.416666666666666,2.5,null,-4.416666666666666,3.5,null,-4.416666666666666,4.5,null,-4.25,-4.5,null,-4.25,-3.5,null,-4.25,-2.5,null,-4.25,-1.5,null,-4.25,-0.5,null,-4.25,0.5,null,-4.25,1.5,null,-4.25,2.5,null,-4.25,3.5,null,-4.25,4.5,null,-4.083333333333333,-4.5,null,-4.083333333333333,-3.5,null,-4.083333333333333,-2.5,null,-4.083333333333333,-1.5,null,-4.083333333333333,-0.5,null,-4.083333333333333,1.5,null,-4.083333333333333,2.5,null,-4.083333333333333,3.5,null,-4.083333333333333,4.5,null,-3.9166666666666665,-4.5,null,-3.9166666666666665,-3.5,null,-3.9166666666666665,-2.5,null,-3.9166666666666665,-1.5,null,-3.9166666666666665,-0.5,null,-3.9166666666666665,0.5,null,-3.9166666666666665,1.5,null,-3.9166666666666665,2.5,null,-3.9166666666666665,3.5,null,-3.9166666666666665,4.5,null,-3.75,-4.5,null,-3.75,-3.5,null,-3.75,-2.5,null,-3.75,-1.5,null,-3.75,-0.5,null,-3.75,0.5,null,-3.75,1.5,null,-3.75,2.5,null,-3.75,3.5,null,-3.75,4.5,null,-3.583333333333333,-4.5,null,-3.583333333333333,-2.5,null,-3.583333333333333,-1.5,null,-3.583333333333333,-0.5,null,-3.583333333333333,0.5,null,-3.583333333333333,1.5,null,-3.583333333333333,2.5,null,-3.583333333333333,3.5,null,-3.583333333333333,4.5,null,-3.4166666666666665,-4.5,null,-3.4166666666666665,-3.5,null,-3.4166666666666665,-2.5,null,-3.4166666666666665,-1.5,null,-3.4166666666666665,-0.5,null,-3.4166666666666665,0.5,null,-3.4166666666666665,1.5,null,-3.4166666666666665,2.5,null,-3.4166666666666665,3.5,null,-3.4166666666666665,4.5,null,-3.25,-4.5,null,-3.25,-3.5,null,-3.25,-2.5,null,-3.25,-1.5,null,-3.25,-0.5,null,-3.25,0.5,null,-3.25,1.5,null,-3.25,2.5,null,-3.25,3.5,null,-3.25,4.5,null,-3.083333333333333,-4.5,null,-3.083333333333333,-3.5,null,-3.083333333333333,-1.5,null,-3.083333333333333,-0.5,null,-3.083333333333333,0.5,null,-3.083333333333333,1.5,null,-3.083333333333333,2.5,null,-3.083333333333333,3.5,null,-3.083333333333333,4.5,null,-2.9166666666666665,-4.5,null,-2.9166666666666665,-3.5,null,-2.9166666666666665,-2.5,null,-2.9166666666666665,-1.5,null,-2.9166666666666665,-0.5,null,-2.9166666666666665,0.5,null,-2.9166666666666665,1.5,null,-2.9166666666666665,2.5,null,-2.9166666666666665,3.5,null,-2.9166666666666665,4.5,null,-2.75,-4.5,null,-2.75,-3.5,null,-2.75,-2.5,null,-2.75,-1.5,null,-2.75,-0.5,null,-2.75,0.5,null,-2.75,1.5,null,-2.75,2.5,null,-2.75,3.5,null,-2.75,4.5,null,-2.583333333333333,-4.5,null,-2.583333333333333,-3.5,null,-2.583333333333333,-2.5,null,-2.583333333333333,-1.5,null,-2.583333333333333,-0.5,null,-2.583333333333333,0.5,null,-2.583333333333333,1.5,null,-2.583333333333333,2.5,null,-2.583333333333333,3.5,null,-2.583333333333333,4.5,null,-2.4166666666666665,-4.5,null,-2.4166666666666665,-3.5,null,-2.4166666666666665,-2.5,null,-2.4166666666666665,-1.5,null,-2.4166666666666665,-0.5,null,-2.4166666666666665,0.5,null,-2.4166666666666665,1.5,null,-2.4166666666666665,2.5,null,-2.4166666666666665,3.5,null,-2.4166666666666665,4.5,null,-2.25,-4.5,null,-2.25,-3.5,null,-2.25,-2.5,null,-2.25,-1.5,null,-2.25,-0.5,null,-2.25,0.5,null,-2.25,1.5,null,-2.25,2.5,null,-2.25,3.5,null,-2.25,4.5,null,-2.083333333333333,-4.5,null,-2.083333333333333,-3.5,null,-2.083333333333333,-2.5,null,-2.083333333333333,-1.5,null,-2.083333333333333,-0.5,null,-2.083333333333333,0.5,null,-2.083333333333333,1.5,null,-2.083333333333333,2.5,null,-2.083333333333333,3.5,null,-2.083333333333333,4.5,null,-1.9166666666666665,-4.5,null,-1.9166666666666665,-3.5,null,-1.9166666666666665,-2.5,null,-1.9166666666666665,-1.5,null,-1.9166666666666665,-0.5,null,-1.9166666666666665,0.5,null,-1.9166666666666665,1.5,null,-1.9166666666666665,2.5,null,-1.9166666666666665,3.5,null,-1.9166666666666665,4.5,null,-1.75,-4.5,null,-1.75,-3.5,null,-1.75,-2.5,null,-1.75,-1.5,null,-1.75,-0.5,null,-1.75,0.5,null,-1.75,1.5,null,-1.75,2.5,null,-1.75,3.5,null,-1.75,4.5,null,-4.5,0,null,-3.5,0,null,-2.5,0,null,-1.5,0,null,-0.5,0,null,0.5,0,null,1.5,0,null,2.5,0,null,3.5,0,null,4.5,0,null],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":[\"rgba(200, 200, 200, 0.8)\",\"rgba(0, 0, 145, 0.8)\",\"rgba(0, 0, 149, 0.8)\",\"rgba(0, 0, 122, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 118, 0.8)\",\"rgba(0, 0, 140, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 151, 0.8)\",\"rgba(0, 0, 117, 0.8)\",\"rgba(0, 0, 120, 0.8)\",\"rgba(0, 0, 116, 0.8)\",\"rgba(0, 0, 136, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 116, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(0, 0, 144, 0.8)\",\"rgba(0, 0, 222, 0.8)\",\"rgba(0, 0, 165, 0.8)\",\"rgba(0, 0, 213, 0.8)\",\"rgba(150, 150, 150, 0.8)\",\"rgba(141, 0, 0, 0.8)\",\"rgba(0, 0, 222, 0.8)\",\"rgba(0, 0, 143, 0.8)\",\"rgba(0, 0, 174, 0.8)\",\"rgba(0, 0, 148, 0.8)\",\"rgba(0, 0, 179, 0.8)\",\"rgba(0, 0, 196, 0.8)\",\"rgba(0, 0, 183, 0.8)\",\"rgba(0, 0, 176, 0.8)\",\"rgba(0, 0, 195, 0.8)\",\"rgba(0, 0, 164, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(218, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 248, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(216, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(226, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 196, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(190, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(194, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(0, 200, 0, 0.8)\"],\"line\":{\"color\":\"white\",\"width\":2},\"showscale\":false,\"size\":[25,14.426209926605225,14.794762879610062,12.159969061613083,10.782014355063438,11.820298582315445,13.910258412361145,10.41830513626337,14.980689585208893,11.684347577393055,11.99522890150547,11.579801551997662,13.546519577503204,10.024932807427831,11.614008769392967,10.084346365183592,10.52990660071373,14.281815886497498,21.87513440847397,16.306331157684326,21.009680330753326,10.92021830379963,14.037702232599258,21.87088042497635,14.227294623851776,17.202918976545334,14.646335244178772,17.67889529466629,19.34770315885544,18.124561309814453,17.40607663989067,19.22922194004059,16.264444440603256,64.49306011199951,33.77922058105469,39.26502823829651,21.490067541599274,36.06887400150299,69.3750274181366,46.816415786743164,29.57430362701416,24.39698576927185,56.56169295310974,21.313644349575043,35.26889979839325,49.608988761901855,119.09795045852661,28.164219856262207,122.11186647415161,70.66701889038086,116.98128700256348,68.35124611854553,72.84181118011475,64.9736762046814,22.236287593841553,30.95098853111267,102.40747928619385,76.05293035507202,85.21904706954956,101.19524955749512,75.56473970413208,30.465978980064392,19.297768473625183,81.56674385070801,58.97213697433472,128.00690412521362,103.4372615814209,118.038170337677,109.02791500091553,94.42981004714966,18.790319561958313,51.59262180328369,43.751959800720215,66.94939136505127,37.722121477127075,70.15020370483398,69.4115948677063,50.834150314331055,78.09715270996094,42.066062688827515,47.289535999298096,42.422131299972534,101.27232551574707,77.17259168624878,37.45924532413483,27.19230890274048,88.7630820274353,39.06534492969513,30.058406591415405,41.58477067947388,97.0637035369873,42.16312646865845,19.16890799999237,93.11077833175659,80.1039457321167,76.25968217849731,135.20092487335205,115.95611095428467,93.02899599075317,118.68656396865845,58.573442697525024,85.54654836654663,192.94898509979248,25]},\"mode\":\"markers\",\"text\":[\"Imagen de entrada\",\"conv1 neurona 0\\u003cbr\\u003eActivación: 0.2951\",\"conv1 neurona 1\\u003cbr\\u003eActivación: 0.3197\",\"conv1 neurona 2\\u003cbr\\u003eActivación: 0.1440\",\"conv1 neurona 3\\u003cbr\\u003eActivación: 0.0521\",\"conv1 neurona 4\\u003cbr\\u003eActivación: 0.1214\",\"conv1 neurona 5\\u003cbr\\u003eActivación: 0.2607\",\"conv1 neurona 6\\u003cbr\\u003eActivación: 0.0279\",\"conv1 neurona 7\\u003cbr\\u003eActivación: 0.3320\",\"conv1 neurona 8\\u003cbr\\u003eActivación: 0.1123\",\"conv1 neurona 9\\u003cbr\\u003eActivación: 0.1330\",\"conv1 neurona 10\\u003cbr\\u003eActivación: 0.1053\",\"conv1 neurona 11\\u003cbr\\u003eActivación: 0.2364\",\"conv1 neurona 12\\u003cbr\\u003eActivación: 0.0017\",\"conv1 neurona 13\\u003cbr\\u003eActivación: 0.1076\",\"conv1 neurona 14\\u003cbr\\u003eActivación: -0.0056\",\"conv1 neurona 15\\u003cbr\\u003eActivación: 0.0353\",\"conv2 neurona 0\\u003cbr\\u003eActivación: 0.2855\",\"conv2 neurona 1\\u003cbr\\u003eActivación: 0.7917\",\"conv2 neurona 2\\u003cbr\\u003eActivación: 0.4204\",\"conv2 neurona 3\\u003cbr\\u003eActivación: 0.7340\",\"conv2 neurona 4\\u003cbr\\u003eActivación: -0.0613\",\"conv2 neurona 5\\u003cbr\\u003eActivación: -0.2692\",\"conv2 neurona 6\\u003cbr\\u003eActivación: 0.7914\",\"conv2 neurona 7\\u003cbr\\u003eActivación: 0.2818\",\"conv2 neurona 8\\u003cbr\\u003eActivación: 0.4802\",\"conv2 neurona 9\\u003cbr\\u003eActivación: 0.3098\",\"conv2 neurona 10\\u003cbr\\u003eActivación: 0.5119\",\"conv2 neurona 11\\u003cbr\\u003eActivación: 0.6232\",\"conv2 neurona 12\\u003cbr\\u003eActivación: 0.5416\",\"conv2 neurona 13\\u003cbr\\u003eActivación: 0.4937\",\"conv2 neurona 14\\u003cbr\\u003eActivación: 0.6153\",\"conv2 neurona 15\\u003cbr\\u003eActivación: 0.4176\",\"fc1 neurona 0\\u003cbr\\u003eActivación: 3.6329\",\"fc1 neurona 1\\u003cbr\\u003eActivación: -1.5853\",\"fc1 neurona 2\\u003cbr\\u003eActivación: 1.9510\",\"fc1 neurona 3\\u003cbr\\u003eActivación: -0.7660\",\"fc1 neurona 4\\u003cbr\\u003eActivación: -1.7379\",\"fc1 neurona 5\\u003cbr\\u003eActivación: 3.9583\",\"fc1 neurona 6\\u003cbr\\u003eActivación: -2.4544\",\"fc1 neurona 7\\u003cbr\\u003eActivación: -1.3050\",\"fc1 neurona 8\\u003cbr\\u003eActivación: 0.9598\",\"fc1 neurona 9\\u003cbr\\u003eActivación: -3.1041\",\"fc1 neurona 10\\u003cbr\\u003eActivación: -0.7542\",\"fc1 neurona 11\\u003cbr\\u003eActivación: -1.6846\",\"fc1 neurona 12\\u003cbr\\u003eActivación: -2.6406\",\"fc1 neurona 13\\u003cbr\\u003eActivación: 7.2732\",\"fc1 neurona 14\\u003cbr\\u003eActivación: -1.2109\",\"fc1 neurona 15\\u003cbr\\u003eActivación: 7.4741\",\"fc1 neurona 16\\u003cbr\\u003eActivación: 4.0445\",\"fc1 neurona 17\\u003cbr\\u003eActivación: 7.1321\",\"fc1 neurona 18\\u003cbr\\u003eActivación: 3.8901\",\"fc1 neurona 19\\u003cbr\\u003eActivación: 4.1895\",\"fc1 neurona 20\\u003cbr\\u003eActivación: 3.6649\",\"fc1 neurona 21\\u003cbr\\u003eActivación: -0.8158\",\"fc1 neurona 22\\u003cbr\\u003eActivación: -1.3967\",\"fc1 neurona 23\\u003cbr\\u003eActivación: 6.1605\",\"fc1 neurona 24\\u003cbr\\u003eActivación: 4.4035\",\"fc1 neurona 25\\u003cbr\\u003eActivación: 5.0146\",\"fc1 neurona 26\\u003cbr\\u003eActivación: 6.0797\",\"fc1 neurona 27\\u003cbr\\u003eActivación: 4.3710\",\"fc1 neurona 28\\u003cbr\\u003eActivación: -1.3644\",\"fc1 neurona 29\\u003cbr\\u003eActivación: 0.6199\",\"fc1 neurona 30\\u003cbr\\u003eActivación: 4.7711\",\"fc1 neurona 31\\u003cbr\\u003eActivación: -3.2648\",\"fc1 neurona 32\\u003cbr\\u003eActivación: 7.8671\",\"fc1 neurona 33\\u003cbr\\u003eActivación: 6.2292\",\"fc1 neurona 34\\u003cbr\\u003eActivación: 7.2025\",\"fc1 neurona 35\\u003cbr\\u003eActivación: 6.6019\",\"fc1 neurona 36\\u003cbr\\u003eActivación: 5.6287\",\"fc1 neurona 37\\u003cbr\\u003eActivación: -0.5860\",\"fc1 neurona 38\\u003cbr\\u003eActivación: -2.7728\",\"fc1 neurona 39\\u003cbr\\u003eActivación: -2.2501\",\"fc1 neurona 40\\u003cbr\\u003eActivación: 3.7966\",\"fc1 neurona 41\\u003cbr\\u003eActivación: -1.8481\",\"fc1 neurona 42\\u003cbr\\u003eActivación: 4.0100\",\"fc1 neurona 43\\u003cbr\\u003eActivación: 3.9608\",\"fc1 neurona 44\\u003cbr\\u003eActivación: 2.7223\",\"fc1 neurona 45\\u003cbr\\u003eActivación: 4.5398\",\"fc1 neurona 46\\u003cbr\\u003eActivación: -2.1377\",\"fc1 neurona 47\\u003cbr\\u003eActivación: -2.4860\",\"fc1 neurona 48\\u003cbr\\u003eActivación: -2.1615\",\"fc1 neurona 49\\u003cbr\\u003eActivación: 6.0848\",\"fc1 neurona 50\\u003cbr\\u003eActivación: 4.4782\",\"fc1 neurona 51\\u003cbr\\u003eActivación: -1.8306\",\"fc1 neurona 52\\u003cbr\\u003eActivación: -1.1462\",\"fc1 neurona 53\\u003cbr\\u003eActivación: 5.2509\",\"fc1 neurona 54\\u003cbr\\u003eActivación: -1.9377\",\"fc1 neurona 55\\u003cbr\\u003eActivación: -1.3372\",\"fc1 neurona 56\\u003cbr\\u003eActivación: 2.1057\",\"fc1 neurona 57\\u003cbr\\u003eActivación: 5.8042\",\"fc1 neurona 58\\u003cbr\\u003eActivación: -2.1442\",\"fc1 neurona 59\\u003cbr\\u003eActivación: -0.6113\",\"fc2 neurona 0\\u003cbr\\u003eActivación: -5.5407\",\"fc2 neurona 1\\u003cbr\\u003eActivación: 4.6736\",\"fc2 neurona 2\\u003cbr\\u003eActivación: -4.4173\",\"fc2 neurona 3\\u003cbr\\u003eActivación: -8.3467\",\"fc2 neurona 4\\u003cbr\\u003eActivación: -7.0637\",\"fc2 neurona 5\\u003cbr\\u003eActivación: -5.5353\",\"fc2 neurona 6\\u003cbr\\u003eActivación: -7.2458\",\"fc2 neurona 7\\u003cbr\\u003eActivación: -3.2382\",\"fc2 neurona 8\\u003cbr\\u003eActivación: -5.0364\",\"fc2 neurona 9\\u003cbr\\u003eActivación: 12.1966\",\"Clase predicha: 9\"],\"x\":[0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5],\"y\":[0,-4.6875,-4.0625,-3.4375,-2.8125,-2.1875,-1.5625,-0.9375,-0.3125,0.3125,0.9375,1.5625,2.1875,2.8125,3.4375,4.0625,4.6875,-4.6875,-4.0625,-3.4375,-2.8125,-2.1875,-1.5625,-0.9375,-0.3125,0.3125,0.9375,1.5625,2.1875,2.8125,3.4375,4.0625,4.6875,-4.916666666666666,-4.75,-4.583333333333333,-4.416666666666666,-4.25,-4.083333333333333,-3.9166666666666665,-3.75,-3.583333333333333,-3.4166666666666665,-3.25,-3.083333333333333,-2.9166666666666665,-2.75,-2.583333333333333,-2.4166666666666665,-2.25,-2.083333333333333,-1.9166666666666665,-1.75,-1.5833333333333333,-1.4166666666666665,-1.25,-1.0833333333333333,-0.9166666666666666,-0.75,-0.5833333333333333,-0.41666666666666663,-0.25,-0.08333333333333333,0.08333333333333333,0.25,0.41666666666666663,0.5833333333333333,0.75,0.9166666666666666,1.0833333333333333,1.25,1.4166666666666665,1.5833333333333333,1.75,1.9166666666666665,2.083333333333333,2.25,2.4166666666666665,2.583333333333333,2.75,2.9166666666666665,3.083333333333333,3.25,3.4166666666666665,3.583333333333333,3.75,3.9166666666666665,4.083333333333333,4.25,4.416666666666666,4.583333333333333,4.75,4.916666666666666,-4.5,-3.5,-2.5,-1.5,-0.5,0.5,1.5,2.5,3.5,4.5,0],\"type\":\"scatter\"}],                        {\"annotations\":[{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"INPUT\\u003cbr\\u003e(1 neuronas)\",\"x\":0,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"CONV1\\u003cbr\\u003e(16 neuronas)\",\"x\":1,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"CONV2\\u003cbr\\u003e(16 neuronas)\",\"x\":2,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"FC1\\u003cbr\\u003e(60 neuronas)\",\"x\":3,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"FC2\\u003cbr\\u003e(10 neuronas)\",\"x\":4,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"OUTPUT\\u003cbr\\u003e(1 neuronas)\",\"x\":5,\"xref\":\"x\",\"y\":5,\"yref\":\"y\"},{\"align\":\"left\",\"showarrow\":false,\"text\":\"\\u003cb\\u003eLeyenda:\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eColor de nodo:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• \\u003cspan style='color:blue'\\u003eAzul\\u003c\\u002fspan\\u003e: Activación positiva\\u003cbr\\u003e• \\u003cspan style='color:red'\\u003eRojo\\u003c\\u002fspan\\u003e: Activación negativa\\u003cbr\\u003e• \\u003cspan style='color:gray'\\u003eGris\\u003c\\u002fspan\\u003e: Activación cercana a cero\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eTamaño de nodo:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• Mayor tamaño = Mayor magnitud de activación\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eColor de conexión:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• \\u003cspan style='color:blue'\\u003eAzul\\u003c\\u002fspan\\u003e: Peso positivo\\u003cbr\\u003e• \\u003cspan style='color:red'\\u003eRojo\\u003c\\u002fspan\\u003e: Peso negativo\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eGrosor de conexión:\\u003c\\u002fb\\u003e\\u003cbr\\u003e• Mayor grosor = Mayor magnitud del peso\",\"x\":1.02,\"xref\":\"paper\",\"y\":0.5,\"yref\":\"paper\"},{\"align\":\"left\",\"showarrow\":false,\"text\":\"\\u003cb\\u003e¿Por qué hay activaciones negativas?\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eEn redes neuronales, las activaciones negativas\\u003cbr\\u003eocurren cuando el input a una neurona produce\\u003cbr\\u003eun valor negativo. Esto es común en capas con\\u003cbr\\u003efunciones de activación como ReLU, tanh o\\u003cbr\\u003efunciones lineales. Las activaciones negativas\\u003cbr\\u003epueden indicar inhibición o respuesta contraria\\u003cbr\\u003ea ciertas características de entrada.\",\"x\":1.02,\"xref\":\"paper\",\"y\":0.05,\"yref\":\"paper\"}],\"height\":1000,\"hovermode\":\"closest\",\"margin\":{\"b\":20,\"l\":5,\"r\":5,\"t\":40},\"showlegend\":false,\"title\":{\"font\":{\"size\":16},\"text\":\"Red Neuronal - Imagen 1\\u003cbr\\u003eEtiqueta real: 9, Predicción: 9\"},\"width\":1600,\"xaxis\":{\"showgrid\":false,\"showticklabels\":false,\"zeroline\":false},\"yaxis\":{\"showgrid\":false,\"showticklabels\":false,\"zeroline\":false},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6ea0539e-efa4-42c5-aa4b-932a29a8d0ac');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop here"
      ],
      "metadata": {
        "id": "S-EHYsFVBuvm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "2e60bfc2-7ab9-451c-9e6b-0a0f12247402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-14-a96ba3aab008>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-a96ba3aab008>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    stop here\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resnet & Inception (Pretrained, timm)"
      ],
      "metadata": {
        "id": "CIUf06eE2qmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "def setup_model(model_name):\n",
        "    model = timm.create_model(model_name, pretrained=True)  # Check number of classes\n",
        "\n",
        "    # Obtener el número de características de entrada para la capa final (dependiendo del modelo)\n",
        "    in_features = model.fc.in_features if hasattr(model, 'fc') else model.head.fc.in_features\n",
        "\n",
        "    # Modificar la capa final para que tenga 10 salidas (para 10 clases)\n",
        "    if hasattr(model, 'fc'):\n",
        "        model.fc = nn.Linear(in_features, 10)  # Ajuste para ResNet, Inception y otros modelos similares\n",
        "    elif hasattr(model, 'head'):\n",
        "        model.head.fc = nn.Linear(in_features, 10)  # Ajuste para modelos como EfficientNet\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model architecture\")\n",
        "\n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "nTz_Mg6ZU-wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model(model_name):\n",
        "    model = timm.create_model(model_name, pretrained=True,num_classes=10)  # Simplyfied\n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "wr9BxAjuaOVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading here the trained models by Protim\n",
        "def setup_model(model_name, checkpoint_path=None):\n",
        "    model = timm.create_model(model_name, pretrained=False ,num_classes=10)  # Simplyfied\n",
        "\n",
        "    if checkpoint_path:\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "        model.load_state_dict(checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint)\n",
        "        print(f\"Checkpoint loaded: {checkpoint_path}\")\n",
        "\n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "I8NfvrmK7kqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model(model_name, checkpoint_path=None):\n",
        "    model = timm.create_model(model_name, pretrained=False, num_classes=10)\n",
        "\n",
        "    if checkpoint_path:\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "        state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
        "\n",
        "        new_state_dict = {}\n",
        "        for key in state_dict:\n",
        "            new_key = key.replace('model.', '')\n",
        "            new_state_dict[new_key] = state_dict[key]\n",
        "\n",
        "        model.load_state_dict(new_state_dict)\n",
        "        print(f\"Checkpoint loaded: {checkpoint_path}\")\n",
        "\n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "cd6EfeKN-XwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transforms():\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(299),\n",
        "        transforms.CenterCrop(299),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Replicate the single channel!\n",
        "    ])"
      ],
      "metadata": {
        "id": "SudbP6TTfuqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "\n",
        "def get_transforms():\n",
        "    return transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.34433492, 0.38015251, 0.40769764), (0.20382567, 0.13680683, 0.11487552)),\n",
        "            transforms.Resize((224,224)),\n",
        "            transforms.ConvertImageDtype(torch.float32),\n",
        "            ])"
      ],
      "metadata": {
        "id": "NkxjvmMMip_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import EuroSAT\n",
        "from torch.utils.data import random_split, Subset\n",
        "\n",
        "### A first test to split the data and check the models... ###\n",
        "transform = get_transforms()\n",
        "full_dataset = datasets.EuroSAT(root='./data', download=True, transform=transform)\n",
        "#full_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "#MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "indices = torch.randperm(len(full_dataset)).tolist()\n",
        "subset_size = int(0.02 * len(full_dataset))\n",
        "subset_indices = indices[:subset_size]\n",
        "\n",
        "# Get subsample\n",
        "subset = Subset(full_dataset, subset_indices)\n",
        "#############################################\n",
        "\n",
        "calib_percentage = 0.5  # 50% for Calibration\n",
        "\n",
        "# Subset sizes\n",
        "calib_size = math.floor(calib_percentage * len(subset))\n",
        "test_size = len(subset) - calib_size\n",
        "\n",
        "# Data splitting: calibration and test\n",
        "calib_dataset, test_dataset = random_split(subset, [calib_size, test_size])\n",
        "\n",
        "# DataLoaders\n",
        "calib_dataloader = DataLoader(calib_dataset, batch_size=25) #100, probably should be smaller...\n",
        "test_dataloader  = DataLoader(test_dataset, batch_size=25)\n",
        "\n",
        "subset_labels = {full_dataset.targets[i] for i in subset_indices}\n",
        "print(subset_labels)"
      ],
      "metadata": {
        "id": "HBLItk7gP2ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(dataloader, num_images=10):\n",
        "    images, labels = next(iter(dataloader))  # Get the first batch of images and labels\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        img = images[i]\n",
        "        img = F.to_pil_image(img)  # Convert to PIL image\n",
        "\n",
        "        axes[i].imshow(img, cmap=\"gray\")  # Ensure grayscale display\n",
        "        axes[i].axis(\"off\")\n",
        "        axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "show_images(test_dataloader, num_images=10)"
      ],
      "metadata": {
        "id": "Xwt9_nGBukRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Around 4 mins\n",
        "\n",
        "def get_predictions(model, dataloader):\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in tqdm(dataloader, desc=\"Progress\", unit=\"batch\"):\n",
        "            outputs = model(images)\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            predictions.append(probs.numpy())\n",
        "    return np.concatenate(predictions)\n",
        "\n",
        "# Testing models outputs\n",
        "inception_v3_model = setup_model('inception_v3', '/content/drive/MyDrive/protim_checkpoints/inceptionv3_Checkpoint epoch=29-train_acc=1.00-train_loss=0.000-val_acc=0.98.ckpt')\n",
        "inception_preds = get_predictions(inception_v3_model, test_dataloader)\n",
        "print(\"Inception predictions shape:\", inception_preds.shape)\n",
        "\n",
        "resnet50_model = setup_model('resnet50', '/content/drive/MyDrive/protim_checkpoints/resnet_50_Checkpoint epoch=119-train_acc=0.94-train_loss=0.028-val_acc=0.95.ckpt')\n",
        "resnet_preds = get_predictions(resnet50_model, test_dataloader)\n",
        "print(\"ResNet predictions shape:\", resnet_preds.shape)"
      ],
      "metadata": {
        "id": "7dfChuqtLUMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_model_accuracy(predictions, true_labels):\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    accuracy = np.mean(predicted_labels == true_labels)\n",
        "    return accuracy\n",
        "\n",
        "# Extrating labels for each image\n",
        "true_labels = np.array([target for _, target in test_dataset])\n",
        "\n",
        "inception_accuracy = compute_model_accuracy(inception_preds, true_labels)\n",
        "print(f\"Inception Accuracy: {inception_accuracy}\")\n",
        "\n",
        "resnet_accuracy = compute_model_accuracy(resnet_preds, true_labels)\n",
        "print(f\"ResNet Accuracy: {resnet_accuracy}\")"
      ],
      "metadata": {
        "id": "U-cR87CFDjod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(test_dataloader))\n",
        "print(\"Shape of images:\", images.shape)\n",
        "print(\"Labels:\", labels)"
      ],
      "metadata": {
        "id": "edp8ITgNPTKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = np.unique(np.array([target for _, target in calib_dataset]))\n",
        "unique_labels"
      ],
      "metadata": {
        "id": "0qHYvTcEFblf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inception_preds.shape"
      ],
      "metadata": {
        "id": "q80IFKzwD4GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CHECK ACCURACY OF TIMM MODELS"
      ],
      "metadata": {
        "id": "qRYux1982tL4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}